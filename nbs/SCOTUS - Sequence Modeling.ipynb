{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting SCOTUS Behaviour Through Sequence Modeling\n",
    "\n",
    "The purpose of this notebook is to build a sequence model which accurately predict the voting patterns of the justices of the Supreme Court of the United States of America. The purpose is to not build a model using just a set of justices from a particular term. We want to build a general model which can generalize across terms. \n",
    "\n",
    "We work with the justice centered legacy database from http://scdb.wustl.edu/data.php which contains more than 200,000 records spanning over 200 years!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "# Imports\n",
    "import gc\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import statsmodels.stats.proportion\n",
    "import sklearn.pipeline\n",
    "from keras.models import load_model\n",
    "# seaborn\n",
    "import seaborn\n",
    "seaborn.set()\n",
    "seaborn.set_style(\"darkgrid\")\n",
    "\n",
    "# Project imports\n",
    "from encode_data import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Imports\n",
    "\n",
    "The preprocessing work of reading from the CSV file and storing into a pandas dataframe is just done once. We save the dataframes to save time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading raw_data from HDF5 cache\n",
      "Loading feature_df from HDF5 cache\n"
     ]
    }
   ],
   "source": [
    "# Get raw data\n",
    "if os.path.exists(\"../data/output/raw_data_legacy.hdf.gz\"):\n",
    "    print(\"Loading raw_data from HDF5 cache\")\n",
    "    raw_data = pandas.read_hdf(\"../data/output/raw_data_legacy.hdf.gz\", \"root\")\n",
    "else :\n",
    "    print(\"did not find raw_data cache\")\n",
    "    raw_data = get_raw_scdb_data(\"../data/input/SCDB_Legacy_01_justiceCentered_Citation.csv\")\n",
    "    raw_data.to_hdf(\"../data/output/raw_data_legacy.hdf.gz\", \"root\", complevel=6, complib=\"zlib\")\n",
    "\n",
    "# Get feature data\n",
    "if os.path.exists(\"../data/output/feature_data_legacy_no_FE.hdf.gz\"):\n",
    "    print(\"Loading feature_df from HDF5 cache\")\n",
    "    feature_df = pandas.read_hdf(\"../data/output/feature_data_legacy_no_FE.hdf.gz\", \"root\")\n",
    "else:\n",
    "    # Process\n",
    "    print(\"did not find feature_df cache\")\n",
    "    feature_df = preprocess_raw_data(raw_data, include_direction=True)\n",
    "    \n",
    "    # Write out feature datas\n",
    "    feature_df.to_hdf(\"../data/output/feature_data_legacy_no_FE.hdf.gz\", \"root\", complevel=6, complib=\"zlib\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Preprocess labels\n",
    "\n",
    "The outcome/target is the prediction done by individual justices. The predictions pertain to **affirm/reversal/no_opinion**. The *no_opinion* target is given a value of -1. We just change that to 2 in the next few blocks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30079\n",
      "116206\n",
      "101239\n",
      "30079\n",
      "116206\n",
      "101239\n"
     ]
    }
   ],
   "source": [
    "temp_case_row = raw_data[['case_outcome_disposition']].copy()\n",
    "print((temp_case_row['case_outcome_disposition'] == -1).astype(int).sum())\n",
    "print((temp_case_row['case_outcome_disposition'] == 0).astype(int).sum())\n",
    "print((temp_case_row['case_outcome_disposition'] == 1).astype(int).sum())\n",
    "temp_case_row.loc[raw_data['case_outcome_disposition'] == -1] = 2\n",
    "print((temp_case_row['case_outcome_disposition'] == 2).astype(int).sum())\n",
    "print((temp_case_row['case_outcome_disposition'] == 0).astype(int).sum())\n",
    "print((temp_case_row['case_outcome_disposition'] == 1).astype(int).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Downsample to float\n",
    "feature_df = feature_df.astype(numpy.float16)\n",
    "years = feature_df[['term_raw']]\n",
    "#years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove term\n",
    "nonterm_features = [f for f in feature_df.columns if not f.startswith(\"term_\")]\n",
    "original_feature_df = feature_df.copy()\n",
    "feature_df = original_feature_df.loc[:, nonterm_features].copy()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(249793, 64)\n",
      "(249793, 1480)\n"
     ]
    }
   ],
   "source": [
    "# Output some diagnostics on features\n",
    "'''\n",
    "this is for legacy database without any feature engineering\n",
    "(249793, 64)\n",
    "(249793, 1480)\n",
    "'''\n",
    "print(raw_data.shape)\n",
    "print(feature_df.shape)\n",
    "assert(raw_data.shape[0] == feature_df.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40508\n",
      "114780\n",
      "94505\n",
      "30079\n",
      "116206\n",
      "101239\n"
     ]
    }
   ],
   "source": [
    "print((raw_data['justice_outcome_disposition'] == -1).astype(int).sum())\n",
    "print((raw_data['justice_outcome_disposition'] == 0).astype(int).sum())\n",
    "print((raw_data['justice_outcome_disposition'] == 1).astype(int).sum())\n",
    "print((raw_data['case_outcome_disposition'] == -1).astype(int).sum())\n",
    "print((raw_data['case_outcome_disposition'] == 0).astype(int).sum())\n",
    "print((raw_data['case_outcome_disposition'] == 1).astype(int).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "raw_data.loc[raw_data['justice_outcome_disposition'] == -1] = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40508\n",
      "114780\n",
      "94505\n",
      "30079\n",
      "116206\n",
      "101239\n"
     ]
    }
   ],
   "source": [
    "print((raw_data['justice_outcome_disposition'] == 2).astype(int).sum())\n",
    "print((raw_data['justice_outcome_disposition'] == 0).astype(int).sum())\n",
    "print((raw_data['justice_outcome_disposition'] == 1).astype(int).sum())\n",
    "raw_data['case_outcome_disposition'] = temp_case_row['case_outcome_disposition']\n",
    "print((raw_data['case_outcome_disposition'] == 2).astype(int).sum())\n",
    "print((raw_data['case_outcome_disposition'] == 0).astype(int).sum())\n",
    "print((raw_data['case_outcome_disposition'] == 1).astype(int).sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## SPLIT DATA INTO TRAIN AND TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import sklearn.preprocessing as prep\n",
    "import time\n",
    "import math\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.utils import to_categorical\n",
    "import keras\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def preprocess_data(stock, seq_len):\n",
    "    amount_of_features = len(stock.columns)\n",
    "    data = stock.as_matrix()\n",
    "    \n",
    "    sequence_length = seq_len + 1\n",
    "    result = []\n",
    "    \n",
    "    for index in range(len(data) - sequence_length):\n",
    "        result.append(data[index : index + sequence_length])\n",
    "        \n",
    "    result = numpy.array(result)\n",
    "    row = round(0.8 * result.shape[0])\n",
    "    train = result[: int(row), :]\n",
    "\n",
    "    X_train = train[:, : -1]\n",
    "    y_train = train[:, -1][: ,-1]\n",
    "    X_test = result[int(row) :, : -1]\n",
    "    y_test = result[int(row) :, -1][ : ,-1]\n",
    "\n",
    "    X_train = numpy.reshape(X_train, (X_train.shape[0], X_train.shape[1], amount_of_features))\n",
    "    X_test = numpy.reshape(X_test, (X_test.shape[0], X_test.shape[1], amount_of_features))  \n",
    "\n",
    "    return [X_train, y_train, X_test, y_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(249793, 1480)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "feature_df = pandas.concat([feature_df, raw_data['justice_outcome_disposition']], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(249793, 1481)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "window = 1\n",
    "X_train, y_train, X_test, y_test = preprocess_data(feature_df[:: -1], window)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train (199833, 1, 1481)\n",
      "y_train (199833, 3)\n",
      "X_test (49958, 1, 1481)\n",
      "y_test (49958, 3)\n"
     ]
    }
   ],
   "source": [
    "y_train = to_categorical(y_train, num_classes=3)\n",
    "y_test = to_categorical(y_test, num_classes=3)\n",
    "\n",
    "print(\"X_train\", X_train.shape)\n",
    "print(\"y_train\", y_train.shape)\n",
    "print(\"X_test\", X_test.shape)\n",
    "print(\"y_test\", y_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "seq_len = 1\n",
    "shape = [feature_df.shape[1], seq_len] # feature, window\n",
    "neurons = [300, 300, 100, 3]\n",
    "epochs = 300\n",
    "d = 0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#from sklearn import SGD\n",
    "def build_model2(layers, neurons, d):\n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(LSTM(neurons[0], input_shape=(layers[1], layers[0]), return_sequences=True))\n",
    "    model.add(Dropout(d))\n",
    "        \n",
    "    model.add(LSTM(neurons[1], input_shape=(layers[1], layers[0]), return_sequences=False))\n",
    "    model.add(Dropout(d))\n",
    "    \n",
    "    model.add(Dense(neurons[2],kernel_initializer=\"uniform\",activation='relu'))        \n",
    "    model.add(Dense(neurons[3],kernel_initializer=\"uniform\",activation='softmax'))\n",
    "\n",
    "    rms = keras.optimizers.RMSprop(lr=0.03, rho=0.9, epsilon=1e-08, decay=0.02)\n",
    "    model.compile(loss='categorical_crossentropy',optimizer='rmsprop', metrics=['accuracy'])\n",
    "    model.summary()\n",
    "    print(\" Model build done!\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.459500\n",
       "1    0.378333\n",
       "2    0.162166\n",
       "Name: justice_outcome_disposition, dtype: float64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_data['justice_outcome_disposition'].value_counts() / len(raw_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing a few models against the truth values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 1\n",
    "\n",
    "##### neurons = [300 300 100 3]\n",
    "##### dropout = 0.3\n",
    "##### two LSTM and two dense layers\n",
    "#### optimizer =  rmsprop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_5 (LSTM)                (None, 1, 600)            4996800   \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 1, 600)            0         \n",
      "_________________________________________________________________\n",
      "lstm_6 (LSTM)                (None, 400)               1601600   \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 400)               0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 150)               60150     \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 3)                 453       \n",
      "=================================================================\n",
      "Total params: 6,659,003\n",
      "Trainable params: 6,659,003\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      " Model build done!\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_5 (LSTM)                (None, 1, 600)            4996800   \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 1, 600)            0         \n",
      "_________________________________________________________________\n",
      "lstm_6 (LSTM)                (None, 400)               1601600   \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 400)               0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 150)               60150     \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 3)                 453       \n",
      "=================================================================\n",
      "Total params: 6,659,003\n",
      "Trainable params: 6,659,003\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "seq_len = 1\n",
    "shape = [feature_df.shape[1], seq_len] # feature, window\n",
    "neurons = [600, 400, 150, 3]\n",
    "dropout_list = [0.1, 0.15, 0.2, 0.25, 0.3, 0.35, 0.4]\n",
    "model1 = build_model2(shape, neurons, 0.2)\n",
    "model1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_architecture_1_with_different_dropouts():\n",
    "    seq_len = 1\n",
    "    shape = [feature_df.shape[1], seq_len] # feature, window\n",
    "    neurons = [300, 300, 100, 3]\n",
    "    dropout_list = [0.1, 0.15, 0.2, 0.25, 0.3, 0.35, 0.4]\n",
    "    model_list = []\n",
    "    for dropout in dropout_list:\n",
    "        model2 = build_model2(shape, neurons, dropout)\n",
    "        model2.fit(\n",
    "        X_train,\n",
    "        y_train,\n",
    "        batch_size=512,\n",
    "        epochs=85,\n",
    "        validation_split=0.2,\n",
    "        shuffle = False,\n",
    "        class_weight = {0 : 1.2, 1 : 1, 2 : 1.1},\n",
    "        verbose=2)\n",
    "        curr_eval = model2.evaluate(X_test,y_test)\n",
    "        print(\" Dropout : \" + str(dropout) + \" accuracy is : \" + str(curr_eval[1]))\n",
    "        model_list.append(model2)\n",
    "    return model_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 1, 300)            2138400   \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 1, 300)            0         \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 300)               721200    \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 100)               30100     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 3)                 303       \n",
      "=================================================================\n",
      "Total params: 2,890,003\n",
      "Trainable params: 2,890,003\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      " Model build done!\n",
      "Train on 159866 samples, validate on 39967 samples\n",
      "Epoch 1/85\n",
      " - 9s - loss: 1.0451 - acc: 0.5371 - val_loss: 1.1041 - val_acc: 0.5333\n",
      "Epoch 2/85\n",
      " - 6s - loss: 1.0299 - acc: 0.5339 - val_loss: 0.9636 - val_acc: 0.5731\n",
      "Epoch 3/85\n",
      " - 6s - loss: 0.9818 - acc: 0.5557 - val_loss: 0.8714 - val_acc: 0.6571\n",
      "Epoch 4/85\n",
      " - 6s - loss: 0.9501 - acc: 0.5747 - val_loss: 0.8463 - val_acc: 0.6624\n",
      "Epoch 5/85\n",
      " - 6s - loss: 0.9224 - acc: 0.5898 - val_loss: 1.1992 - val_acc: 0.5465\n",
      "Epoch 6/85\n",
      " - 6s - loss: 0.9030 - acc: 0.6086 - val_loss: 0.9008 - val_acc: 0.6123\n",
      "Epoch 7/85\n",
      " - 6s - loss: 0.8789 - acc: 0.6260 - val_loss: 0.9529 - val_acc: 0.5886\n",
      "Epoch 8/85\n",
      " - 6s - loss: 0.8545 - acc: 0.6445 - val_loss: 0.6521 - val_acc: 0.8239\n",
      "Epoch 9/85\n",
      " - 6s - loss: 0.8272 - acc: 0.6649 - val_loss: 0.6435 - val_acc: 0.8206\n",
      "Epoch 10/85\n",
      " - 6s - loss: 0.8103 - acc: 0.6780 - val_loss: 0.8582 - val_acc: 0.7283\n",
      "Epoch 11/85\n",
      " - 6s - loss: 0.7908 - acc: 0.6927 - val_loss: 0.6271 - val_acc: 0.8372\n",
      "Epoch 12/85\n",
      " - 6s - loss: 0.7850 - acc: 0.6956 - val_loss: 0.6041 - val_acc: 0.8358\n",
      "Epoch 13/85\n",
      " - 6s - loss: 0.7700 - acc: 0.7060 - val_loss: 0.6217 - val_acc: 0.8342\n",
      "Epoch 14/85\n",
      " - 6s - loss: 0.7726 - acc: 0.7084 - val_loss: 0.7975 - val_acc: 0.7624\n",
      "Epoch 15/85\n",
      " - 6s - loss: 0.7596 - acc: 0.7174 - val_loss: 0.6836 - val_acc: 0.8025\n",
      "Epoch 16/85\n",
      " - 6s - loss: 0.7515 - acc: 0.7195 - val_loss: 0.8105 - val_acc: 0.7489\n",
      "Epoch 17/85\n",
      " - 6s - loss: 0.7392 - acc: 0.7235 - val_loss: 0.6563 - val_acc: 0.8231\n",
      "Epoch 18/85\n",
      " - 6s - loss: 0.7354 - acc: 0.7285 - val_loss: 0.6508 - val_acc: 0.8203\n",
      "Epoch 19/85\n",
      " - 6s - loss: 0.7321 - acc: 0.7310 - val_loss: 0.7838 - val_acc: 0.6998\n",
      "Epoch 20/85\n",
      " - 6s - loss: 0.7315 - acc: 0.7314 - val_loss: 0.5911 - val_acc: 0.8392\n",
      "Epoch 21/85\n",
      " - 6s - loss: 0.7205 - acc: 0.7379 - val_loss: 0.6663 - val_acc: 0.8126\n",
      "Epoch 22/85\n",
      " - 6s - loss: 0.7179 - acc: 0.7377 - val_loss: 0.5964 - val_acc: 0.8414\n",
      "Epoch 23/85\n",
      " - 6s - loss: 0.7180 - acc: 0.7406 - val_loss: 0.6469 - val_acc: 0.8303\n",
      "Epoch 24/85\n",
      " - 6s - loss: 0.7114 - acc: 0.7366 - val_loss: 0.7958 - val_acc: 0.7209\n",
      "Epoch 25/85\n",
      " - 6s - loss: 0.7051 - acc: 0.7439 - val_loss: 0.7363 - val_acc: 0.7824\n",
      "Epoch 26/85\n",
      " - 6s - loss: 0.7050 - acc: 0.7441 - val_loss: 0.5911 - val_acc: 0.8405\n",
      "Epoch 27/85\n",
      " - 6s - loss: 0.7007 - acc: 0.7437 - val_loss: 0.5737 - val_acc: 0.8463\n",
      "Epoch 28/85\n",
      " - 6s - loss: 0.6965 - acc: 0.7458 - val_loss: 0.5878 - val_acc: 0.8447\n",
      "Epoch 29/85\n",
      " - 6s - loss: 0.6912 - acc: 0.7493 - val_loss: 0.7441 - val_acc: 0.8279\n",
      "Epoch 30/85\n",
      " - 6s - loss: 0.6978 - acc: 0.7417 - val_loss: 0.6131 - val_acc: 0.8372\n",
      "Epoch 31/85\n",
      " - 6s - loss: 0.6888 - acc: 0.7489 - val_loss: 0.5700 - val_acc: 0.8432\n",
      "Epoch 32/85\n",
      " - 6s - loss: 0.6865 - acc: 0.7501 - val_loss: 0.6088 - val_acc: 0.8380\n",
      "Epoch 33/85\n",
      " - 6s - loss: 0.6814 - acc: 0.7521 - val_loss: 0.6520 - val_acc: 0.8357\n",
      "Epoch 34/85\n",
      " - 6s - loss: 0.6759 - acc: 0.7541 - val_loss: 0.5716 - val_acc: 0.8399\n",
      "Epoch 35/85\n",
      " - 6s - loss: 0.6823 - acc: 0.7507 - val_loss: 0.6717 - val_acc: 0.7192\n",
      "Epoch 36/85\n",
      " - 6s - loss: 0.6767 - acc: 0.7536 - val_loss: 0.5664 - val_acc: 0.8395\n",
      "Epoch 37/85\n",
      " - 6s - loss: 0.6700 - acc: 0.7559 - val_loss: 0.5680 - val_acc: 0.8404\n",
      "Epoch 38/85\n",
      " - 6s - loss: 0.6690 - acc: 0.7562 - val_loss: 0.5791 - val_acc: 0.8412\n",
      "Epoch 39/85\n",
      " - 6s - loss: 0.6729 - acc: 0.7551 - val_loss: 0.6029 - val_acc: 0.8322\n",
      "Epoch 40/85\n",
      " - 6s - loss: 0.6736 - acc: 0.7554 - val_loss: 0.5804 - val_acc: 0.8346\n",
      "Epoch 41/85\n",
      " - 6s - loss: 0.6679 - acc: 0.7579 - val_loss: 0.5966 - val_acc: 0.8356\n",
      "Epoch 42/85\n",
      " - 6s - loss: 0.6630 - acc: 0.7584 - val_loss: 0.6691 - val_acc: 0.7714\n",
      "Epoch 43/85\n",
      " - 6s - loss: 0.6697 - acc: 0.7574 - val_loss: 0.5981 - val_acc: 0.8410\n",
      "Epoch 44/85\n",
      " - 6s - loss: 0.6609 - acc: 0.7584 - val_loss: 0.6737 - val_acc: 0.7168\n",
      "Epoch 45/85\n",
      " - 6s - loss: 0.6618 - acc: 0.7588 - val_loss: 0.6056 - val_acc: 0.8081\n",
      "Epoch 46/85\n",
      " - 6s - loss: 0.6638 - acc: 0.7562 - val_loss: 0.7105 - val_acc: 0.7917\n",
      "Epoch 47/85\n",
      " - 6s - loss: 0.6568 - acc: 0.7607 - val_loss: 0.6565 - val_acc: 0.8266\n",
      "Epoch 48/85\n",
      " - 6s - loss: 0.6565 - acc: 0.7605 - val_loss: 0.6960 - val_acc: 0.8136\n",
      "Epoch 49/85\n",
      " - 6s - loss: 0.6572 - acc: 0.7570 - val_loss: 0.5669 - val_acc: 0.8390\n",
      "Epoch 50/85\n",
      " - 6s - loss: 0.6509 - acc: 0.7638 - val_loss: 0.7161 - val_acc: 0.7959\n",
      "Epoch 51/85\n",
      " - 6s - loss: 0.6561 - acc: 0.7579 - val_loss: 0.5806 - val_acc: 0.8302\n",
      "Epoch 52/85\n",
      " - 6s - loss: 0.6536 - acc: 0.7629 - val_loss: 0.5881 - val_acc: 0.8347\n",
      "Epoch 53/85\n",
      " - 6s - loss: 0.6521 - acc: 0.7618 - val_loss: 0.6126 - val_acc: 0.8283\n",
      "Epoch 54/85\n",
      " - 7s - loss: 0.6524 - acc: 0.7623 - val_loss: 0.6162 - val_acc: 0.8219\n",
      "Epoch 55/85\n",
      " - 7s - loss: 0.6520 - acc: 0.7612 - val_loss: 0.6194 - val_acc: 0.8314\n",
      "Epoch 56/85\n",
      " - 6s - loss: 0.6500 - acc: 0.7617 - val_loss: 0.5589 - val_acc: 0.8407\n",
      "Epoch 57/85\n",
      " - 6s - loss: 0.6484 - acc: 0.7640 - val_loss: 0.5531 - val_acc: 0.8462\n",
      "Epoch 58/85\n",
      " - 6s - loss: 0.6457 - acc: 0.7662 - val_loss: 0.6835 - val_acc: 0.8217\n",
      "Epoch 59/85\n",
      " - 6s - loss: 0.6442 - acc: 0.7660 - val_loss: 0.6096 - val_acc: 0.8199\n",
      "Epoch 60/85\n",
      " - 6s - loss: 0.6466 - acc: 0.7637 - val_loss: 0.6487 - val_acc: 0.8211\n",
      "Epoch 61/85\n",
      " - 6s - loss: 0.6441 - acc: 0.7653 - val_loss: 0.5777 - val_acc: 0.8321\n",
      "Epoch 62/85\n",
      " - 6s - loss: 0.6467 - acc: 0.7640 - val_loss: 0.6713 - val_acc: 0.7213\n",
      "Epoch 63/85\n",
      " - 6s - loss: 0.6439 - acc: 0.7648 - val_loss: 0.5673 - val_acc: 0.8420\n",
      "Epoch 64/85\n",
      " - 6s - loss: 0.6395 - acc: 0.7660 - val_loss: 0.5637 - val_acc: 0.8436\n",
      "Epoch 65/85\n",
      " - 6s - loss: 0.6409 - acc: 0.7656 - val_loss: 0.6373 - val_acc: 0.8224\n",
      "Epoch 66/85\n",
      " - 6s - loss: 0.6471 - acc: 0.7641 - val_loss: 0.5602 - val_acc: 0.8466\n",
      "Epoch 67/85\n",
      " - 6s - loss: 0.6382 - acc: 0.7675 - val_loss: 0.5850 - val_acc: 0.8482\n",
      "Epoch 68/85\n",
      " - 6s - loss: 0.6389 - acc: 0.7655 - val_loss: 0.5797 - val_acc: 0.8448\n",
      "Epoch 69/85\n",
      " - 6s - loss: 0.6383 - acc: 0.7670 - val_loss: 0.5730 - val_acc: 0.8377\n",
      "Epoch 70/85\n",
      " - 6s - loss: 0.6441 - acc: 0.7655 - val_loss: 0.6603 - val_acc: 0.8198\n",
      "Epoch 71/85\n",
      " - 6s - loss: 0.6402 - acc: 0.7668 - val_loss: 0.5683 - val_acc: 0.8422\n",
      "Epoch 72/85\n",
      " - 6s - loss: 0.6369 - acc: 0.7679 - val_loss: 0.5717 - val_acc: 0.8426\n",
      "Epoch 73/85\n",
      " - 6s - loss: 0.6386 - acc: 0.7681 - val_loss: 0.6201 - val_acc: 0.8201\n",
      "Epoch 74/85\n",
      " - 6s - loss: 0.6374 - acc: 0.7666 - val_loss: 0.5974 - val_acc: 0.8286\n",
      "Epoch 75/85\n",
      " - 6s - loss: 0.6365 - acc: 0.7690 - val_loss: 0.5555 - val_acc: 0.8427\n",
      "Epoch 76/85\n",
      " - 6s - loss: 0.6343 - acc: 0.7680 - val_loss: 0.5848 - val_acc: 0.8323\n",
      "Epoch 77/85\n",
      " - 6s - loss: 0.6365 - acc: 0.7684 - val_loss: 0.6009 - val_acc: 0.8331\n",
      "Epoch 78/85\n",
      " - 6s - loss: 0.6313 - acc: 0.7705 - val_loss: 0.5593 - val_acc: 0.8455\n",
      "Epoch 79/85\n",
      " - 6s - loss: 0.6323 - acc: 0.7698 - val_loss: 0.5820 - val_acc: 0.8346\n",
      "Epoch 80/85\n",
      " - 6s - loss: 0.6340 - acc: 0.7681 - val_loss: 0.5830 - val_acc: 0.8400\n",
      "Epoch 81/85\n",
      " - 6s - loss: 0.6352 - acc: 0.7697 - val_loss: 0.6699 - val_acc: 0.8122\n",
      "Epoch 82/85\n",
      " - 6s - loss: 0.6366 - acc: 0.7685 - val_loss: 0.6034 - val_acc: 0.8347\n",
      "Epoch 83/85\n",
      " - 6s - loss: 0.6349 - acc: 0.7713 - val_loss: 0.5892 - val_acc: 0.8441\n",
      "Epoch 84/85\n",
      " - 6s - loss: 0.6318 - acc: 0.7698 - val_loss: 0.6150 - val_acc: 0.8255\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 85/85\n",
      " - 6s - loss: 0.6345 - acc: 0.7688 - val_loss: 0.6631 - val_acc: 0.8000\n",
      "49958/49958 [==============================] - 5s 97us/step\n",
      " Dropout : 0.1 accuracy is : 0.772228672085\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_3 (LSTM)                (None, 1, 300)            2138400   \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 1, 300)            0         \n",
      "_________________________________________________________________\n",
      "lstm_4 (LSTM)                (None, 300)               721200    \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 100)               30100     \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 3)                 303       \n",
      "=================================================================\n",
      "Total params: 2,890,003\n",
      "Trainable params: 2,890,003\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      " Model build done!\n",
      "Train on 159866 samples, validate on 39967 samples\n",
      "Epoch 1/85\n",
      " - 7s - loss: 1.0448 - acc: 0.5379 - val_loss: 1.0991 - val_acc: 0.5333\n",
      "Epoch 2/85\n",
      " - 6s - loss: 1.0217 - acc: 0.5420 - val_loss: 1.1034 - val_acc: 0.5388\n",
      "Epoch 3/85\n",
      " - 6s - loss: 0.9764 - acc: 0.5600 - val_loss: 1.0705 - val_acc: 0.5494\n",
      "Epoch 4/85\n",
      " - 6s - loss: 0.9449 - acc: 0.5763 - val_loss: 0.8342 - val_acc: 0.6675\n",
      "Epoch 5/85\n",
      " - 6s - loss: 0.9123 - acc: 0.5978 - val_loss: 0.8338 - val_acc: 0.7243\n",
      "Epoch 6/85\n",
      " - 6s - loss: 0.8849 - acc: 0.6202 - val_loss: 0.7752 - val_acc: 0.7457\n",
      "Epoch 7/85\n",
      " - 6s - loss: 0.8568 - acc: 0.6382 - val_loss: 0.8978 - val_acc: 0.6262\n",
      "Epoch 8/85\n",
      " - 6s - loss: 0.8328 - acc: 0.6571 - val_loss: 0.7326 - val_acc: 0.7958\n",
      "Epoch 9/85\n",
      " - 6s - loss: 0.8148 - acc: 0.6720 - val_loss: 0.6600 - val_acc: 0.8275\n",
      "Epoch 10/85\n",
      " - 6s - loss: 0.7934 - acc: 0.6865 - val_loss: 0.7107 - val_acc: 0.7929\n",
      "Epoch 11/85\n",
      " - 6s - loss: 0.7812 - acc: 0.6979 - val_loss: 0.7462 - val_acc: 0.7773\n",
      "Epoch 12/85\n",
      " - 6s - loss: 0.7670 - acc: 0.7086 - val_loss: 0.7000 - val_acc: 0.7938\n",
      "Epoch 13/85\n",
      " - 6s - loss: 0.7626 - acc: 0.7093 - val_loss: 0.7503 - val_acc: 0.7822\n",
      "Epoch 14/85\n",
      " - 6s - loss: 0.7893 - acc: 0.6963 - val_loss: 0.6788 - val_acc: 0.8164\n",
      "Epoch 15/85\n",
      " - 6s - loss: 0.7479 - acc: 0.7230 - val_loss: 0.6233 - val_acc: 0.8300\n",
      "Epoch 16/85\n",
      " - 6s - loss: 0.6729 - acc: 0.7564 - val_loss: 0.5701 - val_acc: 0.8391\n",
      "Epoch 36/85\n",
      " - 6s - loss: 0.6721 - acc: 0.7563 - val_loss: 0.6293 - val_acc: 0.8448\n",
      "Epoch 37/85\n",
      " - 6s - loss: 0.6717 - acc: 0.7550 - val_loss: 0.5867 - val_acc: 0.8445\n",
      "Epoch 38/85\n",
      " - 6s - loss: 0.6668 - acc: 0.7589 - val_loss: 0.6081 - val_acc: 0.8334\n",
      "Epoch 39/85\n",
      " - 6s - loss: 0.6637 - acc: 0.7574 - val_loss: 0.6010 - val_acc: 0.8238\n",
      "Epoch 40/85\n",
      " - 6s - loss: 0.6631 - acc: 0.7598 - val_loss: 0.5597 - val_acc: 0.8430\n",
      "Epoch 41/85\n",
      " - 6s - loss: 0.6595 - acc: 0.7624 - val_loss: 0.6883 - val_acc: 0.7818\n",
      "Epoch 42/85\n",
      " - 6s - loss: 0.6589 - acc: 0.7608 - val_loss: 0.5612 - val_acc: 0.8429\n",
      "Epoch 43/85\n",
      " - 6s - loss: 0.6549 - acc: 0.7624 - val_loss: 0.5946 - val_acc: 0.8420\n",
      "Epoch 44/85\n",
      " - 6s - loss: 0.6568 - acc: 0.7604 - val_loss: 0.6987 - val_acc: 0.7812\n",
      "Epoch 45/85\n",
      " - 6s - loss: 0.6562 - acc: 0.7628 - val_loss: 0.5772 - val_acc: 0.8326\n",
      "Epoch 46/85\n",
      " - 6s - loss: 0.6521 - acc: 0.7636 - val_loss: 0.8401 - val_acc: 0.6435\n",
      "Epoch 47/85\n",
      " - 6s - loss: 0.6506 - acc: 0.7618 - val_loss: 0.5693 - val_acc: 0.8431\n",
      "Epoch 48/85\n",
      " - 6s - loss: 0.6447 - acc: 0.7656 - val_loss: 0.7445 - val_acc: 0.7250\n",
      "Epoch 49/85\n",
      " - 6s - loss: 0.6485 - acc: 0.7628 - val_loss: 0.5960 - val_acc: 0.8444\n",
      "Epoch 50/85\n",
      " - 6s - loss: 0.6469 - acc: 0.7654 - val_loss: 0.5588 - val_acc: 0.8468\n",
      "Epoch 51/85\n",
      " - 6s - loss: 0.6475 - acc: 0.7643 - val_loss: 0.6048 - val_acc: 0.8378\n",
      "Epoch 52/85\n",
      " - 6s - loss: 0.6408 - acc: 0.7681 - val_loss: 0.5943 - val_acc: 0.8417\n",
      "Epoch 53/85\n",
      " - 6s - loss: 0.6424 - acc: 0.7661 - val_loss: 0.5662 - val_acc: 0.8447\n",
      "Epoch 54/85\n",
      " - 6s - loss: 0.6443 - acc: 0.7649 - val_loss: 0.6006 - val_acc: 0.8406\n",
      "Epoch 55/85\n",
      " - 6s - loss: 0.6373 - acc: 0.7698 - val_loss: 0.5605 - val_acc: 0.8429\n",
      "Epoch 56/85\n",
      " - 6s - loss: 0.6372 - acc: 0.7673 - val_loss: 0.5995 - val_acc: 0.8264\n",
      "Epoch 57/85\n",
      " - 6s - loss: 0.6396 - acc: 0.7646 - val_loss: 0.5816 - val_acc: 0.8456\n",
      "Epoch 58/85\n",
      " - 6s - loss: 0.6386 - acc: 0.7670 - val_loss: 0.5671 - val_acc: 0.8500\n",
      "Epoch 59/85\n",
      " - 6s - loss: 0.6340 - acc: 0.7705 - val_loss: 0.5959 - val_acc: 0.8463\n",
      "Epoch 60/85\n",
      " - 6s - loss: 0.6332 - acc: 0.7700 - val_loss: 0.5751 - val_acc: 0.8428\n",
      "Epoch 61/85\n",
      " - 6s - loss: 0.6370 - acc: 0.7681 - val_loss: 0.6018 - val_acc: 0.8337\n",
      "Epoch 62/85\n",
      " - 6s - loss: 0.6338 - acc: 0.7683 - val_loss: 0.5833 - val_acc: 0.8429\n",
      "Epoch 63/85\n",
      " - 6s - loss: 0.6331 - acc: 0.7700 - val_loss: 0.5773 - val_acc: 0.8467\n",
      "Epoch 64/85\n",
      " - 6s - loss: 0.6345 - acc: 0.7691 - val_loss: 0.6341 - val_acc: 0.8420\n",
      "Epoch 65/85\n",
      " - 6s - loss: 0.6370 - acc: 0.7679 - val_loss: 0.5524 - val_acc: 0.8448\n",
      "Epoch 66/85\n",
      " - 6s - loss: 0.6300 - acc: 0.7712 - val_loss: 0.5636 - val_acc: 0.8477\n",
      "Epoch 67/85\n",
      " - 6s - loss: 0.6312 - acc: 0.7698 - val_loss: 0.7221 - val_acc: 0.7577\n",
      "Epoch 68/85\n",
      " - 6s - loss: 0.6304 - acc: 0.7703 - val_loss: 0.5790 - val_acc: 0.8364\n",
      "Epoch 69/85\n",
      " - 6s - loss: 0.6332 - acc: 0.7688 - val_loss: 0.5659 - val_acc: 0.8425\n",
      "Epoch 70/85\n",
      " - 6s - loss: 0.6282 - acc: 0.7726 - val_loss: 0.5737 - val_acc: 0.8471\n",
      "Epoch 71/85\n",
      " - 6s - loss: 0.6252 - acc: 0.7741 - val_loss: 0.6496 - val_acc: 0.7919\n",
      "Epoch 72/85\n",
      " - 6s - loss: 0.6261 - acc: 0.7729 - val_loss: 0.5578 - val_acc: 0.8477\n",
      "Epoch 73/85\n",
      " - 6s - loss: 0.6237 - acc: 0.7735 - val_loss: 0.5874 - val_acc: 0.8327\n",
      "Epoch 74/85\n",
      " - 6s - loss: 0.6293 - acc: 0.7672 - val_loss: 0.5468 - val_acc: 0.8491\n",
      "Epoch 75/85\n",
      " - 6s - loss: 0.6238 - acc: 0.7725 - val_loss: 0.5588 - val_acc: 0.8478\n",
      "Epoch 76/85\n",
      " - 6s - loss: 0.6243 - acc: 0.7722 - val_loss: 0.6252 - val_acc: 0.7830\n",
      "Epoch 77/85\n",
      " - 6s - loss: 0.6247 - acc: 0.7713 - val_loss: 0.7282 - val_acc: 0.7422\n",
      "Epoch 78/85\n",
      " - 6s - loss: 0.6254 - acc: 0.7724 - val_loss: 0.5981 - val_acc: 0.8470\n",
      "Epoch 79/85\n",
      " - 6s - loss: 0.6237 - acc: 0.7717 - val_loss: 0.5701 - val_acc: 0.8454\n",
      "Epoch 80/85\n",
      " - 6s - loss: 0.6223 - acc: 0.7728 - val_loss: 0.5891 - val_acc: 0.8478\n",
      "Epoch 81/85\n",
      " - 6s - loss: 0.6223 - acc: 0.7740 - val_loss: 0.5667 - val_acc: 0.8442\n",
      "Epoch 82/85\n",
      " - 6s - loss: 0.6218 - acc: 0.7744 - val_loss: 0.5691 - val_acc: 0.8451\n",
      "Epoch 83/85\n",
      " - 6s - loss: 0.6227 - acc: 0.7737 - val_loss: 0.6239 - val_acc: 0.7985\n",
      "Epoch 84/85\n",
      " - 6s - loss: 0.6212 - acc: 0.7735 - val_loss: 0.5567 - val_acc: 0.8462\n",
      "Epoch 85/85\n",
      " - 6s - loss: 0.6204 - acc: 0.7745 - val_loss: 0.5634 - val_acc: 0.8496\n",
      "49958/49958 [==============================] - 5s 93us/step\n",
      " Dropout : 0.15 accuracy is : 0.792185435766\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_5 (LSTM)                (None, 1, 300)            2138400   \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 1, 300)            0         \n",
      "_________________________________________________________________\n",
      "lstm_6 (LSTM)                (None, 300)               721200    \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 100)               30100     \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 3)                 303       \n",
      "=================================================================\n",
      "Total params: 2,890,003\n",
      "Trainable params: 2,890,003\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      " Model build done!\n",
      "Train on 159866 samples, validate on 39967 samples\n",
      "Epoch 1/85\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 7s - loss: 1.0457 - acc: 0.5371 - val_loss: 1.1058 - val_acc: 0.5333\n",
      "Epoch 2/85\n",
      " - 6s - loss: 1.0303 - acc: 0.5359 - val_loss: 0.9467 - val_acc: 0.6470\n",
      "Epoch 3/85\n",
      " - 6s - loss: 0.9770 - acc: 0.5604 - val_loss: 1.0934 - val_acc: 0.5496\n",
      "Epoch 4/85\n",
      " - 6s - loss: 0.9583 - acc: 0.5712 - val_loss: 0.9599 - val_acc: 0.6175\n",
      "Epoch 5/85\n",
      " - 6s - loss: 0.9266 - acc: 0.5867 - val_loss: 0.8308 - val_acc: 0.6635\n",
      "Epoch 6/85\n",
      " - 6s - loss: 0.9033 - acc: 0.5998 - val_loss: 1.0713 - val_acc: 0.5660\n",
      "Epoch 7/85\n",
      " - 6s - loss: 0.8916 - acc: 0.6116 - val_loss: 0.9893 - val_acc: 0.5724\n",
      "Epoch 8/85\n",
      " - 6s - loss: 0.8588 - acc: 0.6338 - val_loss: 0.6948 - val_acc: 0.7679\n",
      "Epoch 9/85\n",
      " - 6s - loss: 0.8433 - acc: 0.6484 - val_loss: 0.6289 - val_acc: 0.8313\n",
      "Epoch 10/85\n",
      " - 6s - loss: 0.8204 - acc: 0.6734 - val_loss: 0.9796 - val_acc: 0.6838\n",
      "Epoch 11/85\n",
      " - 6s - loss: 0.8106 - acc: 0.6789 - val_loss: 0.7224 - val_acc: 0.7891\n",
      "Epoch 12/85\n",
      " - 6s - loss: 0.7895 - acc: 0.6946 - val_loss: 0.7670 - val_acc: 0.7649\n",
      "Epoch 13/85\n",
      " - 6s - loss: 0.7841 - acc: 0.6984 - val_loss: 0.6221 - val_acc: 0.8304\n",
      "Epoch 14/85\n",
      " - 6s - loss: 0.7803 - acc: 0.7048 - val_loss: 0.5922 - val_acc: 0.8369\n",
      "Epoch 15/85\n",
      " - 6s - loss: 0.7647 - acc: 0.7139 - val_loss: 0.9923 - val_acc: 0.5168\n",
      "Epoch 16/85\n",
      " - 6s - loss: 0.7640 - acc: 0.7130 - val_loss: 0.5865 - val_acc: 0.8395\n",
      "Epoch 17/85\n",
      " - 6s - loss: 0.7605 - acc: 0.7128 - val_loss: 0.6410 - val_acc: 0.8263\n",
      "Epoch 18/85\n",
      " - 6s - loss: 0.7501 - acc: 0.7226 - val_loss: 1.3482 - val_acc: 0.4005\n",
      "Epoch 19/85\n",
      " - 6s - loss: 0.7433 - acc: 0.7278 - val_loss: 0.9431 - val_acc: 0.4388\n",
      "Epoch 20/85\n",
      " - 6s - loss: 0.7522 - acc: 0.7158 - val_loss: 0.6590 - val_acc: 0.8151\n",
      "Epoch 21/85\n",
      " - 6s - loss: 0.7410 - acc: 0.7298 - val_loss: 1.2632 - val_acc: 0.4252\n",
      "Epoch 22/85\n",
      " - 6s - loss: 0.7400 - acc: 0.7273 - val_loss: 0.6382 - val_acc: 0.8184\n",
      "Epoch 23/85\n",
      " - 6s - loss: 0.7336 - acc: 0.7313 - val_loss: 0.6066 - val_acc: 0.8379\n",
      "Epoch 24/85\n",
      " - 6s - loss: 0.7360 - acc: 0.7298 - val_loss: 0.7059 - val_acc: 0.8224\n",
      "Epoch 25/85\n",
      " - 6s - loss: 0.7324 - acc: 0.7323 - val_loss: 1.0269 - val_acc: 0.5543\n",
      "Epoch 26/85\n",
      " - 6s - loss: 0.7262 - acc: 0.7380 - val_loss: 0.5848 - val_acc: 0.8377\n",
      "Epoch 27/85\n",
      " - 6s - loss: 0.7267 - acc: 0.7352 - val_loss: 0.6607 - val_acc: 0.8129\n",
      "Epoch 28/85\n",
      " - 6s - loss: 0.7215 - acc: 0.7400 - val_loss: 0.5742 - val_acc: 0.8406\n",
      "Epoch 29/85\n",
      " - 6s - loss: 0.7230 - acc: 0.7370 - val_loss: 0.9060 - val_acc: 0.6431\n",
      "Epoch 30/85\n",
      " - 6s - loss: 0.7186 - acc: 0.7412 - val_loss: 0.6277 - val_acc: 0.8105\n",
      "Epoch 31/85\n",
      " - 6s - loss: 0.7189 - acc: 0.7362 - val_loss: 0.5757 - val_acc: 0.8390\n",
      "Epoch 32/85\n",
      " - 6s - loss: 0.7134 - acc: 0.7398 - val_loss: 0.8383 - val_acc: 0.6829\n",
      "Epoch 33/85\n",
      " - 6s - loss: 0.7103 - acc: 0.7459 - val_loss: 0.5817 - val_acc: 0.8399\n",
      "Epoch 34/85\n",
      " - 6s - loss: 0.7117 - acc: 0.7404 - val_loss: 0.7023 - val_acc: 0.7829\n",
      "Epoch 35/85\n",
      " - 6s - loss: 0.7117 - acc: 0.7447 - val_loss: 0.5730 - val_acc: 0.8384\n",
      "Epoch 36/85\n",
      " - 6s - loss: 0.7069 - acc: 0.7452 - val_loss: 0.7272 - val_acc: 0.8319\n",
      "Epoch 37/85\n",
      " - 6s - loss: 0.7065 - acc: 0.7453 - val_loss: 0.9495 - val_acc: 0.6242\n",
      "Epoch 38/85\n",
      " - 6s - loss: 0.7042 - acc: 0.7483 - val_loss: 0.6394 - val_acc: 0.8407\n",
      "Epoch 39/85\n",
      " - 6s - loss: 0.7031 - acc: 0.7488 - val_loss: 0.7067 - val_acc: 0.8267\n",
      "Epoch 40/85\n",
      " - 6s - loss: 0.7018 - acc: 0.7473 - val_loss: 0.6198 - val_acc: 0.8360\n",
      "Epoch 41/85\n",
      " - 6s - loss: 0.7003 - acc: 0.7484 - val_loss: 0.6278 - val_acc: 0.8302\n",
      "Epoch 42/85\n",
      " - 6s - loss: 0.7003 - acc: 0.7467 - val_loss: 0.6558 - val_acc: 0.8429\n",
      "Epoch 43/85\n",
      " - 6s - loss: 0.6968 - acc: 0.7504 - val_loss: 0.5743 - val_acc: 0.8419\n",
      "Epoch 44/85\n",
      " - 6s - loss: 0.6959 - acc: 0.7484 - val_loss: 0.6829 - val_acc: 0.8282\n",
      "Epoch 45/85\n",
      " - 6s - loss: 0.6950 - acc: 0.7499 - val_loss: 0.6333 - val_acc: 0.8380\n",
      "Epoch 46/85\n",
      " - 6s - loss: 0.6978 - acc: 0.7497 - val_loss: 0.5830 - val_acc: 0.8370\n",
      "Epoch 47/85\n",
      " - 6s - loss: 0.6990 - acc: 0.7468 - val_loss: 0.5838 - val_acc: 0.8363\n",
      "Epoch 48/85\n",
      " - 6s - loss: 0.6976 - acc: 0.7471 - val_loss: 0.6136 - val_acc: 0.8412\n",
      "Epoch 49/85\n",
      " - 6s - loss: 0.6920 - acc: 0.7525 - val_loss: 0.5893 - val_acc: 0.8345\n",
      "Epoch 50/85\n",
      " - 6s - loss: 0.6918 - acc: 0.7499 - val_loss: 0.6571 - val_acc: 0.8267\n",
      "Epoch 51/85\n",
      " - 6s - loss: 0.6927 - acc: 0.7521 - val_loss: 0.5804 - val_acc: 0.8366\n",
      "Epoch 52/85\n",
      " - 6s - loss: 0.6924 - acc: 0.7498 - val_loss: 0.7108 - val_acc: 0.7958\n",
      "Epoch 53/85\n",
      " - 6s - loss: 0.6872 - acc: 0.7543 - val_loss: 0.6027 - val_acc: 0.8401\n",
      "Epoch 54/85\n",
      " - 6s - loss: 0.6890 - acc: 0.7501 - val_loss: 0.6013 - val_acc: 0.8323\n",
      "Epoch 55/85\n",
      " - 6s - loss: 0.6899 - acc: 0.7499 - val_loss: 0.6073 - val_acc: 0.8347\n",
      "Epoch 56/85\n",
      " - 6s - loss: 0.6845 - acc: 0.7541 - val_loss: 0.6453 - val_acc: 0.8371\n",
      "Epoch 57/85\n",
      " - 6s - loss: 0.6866 - acc: 0.7530 - val_loss: 0.6701 - val_acc: 0.8156\n",
      "Epoch 58/85\n",
      " - 6s - loss: 0.6823 - acc: 0.7552 - val_loss: 0.5684 - val_acc: 0.8443\n",
      "Epoch 59/85\n",
      " - 6s - loss: 0.6914 - acc: 0.7480 - val_loss: 0.6994 - val_acc: 0.8095\n",
      "Epoch 60/85\n",
      " - 6s - loss: 0.6854 - acc: 0.7542 - val_loss: 0.5655 - val_acc: 0.8434\n",
      "Epoch 61/85\n",
      " - 6s - loss: 0.6821 - acc: 0.7534 - val_loss: 0.6191 - val_acc: 0.8331\n",
      "Epoch 62/85\n",
      " - 6s - loss: 0.6839 - acc: 0.7535 - val_loss: 0.6057 - val_acc: 0.8308\n",
      "Epoch 63/85\n",
      " - 6s - loss: 0.6838 - acc: 0.7530 - val_loss: 0.6054 - val_acc: 0.8384\n",
      "Epoch 64/85\n",
      " - 6s - loss: 0.6824 - acc: 0.7540 - val_loss: 0.6360 - val_acc: 0.8372\n",
      "Epoch 65/85\n",
      " - 6s - loss: 0.6830 - acc: 0.7554 - val_loss: 0.6312 - val_acc: 0.8381\n",
      "Epoch 66/85\n",
      " - 6s - loss: 0.6807 - acc: 0.7538 - val_loss: 0.6580 - val_acc: 0.8441\n",
      "Epoch 67/85\n",
      " - 6s - loss: 0.6835 - acc: 0.7550 - val_loss: 0.6405 - val_acc: 0.7897\n",
      "Epoch 68/85\n",
      " - 6s - loss: 0.6842 - acc: 0.7541 - val_loss: 0.6585 - val_acc: 0.8412\n",
      "Epoch 69/85\n",
      " - 6s - loss: 0.6791 - acc: 0.7563 - val_loss: 0.6403 - val_acc: 0.8125\n",
      "Epoch 70/85\n",
      " - 6s - loss: 0.6801 - acc: 0.7553 - val_loss: 0.6412 - val_acc: 0.8334\n",
      "Epoch 71/85\n",
      " - 6s - loss: 0.6767 - acc: 0.7589 - val_loss: 0.6661 - val_acc: 0.8324\n",
      "Epoch 72/85\n",
      " - 6s - loss: 0.6750 - acc: 0.7582 - val_loss: 0.5782 - val_acc: 0.8398\n",
      "Epoch 73/85\n",
      " - 6s - loss: 0.6800 - acc: 0.7575 - val_loss: 0.5820 - val_acc: 0.8388\n",
      "Epoch 74/85\n",
      " - 6s - loss: 0.6788 - acc: 0.7557 - val_loss: 0.6420 - val_acc: 0.8415\n",
      "Epoch 75/85\n",
      " - 6s - loss: 0.6829 - acc: 0.7560 - val_loss: 0.5725 - val_acc: 0.8375\n",
      "Epoch 76/85\n",
      " - 6s - loss: 0.6775 - acc: 0.7585 - val_loss: 0.6538 - val_acc: 0.8451\n",
      "Epoch 77/85\n",
      " - 6s - loss: 0.6777 - acc: 0.7574 - val_loss: 0.5634 - val_acc: 0.8434\n",
      "Epoch 78/85\n",
      " - 6s - loss: 0.6758 - acc: 0.7572 - val_loss: 0.7123 - val_acc: 0.7850\n",
      "Epoch 79/85\n",
      " - 6s - loss: 0.6775 - acc: 0.7546 - val_loss: 0.5729 - val_acc: 0.8402\n",
      "Epoch 80/85\n",
      " - 6s - loss: 0.6756 - acc: 0.7570 - val_loss: 0.6017 - val_acc: 0.8258\n",
      "Epoch 81/85\n",
      " - 6s - loss: 0.6740 - acc: 0.7582 - val_loss: 0.5966 - val_acc: 0.8412\n",
      "Epoch 82/85\n",
      " - 6s - loss: 0.6755 - acc: 0.7576 - val_loss: 0.6198 - val_acc: 0.8325\n",
      "Epoch 83/85\n",
      " - 6s - loss: 0.6724 - acc: 0.7595 - val_loss: 0.6273 - val_acc: 0.8326\n",
      "Epoch 84/85\n",
      " - 6s - loss: 0.6760 - acc: 0.7558 - val_loss: 0.5926 - val_acc: 0.8284\n",
      "Epoch 85/85\n",
      " - 6s - loss: 0.6729 - acc: 0.7580 - val_loss: 0.6314 - val_acc: 0.8116\n",
      "49958/49958 [==============================] - 5s 94us/step\n",
      " Dropout : 0.2 accuracy is : 0.728371832339\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_7 (LSTM)                (None, 1, 300)            2138400   \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 1, 300)            0         \n",
      "_________________________________________________________________\n",
      "lstm_8 (LSTM)                (None, 300)               721200    \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 100)               30100     \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 3)                 303       \n",
      "=================================================================\n",
      "Total params: 2,890,003\n",
      "Trainable params: 2,890,003\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      " Model build done!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 159866 samples, validate on 39967 samples\n",
      "Epoch 1/85\n",
      " - 7s - loss: 1.0452 - acc: 0.5372 - val_loss: 1.0994 - val_acc: 0.5333\n",
      "Epoch 2/85\n",
      " - 6s - loss: 1.0255 - acc: 0.5375 - val_loss: 0.9666 - val_acc: 0.6410\n",
      "Epoch 3/85\n",
      " - 6s - loss: 0.9810 - acc: 0.5601 - val_loss: 0.8782 - val_acc: 0.6492\n",
      "Epoch 4/85\n",
      " - 6s - loss: 0.9519 - acc: 0.5716 - val_loss: 0.9597 - val_acc: 0.5974\n",
      "Epoch 5/85\n",
      " - 6s - loss: 0.9235 - acc: 0.5891 - val_loss: 1.1467 - val_acc: 0.5518\n",
      "Epoch 6/85\n",
      " - 6s - loss: 0.8991 - acc: 0.6106 - val_loss: 0.8094 - val_acc: 0.7676\n",
      "Epoch 7/85\n",
      " - 6s - loss: 0.8657 - acc: 0.6317 - val_loss: 0.7243 - val_acc: 0.7891\n",
      "Epoch 8/85\n",
      " - 6s - loss: 0.8481 - acc: 0.6435 - val_loss: 0.6601 - val_acc: 0.8264\n",
      "Epoch 9/85\n",
      " - 6s - loss: 0.8288 - acc: 0.6614 - val_loss: 0.8065 - val_acc: 0.7561\n",
      "Epoch 10/85\n",
      " - 6s - loss: 0.8116 - acc: 0.6782 - val_loss: 0.6514 - val_acc: 0.8300\n",
      "Epoch 11/85\n",
      " - 6s - loss: 0.8027 - acc: 0.6835 - val_loss: 0.6258 - val_acc: 0.8277\n",
      "Epoch 12/85\n",
      " - 6s - loss: 0.7821 - acc: 0.6995 - val_loss: 0.6322 - val_acc: 0.8286\n",
      "Epoch 13/85\n",
      " - 6s - loss: 0.7787 - acc: 0.7009 - val_loss: 1.2471 - val_acc: 0.4176\n",
      "Epoch 14/85\n",
      " - 6s - loss: 0.7695 - acc: 0.7082 - val_loss: 0.8001 - val_acc: 0.7163\n",
      "Epoch 15/85\n",
      " - 6s - loss: 0.7679 - acc: 0.7085 - val_loss: 0.6664 - val_acc: 0.8160\n",
      "Epoch 16/85\n",
      " - 6s - loss: 0.7542 - acc: 0.7196 - val_loss: 0.8113 - val_acc: 0.7422\n",
      "Epoch 17/85\n",
      " - 6s - loss: 0.7594 - acc: 0.7146 - val_loss: 0.6069 - val_acc: 0.8309\n",
      "Epoch 18/85\n",
      " - 6s - loss: 0.7662 - acc: 0.7053 - val_loss: 0.6157 - val_acc: 0.8300\n",
      "Epoch 19/85\n",
      " - 7s - loss: 0.7520 - acc: 0.7191 - val_loss: 0.5891 - val_acc: 0.8365\n",
      "Epoch 20/85\n",
      " - 6s - loss: 0.7514 - acc: 0.7103 - val_loss: 0.7369 - val_acc: 0.7564\n",
      "Epoch 21/85\n",
      " - 6s - loss: 0.7440 - acc: 0.7266 - val_loss: 0.6650 - val_acc: 0.8066\n",
      "Epoch 22/85\n",
      " - 6s - loss: 0.7429 - acc: 0.7192 - val_loss: 0.5885 - val_acc: 0.8362\n",
      "Epoch 23/85\n",
      " - 6s - loss: 0.7415 - acc: 0.7190 - val_loss: 0.8713 - val_acc: 0.6699\n",
      "Epoch 24/85\n",
      " - 6s - loss: 0.7360 - acc: 0.7300 - val_loss: 0.6223 - val_acc: 0.8235\n",
      "Epoch 25/85\n",
      " - 6s - loss: 0.7480 - acc: 0.7113 - val_loss: 0.6357 - val_acc: 0.8226\n",
      "Epoch 26/85\n",
      " - 6s - loss: 0.7343 - acc: 0.7230 - val_loss: 0.5962 - val_acc: 0.8370\n",
      "Epoch 27/85\n",
      " - 6s - loss: 0.7334 - acc: 0.7239 - val_loss: 0.6512 - val_acc: 0.8226\n",
      "Epoch 28/85\n",
      " - 6s - loss: 0.7201 - acc: 0.7401 - val_loss: 1.0137 - val_acc: 0.5751\n",
      "Epoch 29/85\n",
      " - 6s - loss: 0.7200 - acc: 0.7382 - val_loss: 0.6068 - val_acc: 0.8376\n",
      "Epoch 30/85\n",
      " - 7s - loss: 0.7181 - acc: 0.7405 - val_loss: 0.8779 - val_acc: 0.6653\n",
      "Epoch 31/85\n",
      " - 7s - loss: 0.7171 - acc: 0.7411 - val_loss: 0.5705 - val_acc: 0.8383\n",
      "Epoch 32/85\n",
      " - 7s - loss: 0.7164 - acc: 0.7410 - val_loss: 0.6085 - val_acc: 0.8371\n",
      "Epoch 33/85\n",
      " - 6s - loss: 0.7198 - acc: 0.7380 - val_loss: 0.7318 - val_acc: 0.8081\n",
      "Epoch 34/85\n",
      " - 6s - loss: 0.7322 - acc: 0.7161 - val_loss: 0.7635 - val_acc: 0.7388\n",
      "Epoch 35/85\n",
      " - 7s - loss: 0.7098 - acc: 0.7424 - val_loss: 1.0883 - val_acc: 0.5388\n",
      "Epoch 36/85\n",
      " - 7s - loss: 0.7077 - acc: 0.7445 - val_loss: 0.5823 - val_acc: 0.8395\n",
      "Epoch 37/85\n",
      " - 6s - loss: 0.7118 - acc: 0.7442 - val_loss: 0.5675 - val_acc: 0.8401\n",
      "Epoch 38/85\n",
      " - 6s - loss: 0.7042 - acc: 0.7469 - val_loss: 0.5723 - val_acc: 0.8379\n",
      "Epoch 39/85\n",
      " - 6s - loss: 0.7081 - acc: 0.7438 - val_loss: 0.5827 - val_acc: 0.8421\n",
      "Epoch 40/85\n",
      " - 7s - loss: 0.7055 - acc: 0.7434 - val_loss: 1.1022 - val_acc: 0.5465\n",
      "Epoch 41/85\n",
      " - 6s - loss: 0.7034 - acc: 0.7474 - val_loss: 0.5617 - val_acc: 0.8431\n",
      "Epoch 42/85\n",
      " - 6s - loss: 0.6990 - acc: 0.7495 - val_loss: 0.5781 - val_acc: 0.8470\n",
      "Epoch 43/85\n",
      " - 6s - loss: 0.7064 - acc: 0.7403 - val_loss: 0.5589 - val_acc: 0.8466\n",
      "Epoch 44/85\n",
      " - 6s - loss: 0.6996 - acc: 0.7481 - val_loss: 0.6739 - val_acc: 0.8063\n",
      "Epoch 45/85\n",
      " - 6s - loss: 0.6992 - acc: 0.7458 - val_loss: 0.6175 - val_acc: 0.8017\n",
      "Epoch 46/85\n",
      " - 6s - loss: 0.7002 - acc: 0.7475 - val_loss: 0.5932 - val_acc: 0.8315\n",
      "Epoch 47/85\n",
      " - 6s - loss: 0.6965 - acc: 0.7502 - val_loss: 1.0648 - val_acc: 0.5923\n",
      "Epoch 48/85\n",
      " - 6s - loss: 0.6934 - acc: 0.7510 - val_loss: 0.6374 - val_acc: 0.8246\n",
      "Epoch 49/85\n",
      " - 6s - loss: 0.6950 - acc: 0.7496 - val_loss: 0.5635 - val_acc: 0.8457\n",
      "Epoch 50/85\n",
      " - 6s - loss: 0.6910 - acc: 0.7537 - val_loss: 0.6418 - val_acc: 0.8142\n",
      "Epoch 51/85\n",
      " - 6s - loss: 0.6931 - acc: 0.7498 - val_loss: 0.5895 - val_acc: 0.8353\n",
      "Epoch 52/85\n",
      " - 6s - loss: 0.6897 - acc: 0.7536 - val_loss: 1.1947 - val_acc: 0.5423\n",
      "Epoch 53/85\n",
      " - 7s - loss: 0.6867 - acc: 0.7539 - val_loss: 0.6068 - val_acc: 0.8332\n",
      "Epoch 54/85\n",
      " - 7s - loss: 0.6876 - acc: 0.7541 - val_loss: 0.6099 - val_acc: 0.8248\n",
      "Epoch 55/85\n",
      " - 7s - loss: 0.6978 - acc: 0.7463 - val_loss: 0.6952 - val_acc: 0.8199\n",
      "Epoch 56/85\n",
      " - 6s - loss: 0.6874 - acc: 0.7513 - val_loss: 0.5543 - val_acc: 0.8397\n",
      "Epoch 57/85\n",
      " - 6s - loss: 0.6843 - acc: 0.7555 - val_loss: 0.5613 - val_acc: 0.8459\n",
      "Epoch 58/85\n",
      " - 6s - loss: 0.6827 - acc: 0.7551 - val_loss: 0.5716 - val_acc: 0.8432\n",
      "Epoch 59/85\n",
      " - 7s - loss: 0.6835 - acc: 0.7546 - val_loss: 0.5475 - val_acc: 0.8483\n",
      "Epoch 60/85\n",
      " - 6s - loss: 0.6826 - acc: 0.7561 - val_loss: 0.5659 - val_acc: 0.8442\n",
      "Epoch 61/85\n",
      " - 7s - loss: 0.6845 - acc: 0.7529 - val_loss: 0.6047 - val_acc: 0.8423\n",
      "Epoch 62/85\n",
      " - 6s - loss: 0.6830 - acc: 0.7545 - val_loss: 0.6116 - val_acc: 0.8455\n",
      "Epoch 63/85\n",
      " - 6s - loss: 0.6824 - acc: 0.7546 - val_loss: 0.7301 - val_acc: 0.7969\n",
      "Epoch 64/85\n",
      " - 7s - loss: 0.6821 - acc: 0.7531 - val_loss: 0.6317 - val_acc: 0.8451\n",
      "Epoch 65/85\n",
      " - 7s - loss: 0.6793 - acc: 0.7556 - val_loss: 0.5805 - val_acc: 0.8460\n",
      "Epoch 66/85\n",
      " - 7s - loss: 0.6795 - acc: 0.7568 - val_loss: 0.5695 - val_acc: 0.8432\n",
      "Epoch 67/85\n",
      " - 6s - loss: 0.6760 - acc: 0.7577 - val_loss: 0.7870 - val_acc: 0.7898\n",
      "Epoch 68/85\n",
      " - 6s - loss: 0.6804 - acc: 0.7534 - val_loss: 0.6910 - val_acc: 0.7977\n",
      "Epoch 69/85\n",
      " - 6s - loss: 0.6792 - acc: 0.7555 - val_loss: 0.5492 - val_acc: 0.8459\n",
      "Epoch 70/85\n",
      " - 6s - loss: 0.6786 - acc: 0.7576 - val_loss: 0.5848 - val_acc: 0.8469\n",
      "Epoch 71/85\n",
      " - 6s - loss: 0.6832 - acc: 0.7505 - val_loss: 0.5617 - val_acc: 0.8481\n",
      "Epoch 72/85\n",
      " - 6s - loss: 0.6752 - acc: 0.7583 - val_loss: 0.5564 - val_acc: 0.8472\n",
      "Epoch 73/85\n",
      " - 6s - loss: 0.6742 - acc: 0.7579 - val_loss: 0.6039 - val_acc: 0.8419\n",
      "Epoch 74/85\n",
      " - 6s - loss: 0.6794 - acc: 0.7528 - val_loss: 0.5859 - val_acc: 0.8429\n",
      "Epoch 75/85\n",
      " - 7s - loss: 0.6739 - acc: 0.7582 - val_loss: 0.5528 - val_acc: 0.8450\n",
      "Epoch 76/85\n",
      " - 7s - loss: 0.6750 - acc: 0.7584 - val_loss: 0.6393 - val_acc: 0.8389\n",
      "Epoch 77/85\n",
      " - 7s - loss: 0.6729 - acc: 0.7574 - val_loss: 0.5418 - val_acc: 0.8463\n",
      "Epoch 78/85\n",
      " - 6s - loss: 0.6777 - acc: 0.7563 - val_loss: 0.6460 - val_acc: 0.8305\n",
      "Epoch 79/85\n",
      " - 6s - loss: 0.6749 - acc: 0.7582 - val_loss: 0.6006 - val_acc: 0.8278\n",
      "Epoch 80/85\n",
      " - 6s - loss: 0.6712 - acc: 0.7592 - val_loss: 0.6072 - val_acc: 0.8324\n",
      "Epoch 81/85\n",
      " - 6s - loss: 0.6699 - acc: 0.7597 - val_loss: 0.5604 - val_acc: 0.8473\n",
      "Epoch 82/85\n",
      " - 6s - loss: 0.6745 - acc: 0.7581 - val_loss: 0.5686 - val_acc: 0.8430\n",
      "Epoch 83/85\n",
      " - 6s - loss: 0.6728 - acc: 0.7587 - val_loss: 0.7394 - val_acc: 0.7793\n",
      "Epoch 84/85\n",
      " - 6s - loss: 0.6700 - acc: 0.7596 - val_loss: 0.5599 - val_acc: 0.8404\n",
      "Epoch 85/85\n",
      " - 6s - loss: 0.6766 - acc: 0.7544 - val_loss: 0.5965 - val_acc: 0.8440\n",
      "49958/49958 [==============================] - 5s 101us/step\n",
      " Dropout : 0.25 accuracy is : 0.704291604948\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_9 (LSTM)                (None, 1, 300)            2138400   \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 1, 300)            0         \n",
      "_________________________________________________________________\n",
      "lstm_10 (LSTM)               (None, 300)               721200    \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 100)               30100     \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 3)                 303       \n",
      "=================================================================\n",
      "Total params: 2,890,003\n",
      "Trainable params: 2,890,003\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      " Model build done!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 159866 samples, validate on 39967 samples\n",
      "Epoch 1/85\n",
      " - 8s - loss: 1.0458 - acc: 0.5372 - val_loss: 1.1075 - val_acc: 0.5333\n",
      "Epoch 2/85\n",
      " - 6s - loss: 1.0356 - acc: 0.5328 - val_loss: 1.0295 - val_acc: 0.6382\n",
      "Epoch 3/85\n",
      " - 6s - loss: 0.9850 - acc: 0.5583 - val_loss: 1.3099 - val_acc: 0.4885\n",
      "Epoch 4/85\n",
      " - 6s - loss: 0.9636 - acc: 0.5666 - val_loss: 0.8779 - val_acc: 0.6509\n",
      "Epoch 5/85\n",
      " - 6s - loss: 0.9263 - acc: 0.5860 - val_loss: 0.8267 - val_acc: 0.6992\n",
      "Epoch 6/85\n",
      " - 6s - loss: 0.8992 - acc: 0.6044 - val_loss: 0.7881 - val_acc: 0.7453\n",
      "Epoch 7/85\n",
      " - 6s - loss: 0.8769 - acc: 0.6223 - val_loss: 0.8140 - val_acc: 0.7122\n",
      "Epoch 8/85\n",
      " - 6s - loss: 0.8613 - acc: 0.6340 - val_loss: 0.8873 - val_acc: 0.7105\n",
      "Epoch 9/85\n",
      " - 7s - loss: 0.8357 - acc: 0.6548 - val_loss: 0.6408 - val_acc: 0.8296\n",
      "Epoch 10/85\n",
      " - 6s - loss: 0.8226 - acc: 0.6694 - val_loss: 0.6395 - val_acc: 0.8261\n",
      "Epoch 11/85\n",
      " - 7s - loss: 0.8146 - acc: 0.6766 - val_loss: 0.6232 - val_acc: 0.8337\n",
      "Epoch 12/85\n",
      " - 7s - loss: 0.7911 - acc: 0.6903 - val_loss: 0.6037 - val_acc: 0.8336\n",
      "Epoch 13/85\n",
      " - 6s - loss: 0.7884 - acc: 0.6933 - val_loss: 0.5990 - val_acc: 0.8370\n",
      "Epoch 14/85\n",
      " - 6s - loss: 0.7724 - acc: 0.7055 - val_loss: 0.6659 - val_acc: 0.8094\n",
      "Epoch 15/85\n",
      " - 6s - loss: 0.8033 - acc: 0.6802 - val_loss: 0.6969 - val_acc: 0.8008\n",
      "Epoch 16/85\n",
      " - 6s - loss: 0.7593 - acc: 0.7125 - val_loss: 0.5872 - val_acc: 0.8380\n",
      "Epoch 17/85\n",
      " - 6s - loss: 0.7604 - acc: 0.7153 - val_loss: 0.5921 - val_acc: 0.8384\n",
      "Epoch 18/85\n",
      " - 6s - loss: 0.7459 - acc: 0.7266 - val_loss: 0.5847 - val_acc: 0.8403\n",
      "Epoch 19/85\n",
      " - 7s - loss: 0.7455 - acc: 0.7258 - val_loss: 0.7171 - val_acc: 0.8157\n",
      "Epoch 20/85\n",
      " - 6s - loss: 0.7399 - acc: 0.7269 - val_loss: 0.6063 - val_acc: 0.8344\n",
      "Epoch 21/85\n",
      " - 7s - loss: 0.7349 - acc: 0.7300 - val_loss: 0.7731 - val_acc: 0.7332\n",
      "Epoch 22/85\n",
      " - 7s - loss: 0.7376 - acc: 0.7294 - val_loss: 0.5838 - val_acc: 0.8398\n",
      "Epoch 23/85\n",
      " - 7s - loss: 0.7275 - acc: 0.7368 - val_loss: 0.6564 - val_acc: 0.8233\n",
      "Epoch 24/85\n",
      " - 7s - loss: 0.7282 - acc: 0.7353 - val_loss: 0.6359 - val_acc: 0.8355\n",
      "Epoch 25/85\n",
      " - 7s - loss: 0.7226 - acc: 0.7389 - val_loss: 0.6216 - val_acc: 0.8247\n",
      "Epoch 26/85\n",
      " - 7s - loss: 0.7206 - acc: 0.7373 - val_loss: 0.5838 - val_acc: 0.8408\n",
      "Epoch 27/85\n",
      " - 7s - loss: 0.7210 - acc: 0.7406 - val_loss: 0.5772 - val_acc: 0.8387\n",
      "Epoch 28/85\n",
      " - 7s - loss: 0.7217 - acc: 0.7312 - val_loss: 0.5720 - val_acc: 0.8439\n",
      "Epoch 29/85\n",
      " - 7s - loss: 0.7151 - acc: 0.7436 - val_loss: 0.6202 - val_acc: 0.8391\n",
      "Epoch 30/85\n",
      " - 7s - loss: 0.7164 - acc: 0.7405 - val_loss: 0.5702 - val_acc: 0.8347\n",
      "Epoch 31/85\n",
      " - 7s - loss: 0.7113 - acc: 0.7425 - val_loss: 0.6396 - val_acc: 0.8211\n",
      "Epoch 32/85\n",
      " - 6s - loss: 0.7151 - acc: 0.7434 - val_loss: 0.5892 - val_acc: 0.8423\n",
      "Epoch 33/85\n",
      " - 7s - loss: 0.7069 - acc: 0.7445 - val_loss: 0.5858 - val_acc: 0.8409\n",
      "Epoch 34/85\n",
      " - 7s - loss: 0.7074 - acc: 0.7470 - val_loss: 0.5719 - val_acc: 0.8429\n",
      "Epoch 35/85\n",
      " - 7s - loss: 0.7038 - acc: 0.7460 - val_loss: 0.6178 - val_acc: 0.8340\n",
      "Epoch 36/85\n",
      " - 7s - loss: 0.6985 - acc: 0.7493 - val_loss: 0.5986 - val_acc: 0.8426\n",
      "Epoch 37/85\n",
      " - 7s - loss: 0.6983 - acc: 0.7490 - val_loss: 0.7614 - val_acc: 0.7414\n",
      "Epoch 38/85\n",
      " - 7s - loss: 0.6995 - acc: 0.7473 - val_loss: 0.5858 - val_acc: 0.8437\n",
      "Epoch 39/85\n",
      " - 7s - loss: 0.6943 - acc: 0.7516 - val_loss: 0.5726 - val_acc: 0.8417\n",
      "Epoch 40/85\n",
      " - 7s - loss: 0.6969 - acc: 0.7471 - val_loss: 0.5652 - val_acc: 0.8455\n",
      "Epoch 41/85\n",
      " - 6s - loss: 0.6916 - acc: 0.7517 - val_loss: 0.5635 - val_acc: 0.8443\n",
      "Epoch 42/85\n",
      " - 6s - loss: 0.6897 - acc: 0.7543 - val_loss: 0.5787 - val_acc: 0.8405\n",
      "Epoch 43/85\n",
      " - 7s - loss: 0.6936 - acc: 0.7503 - val_loss: 0.5631 - val_acc: 0.8432\n",
      "Epoch 44/85\n",
      " - 6s - loss: 0.6889 - acc: 0.7516 - val_loss: 0.5507 - val_acc: 0.8450\n",
      "Epoch 45/85\n",
      " - 6s - loss: 0.6914 - acc: 0.7512 - val_loss: 0.5529 - val_acc: 0.8465\n",
      "Epoch 46/85\n",
      " - 6s - loss: 0.6844 - acc: 0.7561 - val_loss: 0.9419 - val_acc: 0.5501\n",
      "Epoch 47/85\n",
      " - 6s - loss: 0.6896 - acc: 0.7508 - val_loss: 0.5631 - val_acc: 0.8419\n",
      "Epoch 48/85\n",
      " - 6s - loss: 0.6866 - acc: 0.7538 - val_loss: 0.5754 - val_acc: 0.8373\n",
      "Epoch 49/85\n",
      " - 6s - loss: 0.6849 - acc: 0.7531 - val_loss: 0.6059 - val_acc: 0.8414\n",
      "Epoch 50/85\n",
      " - 6s - loss: 0.6839 - acc: 0.7552 - val_loss: 0.5834 - val_acc: 0.8318\n",
      "Epoch 51/85\n",
      " - 6s - loss: 0.6814 - acc: 0.7553 - val_loss: 0.5594 - val_acc: 0.8438\n",
      "Epoch 52/85\n",
      " - 7s - loss: 0.6853 - acc: 0.7516 - val_loss: 0.5625 - val_acc: 0.8430\n",
      "Epoch 53/85\n",
      " - 6s - loss: 0.6830 - acc: 0.7540 - val_loss: 0.6047 - val_acc: 0.8364\n",
      "Epoch 54/85\n",
      " - 6s - loss: 0.6776 - acc: 0.7574 - val_loss: 0.5785 - val_acc: 0.8417\n",
      "Epoch 55/85\n",
      " - 7s - loss: 0.6810 - acc: 0.7571 - val_loss: 0.6143 - val_acc: 0.8373\n",
      "Epoch 56/85\n",
      " - 7s - loss: 0.6810 - acc: 0.7549 - val_loss: 0.6225 - val_acc: 0.8434\n",
      "Epoch 57/85\n",
      " - 7s - loss: 0.6777 - acc: 0.7569 - val_loss: 0.5607 - val_acc: 0.8417\n",
      "Epoch 58/85\n",
      " - 7s - loss: 0.6773 - acc: 0.7584 - val_loss: 0.6414 - val_acc: 0.8343\n",
      "Epoch 59/85\n",
      " - 7s - loss: 0.6731 - acc: 0.7602 - val_loss: 0.5859 - val_acc: 0.8423\n",
      "Epoch 60/85\n",
      " - 6s - loss: 0.6750 - acc: 0.7587 - val_loss: 0.6061 - val_acc: 0.8420\n",
      "Epoch 61/85\n",
      " - 6s - loss: 0.6736 - acc: 0.7586 - val_loss: 0.5689 - val_acc: 0.8463\n",
      "Epoch 62/85\n",
      " - 6s - loss: 0.6731 - acc: 0.7584 - val_loss: 0.6069 - val_acc: 0.8434\n",
      "Epoch 63/85\n",
      " - 6s - loss: 0.6734 - acc: 0.7586 - val_loss: 0.6215 - val_acc: 0.8416\n",
      "Epoch 64/85\n",
      " - 6s - loss: 0.6716 - acc: 0.7594 - val_loss: 0.6030 - val_acc: 0.8437\n",
      "Epoch 65/85\n",
      " - 6s - loss: 0.6722 - acc: 0.7591 - val_loss: 0.6393 - val_acc: 0.8177\n",
      "Epoch 66/85\n",
      " - 6s - loss: 0.6769 - acc: 0.7553 - val_loss: 0.5652 - val_acc: 0.8437\n",
      "Epoch 67/85\n",
      " - 6s - loss: 0.6731 - acc: 0.7565 - val_loss: 0.5967 - val_acc: 0.8372\n",
      "Epoch 68/85\n",
      " - 6s - loss: 0.6728 - acc: 0.7578 - val_loss: 0.5564 - val_acc: 0.8451\n",
      "Epoch 69/85\n",
      " - 6s - loss: 0.6735 - acc: 0.7585 - val_loss: 0.5703 - val_acc: 0.8375\n",
      "Epoch 70/85\n",
      " - 6s - loss: 0.6749 - acc: 0.7565 - val_loss: 0.8163 - val_acc: 0.6962\n",
      "Epoch 71/85\n",
      " - 6s - loss: 0.6700 - acc: 0.7602 - val_loss: 0.5603 - val_acc: 0.8426\n",
      "Epoch 72/85\n",
      " - 6s - loss: 0.6677 - acc: 0.7618 - val_loss: 0.5487 - val_acc: 0.8436\n",
      "Epoch 73/85\n",
      " - 6s - loss: 0.6686 - acc: 0.7607 - val_loss: 0.5805 - val_acc: 0.8381\n",
      "Epoch 74/85\n",
      " - 6s - loss: 0.6698 - acc: 0.7591 - val_loss: 0.5808 - val_acc: 0.8425\n",
      "Epoch 75/85\n",
      " - 7s - loss: 0.6675 - acc: 0.7598 - val_loss: 0.5510 - val_acc: 0.8426\n",
      "Epoch 76/85\n",
      " - 7s - loss: 0.6707 - acc: 0.7597 - val_loss: 0.5493 - val_acc: 0.8423\n",
      "Epoch 77/85\n",
      " - 6s - loss: 0.6675 - acc: 0.7593 - val_loss: 0.6335 - val_acc: 0.8313\n",
      "Epoch 78/85\n",
      " - 7s - loss: 0.6676 - acc: 0.7608 - val_loss: 0.5911 - val_acc: 0.8387\n",
      "Epoch 79/85\n",
      " - 6s - loss: 0.6655 - acc: 0.7610 - val_loss: 0.7517 - val_acc: 0.7323\n",
      "Epoch 80/85\n",
      " - 6s - loss: 0.6665 - acc: 0.7599 - val_loss: 0.5731 - val_acc: 0.8425\n",
      "Epoch 81/85\n",
      " - 6s - loss: 0.6780 - acc: 0.7558 - val_loss: 0.5618 - val_acc: 0.8474\n",
      "Epoch 82/85\n",
      " - 6s - loss: 0.6673 - acc: 0.7596 - val_loss: 0.6058 - val_acc: 0.8303\n",
      "Epoch 83/85\n",
      " - 6s - loss: 0.6615 - acc: 0.7633 - val_loss: 0.6021 - val_acc: 0.8469\n",
      "Epoch 84/85\n",
      " - 6s - loss: 0.6646 - acc: 0.7620 - val_loss: 0.5524 - val_acc: 0.8420\n",
      "Epoch 85/85\n",
      " - 6s - loss: 0.6698 - acc: 0.7618 - val_loss: 0.6134 - val_acc: 0.8277\n",
      "49958/49958 [==============================] - 5s 93us/step\n",
      " Dropout : 0.3 accuracy is : 0.710276632371\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_11 (LSTM)               (None, 1, 300)            2138400   \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, 1, 300)            0         \n",
      "_________________________________________________________________\n",
      "lstm_12 (LSTM)               (None, 300)               721200    \n",
      "_________________________________________________________________\n",
      "dropout_12 (Dropout)         (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 100)               30100     \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 3)                 303       \n",
      "=================================================================\n",
      "Total params: 2,890,003\n",
      "Trainable params: 2,890,003\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      " Model build done!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 159866 samples, validate on 39967 samples\n",
      "Epoch 1/85\n",
      " - 8s - loss: 1.0457 - acc: 0.5375 - val_loss: 1.1035 - val_acc: 0.5333\n",
      "Epoch 2/85\n",
      " - 6s - loss: 1.0260 - acc: 0.5382 - val_loss: 1.0564 - val_acc: 0.5431\n",
      "Epoch 3/85\n",
      " - 6s - loss: 0.9849 - acc: 0.5572 - val_loss: 1.0464 - val_acc: 0.5652\n",
      "Epoch 4/85\n",
      " - 6s - loss: 0.9572 - acc: 0.5687 - val_loss: 1.0458 - val_acc: 0.6023\n",
      "Epoch 5/85\n",
      " - 6s - loss: 0.9354 - acc: 0.5778 - val_loss: 0.9906 - val_acc: 0.5871\n",
      "Epoch 6/85\n",
      " - 6s - loss: 0.9143 - acc: 0.5939 - val_loss: 0.8125 - val_acc: 0.6786\n",
      "Epoch 7/85\n",
      " - 6s - loss: 0.8921 - acc: 0.6080 - val_loss: 0.7802 - val_acc: 0.6754\n",
      "Epoch 8/85\n",
      " - 6s - loss: 0.8670 - acc: 0.6274 - val_loss: 0.7283 - val_acc: 0.7817\n",
      "Epoch 9/85\n",
      " - 6s - loss: 0.8566 - acc: 0.6424 - val_loss: 0.6707 - val_acc: 0.8228\n",
      "Epoch 10/85\n",
      " - 6s - loss: 0.8378 - acc: 0.6573 - val_loss: 0.6939 - val_acc: 0.7992\n",
      "Epoch 11/85\n",
      " - 7s - loss: 0.8248 - acc: 0.6726 - val_loss: 0.6182 - val_acc: 0.8346\n",
      "Epoch 12/85\n",
      " - 6s - loss: 0.8139 - acc: 0.6775 - val_loss: 0.6058 - val_acc: 0.8356\n",
      "Epoch 13/85\n",
      " - 6s - loss: 0.8059 - acc: 0.6813 - val_loss: 0.6304 - val_acc: 0.8310\n",
      "Epoch 14/85\n",
      " - 6s - loss: 0.7968 - acc: 0.6928 - val_loss: 0.7184 - val_acc: 0.7816\n",
      "Epoch 15/85\n",
      " - 6s - loss: 0.7882 - acc: 0.6994 - val_loss: 0.6074 - val_acc: 0.8309\n",
      "Epoch 16/85\n",
      " - 6s - loss: 0.7980 - acc: 0.6840 - val_loss: 0.5927 - val_acc: 0.8372\n",
      "Epoch 17/85\n",
      " - 6s - loss: 0.7893 - acc: 0.6942 - val_loss: 0.5934 - val_acc: 0.8376\n",
      "Epoch 18/85\n",
      " - 6s - loss: 0.7723 - acc: 0.7075 - val_loss: 0.5939 - val_acc: 0.8383\n",
      "Epoch 19/85\n",
      " - 6s - loss: 0.7710 - acc: 0.7097 - val_loss: 0.5912 - val_acc: 0.8377\n",
      "Epoch 20/85\n",
      " - 6s - loss: 0.7718 - acc: 0.7099 - val_loss: 0.6561 - val_acc: 0.8165\n",
      "Epoch 21/85\n",
      " - 6s - loss: 0.7732 - acc: 0.7040 - val_loss: 0.5976 - val_acc: 0.8365\n",
      "Epoch 22/85\n",
      " - 6s - loss: 0.7617 - acc: 0.7161 - val_loss: 0.6045 - val_acc: 0.8355\n",
      "Epoch 23/85\n",
      " - 6s - loss: 0.7597 - acc: 0.7157 - val_loss: 0.5950 - val_acc: 0.8366\n",
      "Epoch 24/85\n",
      " - 6s - loss: 0.7579 - acc: 0.7176 - val_loss: 0.6083 - val_acc: 0.8345\n",
      "Epoch 25/85\n",
      " - 6s - loss: 0.7822 - acc: 0.6928 - val_loss: 0.6072 - val_acc: 0.8306\n",
      "Epoch 26/85\n",
      " - 6s - loss: 0.7581 - acc: 0.7169 - val_loss: 0.6232 - val_acc: 0.8382\n",
      "Epoch 27/85\n",
      " - 6s - loss: 0.7525 - acc: 0.7198 - val_loss: 0.7200 - val_acc: 0.8265\n",
      "Epoch 28/85\n",
      " - 6s - loss: 0.7497 - acc: 0.7206 - val_loss: 0.6335 - val_acc: 0.8350\n",
      "Epoch 29/85\n",
      " - 7s - loss: 0.7711 - acc: 0.6974 - val_loss: 0.6259 - val_acc: 0.8239\n",
      "Epoch 30/85\n",
      " - 6s - loss: 0.7473 - acc: 0.7256 - val_loss: 1.0954 - val_acc: 0.5218\n",
      "Epoch 31/85\n",
      " - 6s - loss: 0.7568 - acc: 0.7173 - val_loss: 0.6131 - val_acc: 0.8297\n",
      "Epoch 32/85\n",
      " - 7s - loss: 0.7478 - acc: 0.7234 - val_loss: 0.6025 - val_acc: 0.8366\n",
      "Epoch 33/85\n",
      " - 6s - loss: 0.8192 - acc: 0.6460 - val_loss: 0.6103 - val_acc: 0.8331\n",
      "Epoch 34/85\n",
      " - 7s - loss: 0.7465 - acc: 0.7228 - val_loss: 0.8056 - val_acc: 0.7092\n",
      "Epoch 35/85\n",
      " - 6s - loss: 0.7647 - acc: 0.7016 - val_loss: 0.6270 - val_acc: 0.8283\n",
      "Epoch 36/85\n",
      " - 6s - loss: 0.7416 - acc: 0.7262 - val_loss: 1.1231 - val_acc: 0.4911\n",
      "Epoch 37/85\n",
      " - 6s - loss: 0.7404 - acc: 0.7279 - val_loss: 0.5884 - val_acc: 0.8396\n",
      "Epoch 38/85\n",
      " - 6s - loss: 0.7394 - acc: 0.7280 - val_loss: 0.6257 - val_acc: 0.8268\n",
      "Epoch 39/85\n",
      " - 6s - loss: 0.7433 - acc: 0.7213 - val_loss: 0.7619 - val_acc: 0.7505\n",
      "Epoch 40/85\n",
      " - 6s - loss: 0.7371 - acc: 0.7276 - val_loss: 0.6324 - val_acc: 0.8359\n",
      "Epoch 41/85\n",
      " - 6s - loss: 0.7357 - acc: 0.7301 - val_loss: 0.6114 - val_acc: 0.8293\n",
      "Epoch 42/85\n",
      " - 6s - loss: 0.7621 - acc: 0.7018 - val_loss: 0.5927 - val_acc: 0.8401\n",
      "Epoch 43/85\n",
      " - 6s - loss: 0.7346 - acc: 0.7301 - val_loss: 0.6057 - val_acc: 0.8363\n",
      "Epoch 44/85\n",
      " - 6s - loss: 0.7333 - acc: 0.7305 - val_loss: 0.8158 - val_acc: 0.7093\n",
      "Epoch 45/85\n",
      " - 6s - loss: 0.7358 - acc: 0.7295 - val_loss: 0.6181 - val_acc: 0.8324\n",
      "Epoch 46/85\n",
      " - 6s - loss: 0.7341 - acc: 0.7298 - val_loss: 0.7574 - val_acc: 0.7591\n",
      "Epoch 47/85\n",
      " - 6s - loss: 0.7322 - acc: 0.7312 - val_loss: 0.6045 - val_acc: 0.8346\n",
      "Epoch 48/85\n",
      " - 6s - loss: 0.7344 - acc: 0.7289 - val_loss: 0.6005 - val_acc: 0.8329\n",
      "Epoch 49/85\n",
      " - 6s - loss: 0.7283 - acc: 0.7327 - val_loss: 0.5985 - val_acc: 0.8366\n",
      "Epoch 50/85\n",
      " - 6s - loss: 0.7300 - acc: 0.7318 - val_loss: 0.6016 - val_acc: 0.8338\n",
      "Epoch 51/85\n",
      " - 6s - loss: 0.7297 - acc: 0.7324 - val_loss: 0.6006 - val_acc: 0.8376\n",
      "Epoch 52/85\n",
      " - 6s - loss: 0.7279 - acc: 0.7343 - val_loss: 0.6739 - val_acc: 0.8157\n",
      "Epoch 53/85\n",
      " - 6s - loss: 0.7333 - acc: 0.7279 - val_loss: 0.6476 - val_acc: 0.8243\n",
      "Epoch 54/85\n",
      " - 7s - loss: 0.7248 - acc: 0.7347 - val_loss: 0.6680 - val_acc: 0.8096\n",
      "Epoch 55/85\n",
      " - 6s - loss: 0.7253 - acc: 0.7354 - val_loss: 0.5923 - val_acc: 0.8384\n",
      "Epoch 56/85\n",
      " - 6s - loss: 0.7250 - acc: 0.7360 - val_loss: 0.5912 - val_acc: 0.8383\n",
      "Epoch 57/85\n",
      " - 6s - loss: 0.7252 - acc: 0.7353 - val_loss: 0.6017 - val_acc: 0.8399\n",
      "Epoch 58/85\n",
      " - 6s - loss: 0.7267 - acc: 0.7351 - val_loss: 0.5890 - val_acc: 0.8364\n",
      "Epoch 59/85\n",
      " - 6s - loss: 0.7224 - acc: 0.7373 - val_loss: 0.5925 - val_acc: 0.8383\n",
      "Epoch 60/85\n",
      " - 6s - loss: 0.7246 - acc: 0.7353 - val_loss: 0.7092 - val_acc: 0.7874\n",
      "Epoch 61/85\n",
      " - 6s - loss: 0.7216 - acc: 0.7376 - val_loss: 0.6329 - val_acc: 0.8280\n",
      "Epoch 62/85\n",
      " - 6s - loss: 0.7214 - acc: 0.7376 - val_loss: 0.5999 - val_acc: 0.8326\n",
      "Epoch 63/85\n",
      " - 6s - loss: 0.7238 - acc: 0.7365 - val_loss: 0.6331 - val_acc: 0.8270\n",
      "Epoch 64/85\n",
      " - 6s - loss: 0.7229 - acc: 0.7372 - val_loss: 0.6117 - val_acc: 0.8386\n",
      "Epoch 65/85\n",
      " - 6s - loss: 0.7221 - acc: 0.7387 - val_loss: 0.6386 - val_acc: 0.8280\n",
      "Epoch 66/85\n",
      " - 6s - loss: 0.7236 - acc: 0.7353 - val_loss: 0.5950 - val_acc: 0.8388\n",
      "Epoch 67/85\n",
      " - 6s - loss: 0.7221 - acc: 0.7366 - val_loss: 0.6051 - val_acc: 0.8287\n",
      "Epoch 68/85\n",
      " - 6s - loss: 0.7216 - acc: 0.7375 - val_loss: 0.6308 - val_acc: 0.8211\n",
      "Epoch 69/85\n",
      "49958/49958 [==============================] - 5s 100us/step\n",
      " Dropout : 0.35 accuracy is : 0.693202289924\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_13 (LSTM)               (None, 1, 300)            2138400   \n",
      "_________________________________________________________________\n",
      "dropout_13 (Dropout)         (None, 1, 300)            0         \n",
      "_________________________________________________________________\n",
      "lstm_14 (LSTM)               (None, 300)               721200    \n",
      "_________________________________________________________________\n",
      "dropout_14 (Dropout)         (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 100)               30100     \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 3)                 303       \n",
      "=================================================================\n",
      "Total params: 2,890,003\n",
      "Trainable params: 2,890,003\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      " Model build done!\n",
      "Train on 159866 samples, validate on 39967 samples\n",
      "Epoch 1/85\n",
      " - 8s - loss: 1.0460 - acc: 0.5377 - val_loss: 1.1063 - val_acc: 0.5333\n",
      "Epoch 2/85\n",
      " - 6s - loss: 1.0382 - acc: 0.5327 - val_loss: 1.0552 - val_acc: 0.5333\n",
      "Epoch 3/85\n",
      " - 6s - loss: 0.9864 - acc: 0.5562 - val_loss: 0.8771 - val_acc: 0.6492\n",
      "Epoch 4/85\n",
      " - 6s - loss: 0.9662 - acc: 0.5680 - val_loss: 1.0013 - val_acc: 0.5961\n",
      "Epoch 5/85\n",
      " - 6s - loss: 0.9322 - acc: 0.5838 - val_loss: 0.8988 - val_acc: 0.6177\n",
      "Epoch 6/85\n",
      " - 6s - loss: 0.9104 - acc: 0.6029 - val_loss: 0.8104 - val_acc: 0.6650\n",
      "Epoch 7/85\n",
      " - 6s - loss: 0.8884 - acc: 0.6145 - val_loss: 0.7271 - val_acc: 0.7055\n",
      "Epoch 8/85\n",
      " - 6s - loss: 0.8577 - acc: 0.6384 - val_loss: 0.6528 - val_acc: 0.8294\n",
      "Epoch 9/85\n",
      " - 6s - loss: 0.8392 - acc: 0.6553 - val_loss: 0.6379 - val_acc: 0.8351\n",
      "Epoch 10/85\n",
      " - 6s - loss: 0.8257 - acc: 0.6692 - val_loss: 0.6471 - val_acc: 0.8317\n",
      "Epoch 11/85\n",
      " - 6s - loss: 0.8107 - acc: 0.6799 - val_loss: 0.8330 - val_acc: 0.7508\n",
      "Epoch 12/85\n",
      " - 6s - loss: 0.7966 - acc: 0.6924 - val_loss: 0.6425 - val_acc: 0.8194\n",
      "Epoch 13/85\n",
      " - 6s - loss: 0.7929 - acc: 0.6924 - val_loss: 0.6009 - val_acc: 0.8361\n",
      "Epoch 14/85\n",
      " - 6s - loss: 0.7821 - acc: 0.7003 - val_loss: 0.6003 - val_acc: 0.8380\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/85\n",
      " - 6s - loss: 0.7666 - acc: 0.7125 - val_loss: 0.5888 - val_acc: 0.8398\n",
      "Epoch 16/85\n",
      " - 6s - loss: 0.7667 - acc: 0.7129 - val_loss: 0.6147 - val_acc: 0.8321\n",
      "Epoch 17/85\n",
      " - 6s - loss: 0.7601 - acc: 0.7173 - val_loss: 0.5989 - val_acc: 0.8364\n",
      "Epoch 18/85\n",
      " - 6s - loss: 0.7578 - acc: 0.7188 - val_loss: 0.5927 - val_acc: 0.8384\n",
      "Epoch 19/85\n",
      " - 6s - loss: 0.7477 - acc: 0.7239 - val_loss: 0.6224 - val_acc: 0.8308\n",
      "Epoch 20/85\n",
      " - 6s - loss: 0.7444 - acc: 0.7281 - val_loss: 0.6056 - val_acc: 0.8350\n",
      "Epoch 21/85\n",
      " - 6s - loss: 0.7382 - acc: 0.7326 - val_loss: 0.5880 - val_acc: 0.8378\n",
      "Epoch 22/85\n",
      " - 6s - loss: 0.7431 - acc: 0.7288 - val_loss: 1.2301 - val_acc: 0.4195\n",
      "Epoch 23/85\n",
      " - 6s - loss: 0.7308 - acc: 0.7346 - val_loss: 0.5991 - val_acc: 0.8370\n",
      "Epoch 24/85\n",
      " - 6s - loss: 0.7315 - acc: 0.7353 - val_loss: 0.5988 - val_acc: 0.8396\n",
      "Epoch 25/85\n",
      " - 6s - loss: 0.7271 - acc: 0.7358 - val_loss: 0.6000 - val_acc: 0.8370\n",
      "Epoch 26/85\n",
      " - 6s - loss: 0.7249 - acc: 0.7373 - val_loss: 0.6585 - val_acc: 0.8195\n",
      "Epoch 27/85\n",
      " - 6s - loss: 0.7206 - acc: 0.7420 - val_loss: 0.5853 - val_acc: 0.8397\n",
      "Epoch 28/85\n",
      " - 6s - loss: 0.7143 - acc: 0.7431 - val_loss: 0.6088 - val_acc: 0.8342\n",
      "Epoch 29/85\n",
      " - 6s - loss: 0.7201 - acc: 0.7390 - val_loss: 0.5888 - val_acc: 0.8385\n",
      "Epoch 30/85\n",
      " - 6s - loss: 0.7141 - acc: 0.7422 - val_loss: 0.5888 - val_acc: 0.8390\n",
      "Epoch 31/85\n",
      " - 6s - loss: 0.7077 - acc: 0.7469 - val_loss: 0.6095 - val_acc: 0.8255\n",
      "Epoch 32/85\n",
      " - 6s - loss: 0.7149 - acc: 0.7363 - val_loss: 0.5833 - val_acc: 0.8386\n",
      "Epoch 33/85\n",
      " - 6s - loss: 0.7117 - acc: 0.7377 - val_loss: 0.7551 - val_acc: 0.7933\n",
      "Epoch 34/85\n",
      " - 6s - loss: 0.7030 - acc: 0.7482 - val_loss: 0.6313 - val_acc: 0.8367\n",
      "Epoch 35/85\n",
      " - 6s - loss: 0.7005 - acc: 0.7469 - val_loss: 0.5739 - val_acc: 0.8407\n",
      "Epoch 36/85\n",
      " - 6s - loss: 0.7020 - acc: 0.7480 - val_loss: 0.5772 - val_acc: 0.8415\n",
      "Epoch 37/85\n",
      " - 6s - loss: 0.6985 - acc: 0.7471 - val_loss: 0.5819 - val_acc: 0.8369\n",
      "Epoch 38/85\n",
      " - 6s - loss: 0.6929 - acc: 0.7505 - val_loss: 0.6064 - val_acc: 0.8371\n",
      "Epoch 39/85\n",
      " - 6s - loss: 0.6932 - acc: 0.7512 - val_loss: 0.5707 - val_acc: 0.8405\n",
      "Epoch 40/85\n",
      " - 6s - loss: 0.6862 - acc: 0.7555 - val_loss: 0.6675 - val_acc: 0.7970\n",
      "Epoch 41/85\n",
      " - 6s - loss: 0.6917 - acc: 0.7513 - val_loss: 0.5654 - val_acc: 0.8407\n",
      "Epoch 42/85\n",
      " - 6s - loss: 0.6900 - acc: 0.7525 - val_loss: 0.6215 - val_acc: 0.8252\n",
      "Epoch 43/85\n",
      " - 6s - loss: 0.6854 - acc: 0.7530 - val_loss: 0.6650 - val_acc: 0.8011\n",
      "Epoch 44/85\n",
      " - 6s - loss: 0.6823 - acc: 0.7530 - val_loss: 0.8709 - val_acc: 0.6649\n",
      "Epoch 45/85\n",
      " - 6s - loss: 0.6845 - acc: 0.7520 - val_loss: 0.5646 - val_acc: 0.8418\n",
      "Epoch 46/85\n",
      " - 6s - loss: 0.6796 - acc: 0.7559 - val_loss: 1.2052 - val_acc: 0.4356\n",
      "Epoch 47/85\n",
      " - 6s - loss: 0.6818 - acc: 0.7550 - val_loss: 0.5832 - val_acc: 0.8392\n",
      "Epoch 48/85\n",
      " - 6s - loss: 0.6788 - acc: 0.7553 - val_loss: 0.6635 - val_acc: 0.7974\n",
      "Epoch 49/85\n",
      " - 6s - loss: 0.6797 - acc: 0.7548 - val_loss: 0.5791 - val_acc: 0.8371\n",
      "Epoch 50/85\n",
      " - 6s - loss: 0.6794 - acc: 0.7555 - val_loss: 0.5694 - val_acc: 0.8385\n",
      "Epoch 51/85\n",
      " - 6s - loss: 0.6751 - acc: 0.7575 - val_loss: 0.6170 - val_acc: 0.8194\n",
      "Epoch 52/85\n",
      " - 6s - loss: 0.6779 - acc: 0.7532 - val_loss: 0.6035 - val_acc: 0.8244\n",
      "Epoch 53/85\n",
      " - 6s - loss: 0.6737 - acc: 0.7554 - val_loss: 0.5582 - val_acc: 0.8438\n",
      "Epoch 54/85\n",
      " - 6s - loss: 0.6742 - acc: 0.7562 - val_loss: 0.5657 - val_acc: 0.8445\n",
      "Epoch 55/85\n",
      " - 6s - loss: 0.6704 - acc: 0.7576 - val_loss: 0.6520 - val_acc: 0.7888\n",
      "Epoch 56/85\n",
      " - 6s - loss: 0.6698 - acc: 0.7576 - val_loss: 0.5719 - val_acc: 0.8389\n",
      "Epoch 57/85\n",
      " - 6s - loss: 0.6677 - acc: 0.7596 - val_loss: 0.5566 - val_acc: 0.8422\n",
      "Epoch 58/85\n",
      " - 6s - loss: 0.6708 - acc: 0.7578 - val_loss: 0.5683 - val_acc: 0.8416\n",
      "Epoch 59/85\n",
      " - 6s - loss: 0.6701 - acc: 0.7587 - val_loss: 0.5626 - val_acc: 0.8451\n",
      "Epoch 60/85\n",
      " - 6s - loss: 0.6703 - acc: 0.7574 - val_loss: 0.5527 - val_acc: 0.8422\n",
      "Epoch 61/85\n",
      " - 6s - loss: 0.6666 - acc: 0.7585 - val_loss: 0.6137 - val_acc: 0.8369\n",
      "Epoch 62/85\n",
      " - 6s - loss: 0.6690 - acc: 0.7569 - val_loss: 0.5733 - val_acc: 0.8434\n",
      "Epoch 63/85\n",
      " - 6s - loss: 0.6671 - acc: 0.7567 - val_loss: 0.5800 - val_acc: 0.8439\n",
      "Epoch 64/85\n",
      " - 6s - loss: 0.6704 - acc: 0.7559 - val_loss: 0.5781 - val_acc: 0.8412\n",
      "Epoch 65/85\n",
      " - 6s - loss: 0.6688 - acc: 0.7554 - val_loss: 0.5837 - val_acc: 0.8423\n",
      "Epoch 66/85\n",
      " - 6s - loss: 0.6682 - acc: 0.7568 - val_loss: 0.5982 - val_acc: 0.8244\n",
      "Epoch 67/85\n",
      " - 6s - loss: 0.6649 - acc: 0.7588 - val_loss: 0.5468 - val_acc: 0.8476\n",
      "Epoch 68/85\n",
      " - 6s - loss: 0.6665 - acc: 0.7560 - val_loss: 0.5671 - val_acc: 0.8407\n",
      "Epoch 69/85\n",
      " - 6s - loss: 0.6597 - acc: 0.7614 - val_loss: 0.5638 - val_acc: 0.8432\n",
      "Epoch 70/85\n",
      " - 6s - loss: 0.6647 - acc: 0.7585 - val_loss: 0.5557 - val_acc: 0.8440\n",
      "Epoch 71/85\n",
      " - 6s - loss: 0.6600 - acc: 0.7605 - val_loss: 0.5640 - val_acc: 0.8403\n",
      "Epoch 72/85\n",
      " - 6s - loss: 0.6595 - acc: 0.7620 - val_loss: 0.5532 - val_acc: 0.8478\n",
      "Epoch 73/85\n",
      " - 6s - loss: 0.6591 - acc: 0.7603 - val_loss: 0.5699 - val_acc: 0.8462\n",
      "Epoch 74/85\n",
      " - 6s - loss: 0.6664 - acc: 0.7563 - val_loss: 0.5411 - val_acc: 0.8485\n",
      "Epoch 75/85\n",
      " - 6s - loss: 0.6624 - acc: 0.7592 - val_loss: 0.5608 - val_acc: 0.8447\n",
      "Epoch 76/85\n",
      " - 6s - loss: 0.6601 - acc: 0.7604 - val_loss: 0.5817 - val_acc: 0.8407\n",
      "Epoch 77/85\n",
      " - 6s - loss: 0.6606 - acc: 0.7600 - val_loss: 0.5602 - val_acc: 0.8426\n",
      "Epoch 78/85\n",
      " - 6s - loss: 0.6603 - acc: 0.7592 - val_loss: 0.5554 - val_acc: 0.8452\n",
      "Epoch 79/85\n",
      " - 6s - loss: 0.6568 - acc: 0.7623 - val_loss: 0.5569 - val_acc: 0.8459\n",
      "Epoch 80/85\n",
      " - 6s - loss: 0.6618 - acc: 0.7590 - val_loss: 0.5491 - val_acc: 0.8469\n",
      "Epoch 81/85\n",
      " - 6s - loss: 0.6575 - acc: 0.7614 - val_loss: 0.5627 - val_acc: 0.8444\n",
      "Epoch 82/85\n",
      " - 6s - loss: 0.6591 - acc: 0.7616 - val_loss: 0.5480 - val_acc: 0.8454\n",
      "Epoch 83/85\n",
      " - 6s - loss: 0.6600 - acc: 0.7600 - val_loss: 0.5516 - val_acc: 0.8478\n",
      "Epoch 84/85\n",
      " - 6s - loss: 0.6580 - acc: 0.7600 - val_loss: 0.5518 - val_acc: 0.8458\n",
      "Epoch 85/85\n",
      " - 6s - loss: 0.6564 - acc: 0.7620 - val_loss: 0.5678 - val_acc: 0.8447\n",
      "49958/49958 [==============================] - 5s 94us/step\n",
      " Dropout : 0.4 accuracy is : 0.788542375596\n"
     ]
    }
   ],
   "source": [
    "model_list = run_architecture_1_with_different_dropouts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/kshitijg1992/scotus-predict-v2/src/saving_models\n"
     ]
    }
   ],
   "source": [
    "%cd saving_models/\n",
    "for i,model in enumerate(model_list):\n",
    "    dropout_list = [0.1, 0.15, 0.2, 0.25, 0.3, 0.35, 0.4]\n",
    "    name = 'model_arch_1_dp_'+str(dropout_list[i])+'.h5'\n",
    "    model.save(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "p = load_model('model_arch_1_dp_3.5.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49958/49958 [==============================] - 6s 122us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.91787002278393648, 0.69320228992353572]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_architecture_2_with_different_dropouts():\n",
    "    seq_len = 1\n",
    "    shape = [feature_df.shape[1], seq_len] # feature, window\n",
    "    neurons = [600, 400, 150, 3]\n",
    "    dropout_list = [0.1, 0.15, 0.2, 0.25, 0.3, 0.35, 0.4]\n",
    "    model_list = []\n",
    "    for dropout in dropout_list:\n",
    "        model2 = build_model2(shape, neurons, dropout)\n",
    "        model2.fit(\n",
    "        X_train,\n",
    "        y_train,\n",
    "        batch_size=512,\n",
    "        epochs=85,\n",
    "        validation_split=0.2,\n",
    "        shuffle = False,\n",
    "        class_weight = {0 : 1.2, 1 : 1, 2 : 1.1},\n",
    "        verbose=2)\n",
    "        curr_eval = model2.evaluate(X_test,y_test)\n",
    "        print(\" Dropout : \" + str(dropout) + \" accuracy is : \" + str(curr_eval[1]))\n",
    "        model_list.append(model2)\n",
    "    return model_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_15 (LSTM)               (None, 1, 600)            4996800   \n",
      "_________________________________________________________________\n",
      "dropout_15 (Dropout)         (None, 1, 600)            0         \n",
      "_________________________________________________________________\n",
      "lstm_16 (LSTM)               (None, 400)               1601600   \n",
      "_________________________________________________________________\n",
      "dropout_16 (Dropout)         (None, 400)               0         \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 150)               60150     \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 3)                 453       \n",
      "=================================================================\n",
      "Total params: 6,659,003\n",
      "Trainable params: 6,659,003\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      " Model build done!\n",
      "Train on 159866 samples, validate on 39967 samples\n",
      "Epoch 1/85\n",
      " - 12s - loss: 1.0450 - acc: 0.5378 - val_loss: 1.1080 - val_acc: 0.5333\n",
      "Epoch 2/85\n",
      " - 10s - loss: 1.0419 - acc: 0.5319 - val_loss: 1.0876 - val_acc: 0.5333\n",
      "Epoch 3/85\n",
      " - 10s - loss: 0.9963 - acc: 0.5526 - val_loss: 0.9469 - val_acc: 0.6270\n",
      "Epoch 4/85\n",
      " - 10s - loss: 0.9689 - acc: 0.5627 - val_loss: 0.8615 - val_acc: 0.6590\n",
      "Epoch 5/85\n",
      " - 10s - loss: 0.9349 - acc: 0.5816 - val_loss: 0.9431 - val_acc: 0.6145\n",
      "Epoch 6/85\n",
      " - 10s - loss: 0.9186 - acc: 0.5946 - val_loss: 1.1635 - val_acc: 0.5535\n",
      "Epoch 7/85\n",
      " - 10s - loss: 0.8850 - acc: 0.6134 - val_loss: 1.0878 - val_acc: 0.5590\n",
      "Epoch 8/85\n",
      " - 10s - loss: 0.8592 - acc: 0.6396 - val_loss: 0.8779 - val_acc: 0.6447\n",
      "Epoch 9/85\n",
      " - 10s - loss: 0.8410 - acc: 0.6527 - val_loss: 1.0533 - val_acc: 0.6624\n",
      "Epoch 10/85\n",
      " - 10s - loss: 0.8116 - acc: 0.6776 - val_loss: 0.6261 - val_acc: 0.8325\n",
      "Epoch 11/85\n",
      " - 10s - loss: 0.8012 - acc: 0.6901 - val_loss: 0.6228 - val_acc: 0.8330\n",
      "Epoch 12/85\n",
      " - 10s - loss: 0.7763 - acc: 0.7002 - val_loss: 0.9453 - val_acc: 0.7025\n",
      "Epoch 13/85\n",
      " - 10s - loss: 0.7673 - acc: 0.7130 - val_loss: 0.5898 - val_acc: 0.8382\n",
      "Epoch 14/85\n",
      " - 10s - loss: 0.7627 - acc: 0.7098 - val_loss: 0.5923 - val_acc: 0.8404\n",
      "Epoch 15/85\n",
      " - 10s - loss: 0.7524 - acc: 0.7194 - val_loss: 0.8199 - val_acc: 0.6657\n",
      "Epoch 16/85\n",
      " - 10s - loss: 0.7552 - acc: 0.7127 - val_loss: 0.5858 - val_acc: 0.8386\n",
      "Epoch 17/85\n",
      " - 10s - loss: 0.7800 - acc: 0.6984 - val_loss: 0.6296 - val_acc: 0.8369\n",
      "Epoch 18/85\n",
      " - 10s - loss: 0.7319 - acc: 0.7287 - val_loss: 0.8673 - val_acc: 0.6933\n",
      "Epoch 19/85\n",
      " - 10s - loss: 0.7297 - acc: 0.7323 - val_loss: 0.6240 - val_acc: 0.8366\n",
      "Epoch 20/85\n",
      " - 10s - loss: 0.7310 - acc: 0.7348 - val_loss: 0.7739 - val_acc: 0.7682\n",
      "Epoch 21/85\n",
      " - 10s - loss: 0.7209 - acc: 0.7378 - val_loss: 0.6177 - val_acc: 0.8371\n",
      "Epoch 22/85\n",
      " - 10s - loss: 0.7104 - acc: 0.7425 - val_loss: 0.6338 - val_acc: 0.8265\n",
      "Epoch 23/85\n",
      " - 10s - loss: 0.7126 - acc: 0.7418 - val_loss: 0.7636 - val_acc: 0.7541\n",
      "Epoch 24/85\n",
      " - 10s - loss: 0.7108 - acc: 0.7397 - val_loss: 0.5666 - val_acc: 0.8465\n",
      "Epoch 25/85\n",
      " - 10s - loss: 0.6986 - acc: 0.7478 - val_loss: 0.7465 - val_acc: 0.7495\n",
      "Epoch 26/85\n",
      " - 10s - loss: 0.6973 - acc: 0.7489 - val_loss: 0.5638 - val_acc: 0.8464\n",
      "Epoch 27/85\n",
      " - 10s - loss: 0.7000 - acc: 0.7453 - val_loss: 0.5508 - val_acc: 0.8477\n",
      "Epoch 28/85\n",
      " - 10s - loss: 0.6945 - acc: 0.7474 - val_loss: 0.6531 - val_acc: 0.8296\n",
      "Epoch 29/85\n",
      " - 10s - loss: 0.6871 - acc: 0.7523 - val_loss: 0.7231 - val_acc: 0.7896\n",
      "Epoch 30/85\n",
      " - 10s - loss: 0.6865 - acc: 0.7532 - val_loss: 0.5792 - val_acc: 0.8489\n",
      "Epoch 31/85\n",
      " - 10s - loss: 0.6820 - acc: 0.7538 - val_loss: 0.5566 - val_acc: 0.8483\n",
      "Epoch 32/85\n",
      " - 10s - loss: 0.6810 - acc: 0.7527 - val_loss: 0.7719 - val_acc: 0.7585\n",
      "Epoch 33/85\n",
      " - 10s - loss: 0.6860 - acc: 0.7504 - val_loss: 0.7212 - val_acc: 0.8352\n",
      "Epoch 34/85\n",
      " - 10s - loss: 0.6789 - acc: 0.7538 - val_loss: 0.6585 - val_acc: 0.8426\n",
      "Epoch 35/85\n",
      " - 10s - loss: 0.6719 - acc: 0.7576 - val_loss: 0.6076 - val_acc: 0.8337\n",
      "Epoch 36/85\n",
      " - 10s - loss: 0.6767 - acc: 0.7538 - val_loss: 0.6289 - val_acc: 0.8403\n",
      "Epoch 37/85\n",
      " - 10s - loss: 0.6781 - acc: 0.7516 - val_loss: 0.5977 - val_acc: 0.8503\n",
      "Epoch 38/85\n",
      " - 10s - loss: 0.6694 - acc: 0.7574 - val_loss: 0.6176 - val_acc: 0.8197\n",
      "Epoch 39/85\n",
      " - 10s - loss: 0.6650 - acc: 0.7570 - val_loss: 0.7181 - val_acc: 0.7027\n",
      "Epoch 40/85\n",
      " - 10s - loss: 0.6620 - acc: 0.7574 - val_loss: 0.5682 - val_acc: 0.8497\n",
      "Epoch 41/85\n",
      " - 10s - loss: 0.6597 - acc: 0.7604 - val_loss: 0.5905 - val_acc: 0.8397\n",
      "Epoch 42/85\n",
      " - 10s - loss: 0.6595 - acc: 0.7606 - val_loss: 0.5789 - val_acc: 0.8288\n",
      "Epoch 43/85\n",
      " - 10s - loss: 0.6553 - acc: 0.7606 - val_loss: 0.5614 - val_acc: 0.8523\n",
      "Epoch 44/85\n",
      " - 10s - loss: 0.6528 - acc: 0.7610 - val_loss: 0.5753 - val_acc: 0.8418\n",
      "Epoch 45/85\n",
      " - 10s - loss: 0.6563 - acc: 0.7610 - val_loss: 0.6260 - val_acc: 0.8422\n",
      "Epoch 46/85\n",
      " - 10s - loss: 0.6522 - acc: 0.7634 - val_loss: 0.5963 - val_acc: 0.8489\n",
      "Epoch 47/85\n",
      " - 10s - loss: 0.6488 - acc: 0.7646 - val_loss: 0.5982 - val_acc: 0.8479\n",
      "Epoch 48/85\n",
      " - 10s - loss: 0.6443 - acc: 0.7665 - val_loss: 0.6094 - val_acc: 0.8098\n",
      "Epoch 49/85\n",
      " - 10s - loss: 0.6505 - acc: 0.7611 - val_loss: 0.6475 - val_acc: 0.8138\n",
      "Epoch 50/85\n",
      " - 10s - loss: 0.6498 - acc: 0.7626 - val_loss: 0.6035 - val_acc: 0.8445\n",
      "Epoch 51/85\n",
      " - 10s - loss: 0.6437 - acc: 0.7649 - val_loss: 0.5994 - val_acc: 0.8495\n",
      "Epoch 52/85\n",
      " - 10s - loss: 0.6443 - acc: 0.7650 - val_loss: 0.6361 - val_acc: 0.8354\n",
      "Epoch 53/85\n",
      " - 10s - loss: 0.6411 - acc: 0.7652 - val_loss: 0.5910 - val_acc: 0.8326\n",
      "Epoch 54/85\n",
      " - 10s - loss: 0.6434 - acc: 0.7645 - val_loss: 0.5618 - val_acc: 0.8496\n",
      "Epoch 55/85\n",
      " - 10s - loss: 0.6431 - acc: 0.7658 - val_loss: 0.6070 - val_acc: 0.8472\n",
      "Epoch 56/85\n",
      " - 10s - loss: 0.6385 - acc: 0.7673 - val_loss: 0.5892 - val_acc: 0.8309\n",
      "Epoch 57/85\n",
      " - 10s - loss: 0.6354 - acc: 0.7673 - val_loss: 0.5889 - val_acc: 0.8511\n",
      "Epoch 58/85\n",
      " - 10s - loss: 0.6360 - acc: 0.7689 - val_loss: 0.6498 - val_acc: 0.8021\n",
      "Epoch 59/85\n",
      " - 10s - loss: 0.6377 - acc: 0.7666 - val_loss: 0.6320 - val_acc: 0.8215\n",
      "Epoch 60/85\n",
      " - 10s - loss: 0.6331 - acc: 0.7690 - val_loss: 0.6003 - val_acc: 0.8144\n",
      "Epoch 61/85\n",
      " - 10s - loss: 0.6353 - acc: 0.7676 - val_loss: 0.5692 - val_acc: 0.8390\n",
      "Epoch 62/85\n",
      " - 10s - loss: 0.6335 - acc: 0.7680 - val_loss: 0.6437 - val_acc: 0.8373\n",
      "Epoch 63/85\n",
      " - 10s - loss: 0.6306 - acc: 0.7703 - val_loss: 0.5535 - val_acc: 0.8383\n",
      "Epoch 64/85\n",
      " - 10s - loss: 0.6321 - acc: 0.7686 - val_loss: 0.6036 - val_acc: 0.8480\n",
      "Epoch 65/85\n",
      " - 10s - loss: 0.6320 - acc: 0.7683 - val_loss: 0.6733 - val_acc: 0.7753\n",
      "Epoch 66/85\n",
      " - 10s - loss: 0.6343 - acc: 0.7645 - val_loss: 0.5552 - val_acc: 0.8502\n",
      "Epoch 67/85\n",
      " - 10s - loss: 0.6327 - acc: 0.7637 - val_loss: 0.5774 - val_acc: 0.8363\n",
      "Epoch 68/85\n",
      " - 10s - loss: 0.6299 - acc: 0.7711 - val_loss: 0.6147 - val_acc: 0.8351\n",
      "Epoch 69/85\n",
      " - 10s - loss: 0.6279 - acc: 0.7705 - val_loss: 0.5503 - val_acc: 0.8473\n",
      "Epoch 70/85\n",
      " - 10s - loss: 0.6320 - acc: 0.7683 - val_loss: 0.5700 - val_acc: 0.8498\n",
      "Epoch 71/85\n",
      " - 10s - loss: 0.6256 - acc: 0.7724 - val_loss: 0.5551 - val_acc: 0.8463\n",
      "Epoch 72/85\n",
      " - 10s - loss: 0.6282 - acc: 0.7701 - val_loss: 0.5526 - val_acc: 0.8456\n",
      "Epoch 73/85\n",
      " - 10s - loss: 0.6256 - acc: 0.7707 - val_loss: 0.5676 - val_acc: 0.8470\n",
      "Epoch 74/85\n",
      " - 10s - loss: 0.6216 - acc: 0.7732 - val_loss: 0.5436 - val_acc: 0.8490\n",
      "Epoch 75/85\n",
      " - 10s - loss: 0.6228 - acc: 0.7741 - val_loss: 0.5590 - val_acc: 0.8486\n",
      "Epoch 76/85\n",
      " - 10s - loss: 0.6216 - acc: 0.7735 - val_loss: 0.5961 - val_acc: 0.8382\n",
      "Epoch 77/85\n",
      " - 10s - loss: 0.6239 - acc: 0.7710 - val_loss: 0.5873 - val_acc: 0.8451\n",
      "Epoch 78/85\n",
      " - 10s - loss: 0.6227 - acc: 0.7724 - val_loss: 0.5666 - val_acc: 0.8479\n",
      "Epoch 79/85\n",
      " - 10s - loss: 0.6182 - acc: 0.7753 - val_loss: 0.5370 - val_acc: 0.8511\n",
      "Epoch 80/85\n",
      " - 10s - loss: 0.6263 - acc: 0.7707 - val_loss: 0.7110 - val_acc: 0.7717\n",
      "Epoch 81/85\n",
      " - 10s - loss: 0.6226 - acc: 0.7725 - val_loss: 0.5607 - val_acc: 0.8477\n",
      "Epoch 82/85\n",
      " - 10s - loss: 0.6209 - acc: 0.7745 - val_loss: 0.5790 - val_acc: 0.8390\n",
      "Epoch 83/85\n",
      " - 10s - loss: 0.6195 - acc: 0.7742 - val_loss: 0.5740 - val_acc: 0.8499\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 84/85\n",
      " - 10s - loss: 0.6278 - acc: 0.7638 - val_loss: 0.5792 - val_acc: 0.8447\n",
      "Epoch 85/85\n",
      " - 10s - loss: 0.6201 - acc: 0.7726 - val_loss: 0.5702 - val_acc: 0.8498\n",
      "49958/49958 [==============================] - 6s 111us/step\n",
      " Dropout : 0.1 accuracy is : 0.809860282637\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_17 (LSTM)               (None, 1, 600)            4996800   \n",
      "_________________________________________________________________\n",
      "dropout_17 (Dropout)         (None, 1, 600)            0         \n",
      "_________________________________________________________________\n",
      "lstm_18 (LSTM)               (None, 400)               1601600   \n",
      "_________________________________________________________________\n",
      "dropout_18 (Dropout)         (None, 400)               0         \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 150)               60150     \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 3)                 453       \n",
      "=================================================================\n",
      "Total params: 6,659,003\n",
      "Trainable params: 6,659,003\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      " Model build done!\n",
      "Train on 159866 samples, validate on 39967 samples\n",
      "Epoch 1/85\n",
      " - 12s - loss: 1.0452 - acc: 0.5368 - val_loss: 1.1088 - val_acc: 0.5333\n",
      "Epoch 2/85\n",
      " - 10s - loss: 1.0421 - acc: 0.5323 - val_loss: 1.0927 - val_acc: 0.5333\n",
      "Epoch 3/85\n",
      " - 10s - loss: 0.9996 - acc: 0.5548 - val_loss: 0.9270 - val_acc: 0.6234\n",
      "Epoch 4/85\n",
      " - 10s - loss: 0.9636 - acc: 0.5671 - val_loss: 1.0938 - val_acc: 0.5487\n",
      "Epoch 5/85\n",
      " - 10s - loss: 0.9384 - acc: 0.5812 - val_loss: 0.8954 - val_acc: 0.7196\n",
      "Epoch 6/85\n",
      " - 10s - loss: 0.9080 - acc: 0.6007 - val_loss: 0.7879 - val_acc: 0.7191\n",
      "Epoch 7/85\n",
      " - 10s - loss: 0.8870 - acc: 0.6140 - val_loss: 0.7489 - val_acc: 0.6973\n",
      "Epoch 8/85\n",
      " - 10s - loss: 0.8589 - acc: 0.6396 - val_loss: 0.7501 - val_acc: 0.7973\n",
      "Epoch 9/85\n",
      " - 10s - loss: 0.8321 - acc: 0.6595 - val_loss: 0.6384 - val_acc: 0.8250\n",
      "Epoch 10/85\n",
      " - 10s - loss: 0.8179 - acc: 0.6707 - val_loss: 0.6428 - val_acc: 0.8271\n",
      "Epoch 11/85\n",
      " - 10s - loss: 0.8019 - acc: 0.6863 - val_loss: 0.6077 - val_acc: 0.8364\n",
      "Epoch 12/85\n",
      " - 10s - loss: 0.7853 - acc: 0.6987 - val_loss: 0.6229 - val_acc: 0.8330\n",
      "Epoch 13/85\n",
      " - 10s - loss: 0.7770 - acc: 0.7065 - val_loss: 0.6043 - val_acc: 0.8361\n",
      "Epoch 14/85\n",
      " - 10s - loss: 0.7637 - acc: 0.7127 - val_loss: 0.6309 - val_acc: 0.8291\n",
      "Epoch 15/85\n",
      " - 10s - loss: 0.7608 - acc: 0.7166 - val_loss: 0.7978 - val_acc: 0.7563\n",
      "Epoch 16/85\n",
      " - 10s - loss: 0.7656 - acc: 0.7038 - val_loss: 0.6132 - val_acc: 0.8348\n",
      "Epoch 17/85\n",
      " - 10s - loss: 0.7404 - acc: 0.7276 - val_loss: 0.8069 - val_acc: 0.7465\n",
      "Epoch 18/85\n",
      " - 10s - loss: 0.7384 - acc: 0.7261 - val_loss: 0.8771 - val_acc: 0.6495\n",
      "Epoch 19/85\n",
      " - 10s - loss: 0.7312 - acc: 0.7307 - val_loss: 0.5806 - val_acc: 0.8404\n",
      "Epoch 20/85\n",
      " - 10s - loss: 0.7174 - acc: 0.7400 - val_loss: 0.8038 - val_acc: 0.7796\n",
      "Epoch 21/85\n",
      " - 10s - loss: 0.7257 - acc: 0.7349 - val_loss: 0.5813 - val_acc: 0.8397\n",
      "Epoch 22/85\n",
      " - 10s - loss: 0.7131 - acc: 0.7423 - val_loss: 0.6093 - val_acc: 0.8346\n",
      "Epoch 23/85\n",
      " - 10s - loss: 0.7141 - acc: 0.7393 - val_loss: 1.4246 - val_acc: 0.4080\n",
      "Epoch 24/85\n",
      " - 10s - loss: 0.7145 - acc: 0.7381 - val_loss: 0.6073 - val_acc: 0.8344\n",
      "Epoch 25/85\n",
      " - 10s - loss: 0.7085 - acc: 0.7417 - val_loss: 0.6471 - val_acc: 0.8279\n",
      "Epoch 26/85\n",
      " - 10s - loss: 0.6956 - acc: 0.7456 - val_loss: 0.5884 - val_acc: 0.8391\n",
      "Epoch 27/85\n",
      " - 10s - loss: 0.6987 - acc: 0.7446 - val_loss: 0.6305 - val_acc: 0.8360\n",
      "Epoch 28/85\n",
      " - 10s - loss: 0.6947 - acc: 0.7478 - val_loss: 1.0731 - val_acc: 0.4338\n",
      "Epoch 29/85\n",
      " - 10s - loss: 0.7085 - acc: 0.7361 - val_loss: 0.7170 - val_acc: 0.8292\n",
      "Epoch 30/85\n",
      " - 10s - loss: 0.6854 - acc: 0.7505 - val_loss: 0.5923 - val_acc: 0.8466\n",
      "Epoch 31/85\n",
      " - 10s - loss: 0.6880 - acc: 0.7505 - val_loss: 0.5776 - val_acc: 0.8462\n",
      "Epoch 32/85\n",
      " - 10s - loss: 0.6777 - acc: 0.7527 - val_loss: 0.5973 - val_acc: 0.8054\n",
      "Epoch 33/85\n",
      " - 10s - loss: 0.6776 - acc: 0.7539 - val_loss: 0.6104 - val_acc: 0.8430\n",
      "Epoch 34/85\n",
      " - 10s - loss: 0.6735 - acc: 0.7531 - val_loss: 0.5611 - val_acc: 0.8441\n",
      "Epoch 35/85\n",
      " - 10s - loss: 0.6694 - acc: 0.7552 - val_loss: 0.5566 - val_acc: 0.8461\n",
      "Epoch 36/85\n",
      " - 10s - loss: 0.6737 - acc: 0.7504 - val_loss: 0.6767 - val_acc: 0.8197\n",
      "Epoch 37/85\n",
      " - 10s - loss: 0.6697 - acc: 0.7556 - val_loss: 0.6582 - val_acc: 0.8117\n",
      "Epoch 38/85\n",
      " - 10s - loss: 0.6727 - acc: 0.7550 - val_loss: 0.7052 - val_acc: 0.8105\n",
      "Epoch 39/85\n",
      " - 10s - loss: 0.6659 - acc: 0.7583 - val_loss: 0.7640 - val_acc: 0.7187\n",
      "Epoch 40/85\n",
      " - 10s - loss: 0.6633 - acc: 0.7574 - val_loss: 0.7826 - val_acc: 0.7199\n",
      "Epoch 41/85\n",
      " - 10s - loss: 0.6643 - acc: 0.7596 - val_loss: 0.5657 - val_acc: 0.8373\n",
      "Epoch 42/85\n",
      " - 10s - loss: 0.6598 - acc: 0.7599 - val_loss: 0.7269 - val_acc: 0.7469\n",
      "Epoch 43/85\n",
      " - 10s - loss: 0.6547 - acc: 0.7596 - val_loss: 0.6629 - val_acc: 0.8182\n",
      "Epoch 44/85\n",
      " - 10s - loss: 0.6566 - acc: 0.7605 - val_loss: 0.5664 - val_acc: 0.8469\n",
      "Epoch 45/85\n",
      " - 10s - loss: 0.6542 - acc: 0.7630 - val_loss: 0.6023 - val_acc: 0.8475\n",
      "Epoch 46/85\n",
      " - 10s - loss: 0.6535 - acc: 0.7610 - val_loss: 0.8437 - val_acc: 0.6269\n",
      "Epoch 47/85\n",
      " - 10s - loss: 0.6517 - acc: 0.7611 - val_loss: 0.5599 - val_acc: 0.8440\n",
      "Epoch 48/85\n",
      " - 10s - loss: 0.6506 - acc: 0.7617 - val_loss: 0.6794 - val_acc: 0.7988\n",
      "Epoch 49/85\n",
      " - 10s - loss: 0.6477 - acc: 0.7624 - val_loss: 0.6082 - val_acc: 0.8300\n",
      "Epoch 50/85\n",
      " - 10s - loss: 0.6466 - acc: 0.7624 - val_loss: 0.7030 - val_acc: 0.7732\n",
      "Epoch 51/85\n",
      " - 10s - loss: 0.6433 - acc: 0.7651 - val_loss: 0.5671 - val_acc: 0.8471\n",
      "Epoch 52/85\n",
      " - 10s - loss: 0.6458 - acc: 0.7625 - val_loss: 0.5805 - val_acc: 0.8333\n",
      "Epoch 53/85\n",
      " - 10s - loss: 0.6451 - acc: 0.7619 - val_loss: 0.5844 - val_acc: 0.8455\n",
      "Epoch 54/85\n",
      " - 10s - loss: 0.6426 - acc: 0.7647 - val_loss: 0.5504 - val_acc: 0.8459\n",
      "Epoch 55/85\n",
      " - 10s - loss: 0.6421 - acc: 0.7654 - val_loss: 0.6197 - val_acc: 0.8278\n",
      "Epoch 56/85\n",
      " - 10s - loss: 0.6371 - acc: 0.7669 - val_loss: 0.6557 - val_acc: 0.8105\n",
      "Epoch 57/85\n",
      " - 10s - loss: 0.6371 - acc: 0.7666 - val_loss: 0.5873 - val_acc: 0.8375\n",
      "Epoch 58/85\n",
      " - 10s - loss: 0.6375 - acc: 0.7650 - val_loss: 0.5641 - val_acc: 0.8467\n",
      "Epoch 59/85\n",
      " - 10s - loss: 0.6355 - acc: 0.7690 - val_loss: 0.6098 - val_acc: 0.8338\n",
      "Epoch 60/85\n",
      " - 10s - loss: 0.6332 - acc: 0.7685 - val_loss: 0.5583 - val_acc: 0.8475\n",
      "Epoch 61/85\n",
      " - 10s - loss: 0.6338 - acc: 0.7677 - val_loss: 0.5570 - val_acc: 0.8443\n",
      "Epoch 62/85\n",
      " - 10s - loss: 0.6353 - acc: 0.7669 - val_loss: 0.5798 - val_acc: 0.8445\n",
      "Epoch 63/85\n",
      " - 10s - loss: 0.6326 - acc: 0.7687 - val_loss: 0.6525 - val_acc: 0.8017\n",
      "Epoch 64/85\n",
      " - 10s - loss: 0.6304 - acc: 0.7694 - val_loss: 0.5685 - val_acc: 0.8471\n",
      "Epoch 65/85\n",
      " - 10s - loss: 0.6304 - acc: 0.7691 - val_loss: 0.5737 - val_acc: 0.8410\n",
      "Epoch 66/85\n",
      " - 10s - loss: 0.6270 - acc: 0.7689 - val_loss: 0.6270 - val_acc: 0.8393\n",
      "Epoch 67/85\n",
      " - 10s - loss: 0.6280 - acc: 0.7700 - val_loss: 0.5518 - val_acc: 0.8489\n",
      "Epoch 68/85\n",
      " - 10s - loss: 0.6252 - acc: 0.7717 - val_loss: 0.5772 - val_acc: 0.8490\n",
      "Epoch 69/85\n",
      " - 10s - loss: 0.6241 - acc: 0.7712 - val_loss: 0.5588 - val_acc: 0.8454\n",
      "Epoch 70/85\n",
      " - 10s - loss: 0.6256 - acc: 0.7697 - val_loss: 0.5756 - val_acc: 0.8454\n",
      "Epoch 71/85\n",
      " - 10s - loss: 0.6257 - acc: 0.7709 - val_loss: 0.6354 - val_acc: 0.8328\n",
      "Epoch 72/85\n",
      " - 10s - loss: 0.6241 - acc: 0.7716 - val_loss: 0.5590 - val_acc: 0.8482\n",
      "Epoch 73/85\n",
      " - 10s - loss: 0.6219 - acc: 0.7723 - val_loss: 0.6608 - val_acc: 0.7840\n",
      "Epoch 74/85\n",
      " - 10s - loss: 0.6244 - acc: 0.7714 - val_loss: 0.5428 - val_acc: 0.8474\n",
      "Epoch 75/85\n",
      " - 10s - loss: 0.6227 - acc: 0.7730 - val_loss: 0.5772 - val_acc: 0.8486\n",
      "Epoch 76/85\n",
      " - 10s - loss: 0.6221 - acc: 0.7731 - val_loss: 0.5861 - val_acc: 0.8429\n",
      "Epoch 77/85\n",
      " - 10s - loss: 0.6212 - acc: 0.7728 - val_loss: 0.6150 - val_acc: 0.8161\n",
      "Epoch 78/85\n",
      " - 10s - loss: 0.6223 - acc: 0.7722 - val_loss: 0.5388 - val_acc: 0.8481\n",
      "Epoch 79/85\n",
      " - 10s - loss: 0.6196 - acc: 0.7723 - val_loss: 0.5424 - val_acc: 0.8478\n",
      "Epoch 80/85\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 10s - loss: 0.6172 - acc: 0.7733 - val_loss: 0.5538 - val_acc: 0.8485\n",
      "Epoch 81/85\n",
      " - 10s - loss: 0.6202 - acc: 0.7734 - val_loss: 0.5457 - val_acc: 0.8485\n",
      "Epoch 82/85\n",
      " - 10s - loss: 0.6179 - acc: 0.7746 - val_loss: 0.5606 - val_acc: 0.8487\n",
      "Epoch 83/85\n",
      " - 10s - loss: 0.6185 - acc: 0.7734 - val_loss: 0.5610 - val_acc: 0.8490\n",
      "Epoch 84/85\n",
      " - 10s - loss: 0.6187 - acc: 0.7739 - val_loss: 0.5618 - val_acc: 0.8473\n",
      "Epoch 85/85\n",
      " - 10s - loss: 0.6162 - acc: 0.7738 - val_loss: 0.5469 - val_acc: 0.8488\n",
      "49958/49958 [==============================] - 6s 112us/step\n",
      " Dropout : 0.15 accuracy is : 0.792285519837\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_19 (LSTM)               (None, 1, 600)            4996800   \n",
      "_________________________________________________________________\n",
      "dropout_19 (Dropout)         (None, 1, 600)            0         \n",
      "_________________________________________________________________\n",
      "lstm_20 (LSTM)               (None, 400)               1601600   \n",
      "_________________________________________________________________\n",
      "dropout_20 (Dropout)         (None, 400)               0         \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 150)               60150     \n",
      "_________________________________________________________________\n",
      "dense_20 (Dense)             (None, 3)                 453       \n",
      "=================================================================\n",
      "Total params: 6,659,003\n",
      "Trainable params: 6,659,003\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      " Model build done!\n",
      "Train on 159866 samples, validate on 39967 samples\n",
      "Epoch 1/85\n",
      " - 12s - loss: 1.0451 - acc: 0.5368 - val_loss: 1.1083 - val_acc: 0.5333\n",
      "Epoch 2/85\n",
      " - 10s - loss: 1.0389 - acc: 0.5332 - val_loss: 1.0234 - val_acc: 0.5333\n",
      "Epoch 3/85\n",
      " - 10s - loss: 0.9891 - acc: 0.5597 - val_loss: 0.8577 - val_acc: 0.6555\n",
      "Epoch 4/85\n",
      " - 10s - loss: 0.9634 - acc: 0.5704 - val_loss: 0.9725 - val_acc: 0.6001\n",
      "Epoch 5/85\n",
      " - 10s - loss: 0.9210 - acc: 0.5915 - val_loss: 0.8363 - val_acc: 0.7191\n",
      "Epoch 6/85\n",
      " - 10s - loss: 0.8967 - acc: 0.6122 - val_loss: 0.7449 - val_acc: 0.7677\n",
      "Epoch 7/85\n",
      " - 10s - loss: 0.8677 - acc: 0.6338 - val_loss: 1.0941 - val_acc: 0.5495\n",
      "Epoch 8/85\n",
      " - 10s - loss: 0.8467 - acc: 0.6487 - val_loss: 0.7544 - val_acc: 0.7783\n",
      "Epoch 9/85\n",
      " - 10s - loss: 0.8230 - acc: 0.6686 - val_loss: 0.6968 - val_acc: 0.8071\n",
      "Epoch 10/85\n",
      " - 10s - loss: 0.8058 - acc: 0.6806 - val_loss: 0.6134 - val_acc: 0.8285\n",
      "Epoch 11/85\n",
      " - 10s - loss: 0.7909 - acc: 0.6903 - val_loss: 0.6415 - val_acc: 0.8267\n",
      "Epoch 12/85\n",
      " - 10s - loss: 0.7852 - acc: 0.6949 - val_loss: 0.5994 - val_acc: 0.8380\n",
      "Epoch 13/85\n",
      " - 10s - loss: 0.7716 - acc: 0.7079 - val_loss: 0.6025 - val_acc: 0.8380\n",
      "Epoch 14/85\n",
      " - 10s - loss: 0.7710 - acc: 0.7017 - val_loss: 0.7110 - val_acc: 0.7971\n",
      "Epoch 15/85\n",
      " - 10s - loss: 0.7531 - acc: 0.7180 - val_loss: 0.5924 - val_acc: 0.8325\n",
      "Epoch 16/85\n",
      " - 10s - loss: 0.7477 - acc: 0.7225 - val_loss: 0.5910 - val_acc: 0.8372\n",
      "Epoch 17/85\n",
      " - 10s - loss: 0.7871 - acc: 0.6946 - val_loss: 0.8022 - val_acc: 0.6649\n",
      "Epoch 18/85\n",
      " - 10s - loss: 0.7415 - acc: 0.7190 - val_loss: 0.6766 - val_acc: 0.8014\n",
      "Epoch 19/85\n",
      " - 10s - loss: 0.7266 - acc: 0.7365 - val_loss: 0.5852 - val_acc: 0.8423\n",
      "Epoch 20/85\n",
      " - 10s - loss: 0.7263 - acc: 0.7348 - val_loss: 0.5791 - val_acc: 0.8408\n",
      "Epoch 21/85\n",
      " - 10s - loss: 0.7181 - acc: 0.7402 - val_loss: 0.6180 - val_acc: 0.8292\n",
      "Epoch 22/85\n",
      " - 10s - loss: 0.7171 - acc: 0.7388 - val_loss: 0.7933 - val_acc: 0.7517\n",
      "Epoch 23/85\n",
      " - 10s - loss: 0.7112 - acc: 0.7401 - val_loss: 0.5672 - val_acc: 0.8435\n",
      "Epoch 24/85\n",
      " - 10s - loss: 0.6998 - acc: 0.7495 - val_loss: 0.7257 - val_acc: 0.7763\n",
      "Epoch 25/85\n",
      " - 10s - loss: 0.7062 - acc: 0.7425 - val_loss: 0.8325 - val_acc: 0.6466\n",
      "Epoch 26/85\n",
      " - 10s - loss: 0.6997 - acc: 0.7472 - val_loss: 0.6416 - val_acc: 0.8118\n",
      "Epoch 27/85\n",
      " - 10s - loss: 0.6947 - acc: 0.7504 - val_loss: 0.8295 - val_acc: 0.7148\n",
      "Epoch 28/85\n",
      " - 10s - loss: 0.6964 - acc: 0.7489 - val_loss: 0.6505 - val_acc: 0.8286\n",
      "Epoch 29/85\n",
      " - 10s - loss: 0.6896 - acc: 0.7506 - val_loss: 0.6318 - val_acc: 0.7772\n",
      "Epoch 30/85\n",
      " - 10s - loss: 0.6865 - acc: 0.7523 - val_loss: 0.5617 - val_acc: 0.8470\n",
      "Epoch 31/85\n",
      " - 10s - loss: 0.6820 - acc: 0.7532 - val_loss: 0.8699 - val_acc: 0.6237\n",
      "Epoch 32/85\n",
      " - 10s - loss: 0.6832 - acc: 0.7533 - val_loss: 0.7959 - val_acc: 0.6966\n",
      "Epoch 33/85\n",
      " - 10s - loss: 0.6793 - acc: 0.7551 - val_loss: 1.2841 - val_acc: 0.4057\n",
      "Epoch 34/85\n",
      " - 10s - loss: 0.6762 - acc: 0.7546 - val_loss: 0.8113 - val_acc: 0.6828\n",
      "Epoch 35/85\n",
      " - 10s - loss: 0.6686 - acc: 0.7583 - val_loss: 0.6879 - val_acc: 0.8213\n",
      "Epoch 36/85\n",
      " - 10s - loss: 0.6727 - acc: 0.7555 - val_loss: 0.5862 - val_acc: 0.8487\n",
      "Epoch 37/85\n",
      " - 10s - loss: 0.6679 - acc: 0.7593 - val_loss: 0.7243 - val_acc: 0.7804\n",
      "Epoch 38/85\n",
      " - 10s - loss: 0.6704 - acc: 0.7555 - val_loss: 0.7342 - val_acc: 0.8359\n",
      "Epoch 39/85\n",
      " - 10s - loss: 0.6653 - acc: 0.7572 - val_loss: 0.6268 - val_acc: 0.8374\n",
      "Epoch 40/85\n",
      " - 10s - loss: 0.6651 - acc: 0.7591 - val_loss: 0.6413 - val_acc: 0.8186\n",
      "Epoch 41/85\n",
      " - 10s - loss: 0.6623 - acc: 0.7583 - val_loss: 0.5749 - val_acc: 0.8419\n",
      "Epoch 42/85\n",
      " - 10s - loss: 0.6601 - acc: 0.7606 - val_loss: 0.7285 - val_acc: 0.7802\n",
      "Epoch 43/85\n",
      " - 10s - loss: 0.6576 - acc: 0.7605 - val_loss: 0.6449 - val_acc: 0.8431\n",
      "Epoch 44/85\n",
      " - 10s - loss: 0.6582 - acc: 0.7605 - val_loss: 0.5811 - val_acc: 0.8490\n",
      "Epoch 45/85\n",
      " - 10s - loss: 0.6522 - acc: 0.7641 - val_loss: 0.6764 - val_acc: 0.7875\n",
      "Epoch 46/85\n",
      " - 10s - loss: 0.6580 - acc: 0.7593 - val_loss: 0.5885 - val_acc: 0.8483\n",
      "Epoch 47/85\n",
      " - 10s - loss: 0.6533 - acc: 0.7633 - val_loss: 0.6575 - val_acc: 0.8355\n",
      "Epoch 48/85\n",
      " - 10s - loss: 0.6493 - acc: 0.7646 - val_loss: 0.5514 - val_acc: 0.8461\n",
      "Epoch 49/85\n",
      " - 10s - loss: 0.6483 - acc: 0.7657 - val_loss: 0.5984 - val_acc: 0.8375\n",
      "Epoch 50/85\n",
      " - 10s - loss: 0.6484 - acc: 0.7631 - val_loss: 0.5900 - val_acc: 0.8461\n",
      "Epoch 51/85\n",
      " - 10s - loss: 0.6429 - acc: 0.7674 - val_loss: 0.5792 - val_acc: 0.8486\n",
      "Epoch 52/85\n",
      " - 10s - loss: 0.6459 - acc: 0.7639 - val_loss: 0.6357 - val_acc: 0.8282\n",
      "Epoch 53/85\n",
      " - 10s - loss: 0.6457 - acc: 0.7660 - val_loss: 0.5771 - val_acc: 0.8437\n",
      "Epoch 54/85\n",
      " - 10s - loss: 0.6431 - acc: 0.7656 - val_loss: 0.5979 - val_acc: 0.8443\n",
      "Epoch 55/85\n",
      " - 10s - loss: 0.6400 - acc: 0.7665 - val_loss: 0.5699 - val_acc: 0.8429\n",
      "Epoch 56/85\n",
      " - 10s - loss: 0.6420 - acc: 0.7654 - val_loss: 0.5612 - val_acc: 0.8440\n",
      "Epoch 57/85\n",
      " - 10s - loss: 0.6409 - acc: 0.7663 - val_loss: 0.5635 - val_acc: 0.8474\n",
      "Epoch 58/85\n",
      " - 10s - loss: 0.6366 - acc: 0.7696 - val_loss: 0.5816 - val_acc: 0.8347\n",
      "Epoch 59/85\n",
      " - 10s - loss: 0.6377 - acc: 0.7688 - val_loss: 0.5788 - val_acc: 0.8451\n",
      "Epoch 60/85\n",
      " - 10s - loss: 0.6379 - acc: 0.7683 - val_loss: 0.5988 - val_acc: 0.8355\n",
      "Epoch 61/85\n",
      " - 10s - loss: 0.6345 - acc: 0.7687 - val_loss: 0.7487 - val_acc: 0.7266\n",
      "Epoch 62/85\n",
      " - 10s - loss: 0.6386 - acc: 0.7677 - val_loss: 0.5817 - val_acc: 0.8418\n",
      "Epoch 63/85\n",
      " - 10s - loss: 0.6372 - acc: 0.7687 - val_loss: 0.5583 - val_acc: 0.8510\n",
      "Epoch 64/85\n",
      " - 10s - loss: 0.6358 - acc: 0.7668 - val_loss: 0.5429 - val_acc: 0.8481\n",
      "Epoch 65/85\n",
      " - 10s - loss: 0.6363 - acc: 0.7685 - val_loss: 0.5553 - val_acc: 0.8444\n",
      "Epoch 66/85\n",
      " - 10s - loss: 0.6325 - acc: 0.7702 - val_loss: 0.7297 - val_acc: 0.7459\n",
      "Epoch 67/85\n",
      " - 10s - loss: 0.6379 - acc: 0.7673 - val_loss: 0.5911 - val_acc: 0.8400\n",
      "Epoch 68/85\n",
      " - 10s - loss: 0.6333 - acc: 0.7689 - val_loss: 0.5640 - val_acc: 0.8466\n",
      "Epoch 69/85\n",
      " - 10s - loss: 0.6318 - acc: 0.7703 - val_loss: 0.5479 - val_acc: 0.8461\n",
      "Epoch 70/85\n",
      " - 10s - loss: 0.6327 - acc: 0.7693 - val_loss: 0.5444 - val_acc: 0.8514\n",
      "Epoch 71/85\n",
      " - 10s - loss: 0.6282 - acc: 0.7704 - val_loss: 0.5648 - val_acc: 0.8465\n",
      "Epoch 72/85\n",
      " - 10s - loss: 0.6281 - acc: 0.7707 - val_loss: 0.5766 - val_acc: 0.8376\n",
      "Epoch 73/85\n",
      " - 10s - loss: 0.6314 - acc: 0.7692 - val_loss: 0.5512 - val_acc: 0.8489\n",
      "Epoch 74/85\n",
      " - 10s - loss: 0.6338 - acc: 0.7691 - val_loss: 0.5599 - val_acc: 0.8445\n",
      "Epoch 75/85\n",
      " - 10s - loss: 0.6312 - acc: 0.7690 - val_loss: 0.5442 - val_acc: 0.8476\n",
      "Epoch 76/85\n",
      " - 10s - loss: 0.6282 - acc: 0.7705 - val_loss: 0.5787 - val_acc: 0.8308\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 77/85\n",
      " - 10s - loss: 0.6267 - acc: 0.7728 - val_loss: 0.5528 - val_acc: 0.8475\n",
      "Epoch 78/85\n",
      " - 10s - loss: 0.6272 - acc: 0.7708 - val_loss: 0.5684 - val_acc: 0.8453\n",
      "Epoch 79/85\n",
      " - 10s - loss: 0.6270 - acc: 0.7714 - val_loss: 0.5573 - val_acc: 0.8446\n",
      "Epoch 80/85\n",
      " - 10s - loss: 0.6257 - acc: 0.7712 - val_loss: 0.5633 - val_acc: 0.8475\n",
      "Epoch 81/85\n",
      " - 10s - loss: 0.6251 - acc: 0.7731 - val_loss: 0.5779 - val_acc: 0.8372\n",
      "Epoch 82/85\n",
      " - 10s - loss: 0.6254 - acc: 0.7719 - val_loss: 0.5423 - val_acc: 0.8517\n",
      "Epoch 83/85\n",
      " - 10s - loss: 0.6271 - acc: 0.7726 - val_loss: 0.5613 - val_acc: 0.8427\n",
      "Epoch 84/85\n",
      " - 10s - loss: 0.6247 - acc: 0.7720 - val_loss: 0.5703 - val_acc: 0.8383\n",
      "Epoch 85/85\n",
      " - 10s - loss: 0.6232 - acc: 0.7729 - val_loss: 0.5569 - val_acc: 0.8473\n",
      "49958/49958 [==============================] - 6s 112us/step\n",
      " Dropout : 0.2 accuracy is : 0.820729412707\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_21 (LSTM)               (None, 1, 600)            4996800   \n",
      "_________________________________________________________________\n",
      "dropout_21 (Dropout)         (None, 1, 600)            0         \n",
      "_________________________________________________________________\n",
      "lstm_22 (LSTM)               (None, 400)               1601600   \n",
      "_________________________________________________________________\n",
      "dropout_22 (Dropout)         (None, 400)               0         \n",
      "_________________________________________________________________\n",
      "dense_21 (Dense)             (None, 150)               60150     \n",
      "_________________________________________________________________\n",
      "dense_22 (Dense)             (None, 3)                 453       \n",
      "=================================================================\n",
      "Total params: 6,659,003\n",
      "Trainable params: 6,659,003\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      " Model build done!\n",
      "Train on 159866 samples, validate on 39967 samples\n",
      "Epoch 1/85\n",
      " - 12s - loss: 1.0452 - acc: 0.5374 - val_loss: 1.1077 - val_acc: 0.5333\n",
      "Epoch 2/85\n",
      " - 10s - loss: 1.0369 - acc: 0.5340 - val_loss: 0.9903 - val_acc: 0.5442\n",
      "Epoch 3/85\n",
      " - 10s - loss: 0.9917 - acc: 0.5542 - val_loss: 0.8870 - val_acc: 0.6492\n",
      "Epoch 4/85\n",
      " - 10s - loss: 0.9608 - acc: 0.5670 - val_loss: 0.8810 - val_acc: 0.6514\n",
      "Epoch 5/85\n",
      " - 10s - loss: 0.9282 - acc: 0.5880 - val_loss: 0.9210 - val_acc: 0.6142\n",
      "Epoch 6/85\n",
      " - 10s - loss: 0.9054 - acc: 0.6047 - val_loss: 0.8808 - val_acc: 0.6542\n",
      "Epoch 7/85\n",
      " - 10s - loss: 0.8763 - acc: 0.6264 - val_loss: 0.9055 - val_acc: 0.6186\n",
      "Epoch 8/85\n",
      " - 10s - loss: 0.8528 - acc: 0.6428 - val_loss: 0.9110 - val_acc: 0.6352\n",
      "Epoch 9/85\n",
      " - 10s - loss: 0.8350 - acc: 0.6597 - val_loss: 0.7855 - val_acc: 0.7651\n",
      "Epoch 10/85\n",
      " - 10s - loss: 0.8101 - acc: 0.6783 - val_loss: 0.7470 - val_acc: 0.7729\n",
      "Epoch 11/85\n",
      " - 10s - loss: 0.7948 - acc: 0.6917 - val_loss: 0.6202 - val_acc: 0.8350\n",
      "Epoch 12/85\n",
      " - 10s - loss: 0.7878 - acc: 0.6909 - val_loss: 0.6107 - val_acc: 0.8355\n",
      "Epoch 13/85\n",
      " - 10s - loss: 0.7741 - acc: 0.7073 - val_loss: 0.6227 - val_acc: 0.8300\n",
      "Epoch 14/85\n",
      " - 10s - loss: 0.7638 - acc: 0.7113 - val_loss: 0.5919 - val_acc: 0.8388\n",
      "Epoch 15/85\n",
      " - 10s - loss: 0.7617 - acc: 0.7159 - val_loss: 0.5988 - val_acc: 0.8362\n",
      "Epoch 16/85\n",
      " - 10s - loss: 0.7582 - acc: 0.7137 - val_loss: 0.6302 - val_acc: 0.8267\n",
      "Epoch 17/85\n",
      " - 10s - loss: 0.7419 - acc: 0.7290 - val_loss: 1.1088 - val_acc: 0.4895\n",
      "Epoch 18/85\n",
      " - 10s - loss: 0.7420 - acc: 0.7268 - val_loss: 0.6015 - val_acc: 0.8329\n",
      "Epoch 19/85\n",
      " - 10s - loss: 0.7334 - acc: 0.7336 - val_loss: 0.5870 - val_acc: 0.8365\n",
      "Epoch 20/85\n",
      " - 10s - loss: 0.7307 - acc: 0.7362 - val_loss: 1.1460 - val_acc: 0.4883\n",
      "Epoch 21/85\n",
      " - 10s - loss: 0.7273 - acc: 0.7350 - val_loss: 0.5741 - val_acc: 0.8406\n",
      "Epoch 22/85\n",
      " - 10s - loss: 0.7221 - acc: 0.7393 - val_loss: 0.5847 - val_acc: 0.8395\n",
      "Epoch 23/85\n",
      " - 10s - loss: 0.7184 - acc: 0.7397 - val_loss: 0.7304 - val_acc: 0.7863\n",
      "Epoch 24/85\n",
      " - 10s - loss: 0.7190 - acc: 0.7404 - val_loss: 0.6329 - val_acc: 0.8265\n",
      "Epoch 25/85\n",
      " - 10s - loss: 0.7113 - acc: 0.7436 - val_loss: 0.5944 - val_acc: 0.8375\n",
      "Epoch 26/85\n",
      " - 10s - loss: 0.7183 - acc: 0.7388 - val_loss: 0.7061 - val_acc: 0.7241\n",
      "Epoch 27/85\n",
      " - 10s - loss: 0.7102 - acc: 0.7436 - val_loss: 1.4545 - val_acc: 0.4019\n",
      "Epoch 28/85\n",
      " - 10s - loss: 0.7215 - acc: 0.7249 - val_loss: 0.5872 - val_acc: 0.8378\n",
      "Epoch 29/85\n",
      " - 10s - loss: 0.7006 - acc: 0.7477 - val_loss: 0.5830 - val_acc: 0.8442\n",
      "Epoch 30/85\n",
      " - 10s - loss: 0.6948 - acc: 0.7503 - val_loss: 0.5990 - val_acc: 0.8355\n",
      "Epoch 31/85\n",
      " - 10s - loss: 0.7040 - acc: 0.7459 - val_loss: 0.6402 - val_acc: 0.8164\n",
      "Epoch 32/85\n",
      " - 10s - loss: 0.6932 - acc: 0.7530 - val_loss: 0.6182 - val_acc: 0.8324\n",
      "Epoch 33/85\n",
      " - 10s - loss: 0.6911 - acc: 0.7500 - val_loss: 0.6043 - val_acc: 0.8337\n",
      "Epoch 34/85\n",
      " - 10s - loss: 0.6857 - acc: 0.7537 - val_loss: 0.5897 - val_acc: 0.8439\n",
      "Epoch 35/85\n",
      " - 10s - loss: 0.6861 - acc: 0.7533 - val_loss: 0.5820 - val_acc: 0.8423\n",
      "Epoch 36/85\n",
      " - 10s - loss: 0.6850 - acc: 0.7533 - val_loss: 0.6520 - val_acc: 0.8180\n",
      "Epoch 37/85\n",
      " - 10s - loss: 0.6797 - acc: 0.7562 - val_loss: 0.6040 - val_acc: 0.8399\n",
      "Epoch 38/85\n",
      " - 10s - loss: 0.6772 - acc: 0.7561 - val_loss: 0.6363 - val_acc: 0.7707\n",
      "Epoch 39/85\n",
      " - 10s - loss: 0.6771 - acc: 0.7562 - val_loss: 0.7347 - val_acc: 0.6789\n",
      "Epoch 40/85\n",
      " - 10s - loss: 0.6760 - acc: 0.7547 - val_loss: 0.6634 - val_acc: 0.8077\n",
      "Epoch 41/85\n",
      " - 10s - loss: 0.6726 - acc: 0.7575 - val_loss: 0.5918 - val_acc: 0.8392\n",
      "Epoch 42/85\n",
      " - 10s - loss: 0.6703 - acc: 0.7592 - val_loss: 0.6400 - val_acc: 0.7658\n",
      "Epoch 43/85\n",
      " - 10s - loss: 0.6677 - acc: 0.7572 - val_loss: 0.6190 - val_acc: 0.8438\n",
      "Epoch 44/85\n",
      " - 10s - loss: 0.6653 - acc: 0.7591 - val_loss: 0.5755 - val_acc: 0.8410\n",
      "Epoch 45/85\n",
      " - 10s - loss: 0.6653 - acc: 0.7587 - val_loss: 0.5912 - val_acc: 0.8390\n",
      "Epoch 46/85\n",
      " - 10s - loss: 0.6647 - acc: 0.7586 - val_loss: 0.5841 - val_acc: 0.8386\n",
      "Epoch 47/85\n",
      " - 10s - loss: 0.6597 - acc: 0.7604 - val_loss: 0.6004 - val_acc: 0.8396\n",
      "Epoch 48/85\n",
      " - 10s - loss: 0.6596 - acc: 0.7607 - val_loss: 0.5668 - val_acc: 0.8467\n",
      "Epoch 49/85\n",
      " - 10s - loss: 0.6600 - acc: 0.7579 - val_loss: 0.6701 - val_acc: 0.7961\n",
      "Epoch 50/85\n",
      " - 10s - loss: 0.6542 - acc: 0.7628 - val_loss: 0.5819 - val_acc: 0.8407\n",
      "Epoch 51/85\n",
      " - 10s - loss: 0.6541 - acc: 0.7621 - val_loss: 0.5726 - val_acc: 0.8418\n",
      "Epoch 52/85\n",
      " - 10s - loss: 0.6536 - acc: 0.7623 - val_loss: 0.5859 - val_acc: 0.8431\n",
      "Epoch 53/85\n",
      " - 10s - loss: 0.6541 - acc: 0.7623 - val_loss: 0.9569 - val_acc: 0.6522\n",
      "Epoch 54/85\n",
      " - 10s - loss: 0.6521 - acc: 0.7639 - val_loss: 0.7996 - val_acc: 0.7290\n",
      "Epoch 55/85\n",
      " - 10s - loss: 0.6524 - acc: 0.7628 - val_loss: 0.6039 - val_acc: 0.8203\n",
      "Epoch 56/85\n",
      " - 10s - loss: 0.6492 - acc: 0.7639 - val_loss: 0.5707 - val_acc: 0.8427\n",
      "Epoch 57/85\n",
      " - 10s - loss: 0.6475 - acc: 0.7636 - val_loss: 0.5618 - val_acc: 0.8444\n",
      "Epoch 58/85\n",
      " - 10s - loss: 0.6478 - acc: 0.7636 - val_loss: 0.6043 - val_acc: 0.8444\n",
      "Epoch 59/85\n",
      " - 10s - loss: 0.6477 - acc: 0.7637 - val_loss: 0.5824 - val_acc: 0.8478\n",
      "Epoch 60/85\n",
      " - 10s - loss: 0.6472 - acc: 0.7633 - val_loss: 0.5693 - val_acc: 0.8430\n",
      "Epoch 61/85\n",
      " - 10s - loss: 0.6417 - acc: 0.7666 - val_loss: 0.6202 - val_acc: 0.8274\n",
      "Epoch 62/85\n",
      " - 10s - loss: 0.6435 - acc: 0.7665 - val_loss: 0.5665 - val_acc: 0.8439\n",
      "Epoch 63/85\n",
      " - 10s - loss: 0.6444 - acc: 0.7669 - val_loss: 0.6117 - val_acc: 0.8339\n",
      "Epoch 64/85\n",
      " - 10s - loss: 0.6416 - acc: 0.7680 - val_loss: 0.5590 - val_acc: 0.8434\n",
      "Epoch 65/85\n",
      " - 10s - loss: 0.6421 - acc: 0.7670 - val_loss: 0.5643 - val_acc: 0.8426\n",
      "Epoch 66/85\n",
      " - 10s - loss: 0.6418 - acc: 0.7666 - val_loss: 0.5630 - val_acc: 0.8467\n",
      "Epoch 67/85\n",
      " - 10s - loss: 0.6392 - acc: 0.7674 - val_loss: 0.5872 - val_acc: 0.8444\n",
      "Epoch 68/85\n",
      " - 10s - loss: 0.6430 - acc: 0.7640 - val_loss: 0.6016 - val_acc: 0.8305\n",
      "Epoch 69/85\n",
      " - 10s - loss: 0.6391 - acc: 0.7660 - val_loss: 0.5940 - val_acc: 0.8481\n",
      "Epoch 70/85\n",
      " - 10s - loss: 0.6382 - acc: 0.7675 - val_loss: 0.5752 - val_acc: 0.8381\n",
      "Epoch 71/85\n",
      " - 10s - loss: 0.6391 - acc: 0.7673 - val_loss: 0.5455 - val_acc: 0.8492\n",
      "Epoch 72/85\n",
      " - 10s - loss: 0.6395 - acc: 0.7683 - val_loss: 0.5588 - val_acc: 0.8472\n",
      "Epoch 73/85\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 10s - loss: 0.6402 - acc: 0.7662 - val_loss: 0.5812 - val_acc: 0.8204\n",
      "Epoch 74/85\n",
      " - 10s - loss: 0.6378 - acc: 0.7666 - val_loss: 0.5617 - val_acc: 0.8451\n",
      "Epoch 75/85\n",
      " - 10s - loss: 0.6419 - acc: 0.7617 - val_loss: 0.5460 - val_acc: 0.8489\n",
      "Epoch 76/85\n",
      " - 10s - loss: 0.6350 - acc: 0.7693 - val_loss: 0.5720 - val_acc: 0.8476\n",
      "Epoch 77/85\n",
      " - 10s - loss: 0.6365 - acc: 0.7671 - val_loss: 0.5674 - val_acc: 0.8454\n",
      "Epoch 78/85\n",
      " - 10s - loss: 0.6358 - acc: 0.7687 - val_loss: 0.5539 - val_acc: 0.8493\n",
      "Epoch 79/85\n",
      " - 10s - loss: 0.6387 - acc: 0.7653 - val_loss: 0.5540 - val_acc: 0.8480\n",
      "Epoch 80/85\n",
      " - 10s - loss: 0.6337 - acc: 0.7680 - val_loss: 0.5519 - val_acc: 0.8454\n",
      "Epoch 81/85\n",
      " - 10s - loss: 0.6340 - acc: 0.7683 - val_loss: 0.5769 - val_acc: 0.8472\n",
      "Epoch 82/85\n",
      " - 10s - loss: 0.6349 - acc: 0.7656 - val_loss: 0.5542 - val_acc: 0.8464\n",
      "Epoch 83/85\n",
      " - 10s - loss: 0.6316 - acc: 0.7698 - val_loss: 0.5995 - val_acc: 0.8288\n",
      "Epoch 84/85\n",
      " - 10s - loss: 0.6464 - acc: 0.7608 - val_loss: 0.5489 - val_acc: 0.8461\n",
      "Epoch 85/85\n",
      " - 10s - loss: 0.6317 - acc: 0.7698 - val_loss: 0.5934 - val_acc: 0.8376\n",
      "49958/49958 [==============================] - 6s 113us/step\n",
      " Dropout : 0.25 accuracy is : 0.768725729613\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_23 (LSTM)               (None, 1, 600)            4996800   \n",
      "_________________________________________________________________\n",
      "dropout_23 (Dropout)         (None, 1, 600)            0         \n",
      "_________________________________________________________________\n",
      "lstm_24 (LSTM)               (None, 400)               1601600   \n",
      "_________________________________________________________________\n",
      "dropout_24 (Dropout)         (None, 400)               0         \n",
      "_________________________________________________________________\n",
      "dense_23 (Dense)             (None, 150)               60150     \n",
      "_________________________________________________________________\n",
      "dense_24 (Dense)             (None, 3)                 453       \n",
      "=================================================================\n",
      "Total params: 6,659,003\n",
      "Trainable params: 6,659,003\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      " Model build done!\n",
      "Train on 159866 samples, validate on 39967 samples\n",
      "Epoch 1/85\n",
      " - 12s - loss: 1.0453 - acc: 0.5369 - val_loss: 1.1081 - val_acc: 0.5333\n",
      "Epoch 2/85\n",
      " - 10s - loss: 1.0411 - acc: 0.5332 - val_loss: 1.0357 - val_acc: 0.5333\n",
      "Epoch 3/85\n",
      " - 10s - loss: 1.0029 - acc: 0.5537 - val_loss: 0.8628 - val_acc: 0.6567\n",
      "Epoch 4/85\n",
      " - 10s - loss: 0.9716 - acc: 0.5636 - val_loss: 0.8800 - val_acc: 0.6521\n",
      "Epoch 5/85\n",
      " - 10s - loss: 0.7551 - acc: 0.7204 - val_loss: 0.6131 - val_acc: 0.8279\n",
      "Epoch 17/85\n",
      " - 10s - loss: 0.7567 - acc: 0.7182 - val_loss: 0.6398 - val_acc: 0.8314\n",
      "Epoch 18/85\n",
      " - 10s - loss: 0.7397 - acc: 0.7280 - val_loss: 0.5861 - val_acc: 0.8404\n",
      "Epoch 19/85\n",
      " - 10s - loss: 0.7440 - acc: 0.7216 - val_loss: 0.6493 - val_acc: 0.8073\n",
      "Epoch 20/85\n",
      " - 10s - loss: 0.7369 - acc: 0.7282 - val_loss: 0.7013 - val_acc: 0.8005\n",
      "Epoch 21/85\n",
      " - 10s - loss: 0.7278 - acc: 0.7361 - val_loss: 0.5796 - val_acc: 0.8374\n",
      "Epoch 22/85\n",
      " - 10s - loss: 0.7214 - acc: 0.7400 - val_loss: 0.5929 - val_acc: 0.8381\n",
      "Epoch 23/85\n",
      " - 10s - loss: 0.7231 - acc: 0.7366 - val_loss: 0.7841 - val_acc: 0.7332\n",
      "Epoch 24/85\n",
      " - 10s - loss: 0.7210 - acc: 0.7383 - val_loss: 0.6060 - val_acc: 0.8354\n",
      "Epoch 25/85\n",
      " - 10s - loss: 0.7146 - acc: 0.7419 - val_loss: 0.6469 - val_acc: 0.8390\n",
      "Epoch 26/85\n",
      " - 10s - loss: 0.7138 - acc: 0.7408 - val_loss: 0.6676 - val_acc: 0.8206\n",
      "Epoch 27/85\n",
      " - 10s - loss: 0.7076 - acc: 0.7430 - val_loss: 0.6264 - val_acc: 0.8370\n",
      "Epoch 28/85\n",
      " - 10s - loss: 0.7018 - acc: 0.7478 - val_loss: 0.6016 - val_acc: 0.8421\n",
      "Epoch 29/85\n",
      " - 10s - loss: 0.6986 - acc: 0.7488 - val_loss: 0.6119 - val_acc: 0.8368\n",
      "Epoch 30/85\n",
      " - 10s - loss: 0.7018 - acc: 0.7448 - val_loss: 0.5692 - val_acc: 0.8436\n",
      "Epoch 31/85\n",
      " - 10s - loss: 0.7035 - acc: 0.7449 - val_loss: 0.5602 - val_acc: 0.8408\n",
      "Epoch 32/85\n",
      " - 10s - loss: 0.6926 - acc: 0.7524 - val_loss: 0.5709 - val_acc: 0.8415\n",
      "Epoch 33/85\n",
      " - 10s - loss: 0.6913 - acc: 0.7526 - val_loss: 0.6005 - val_acc: 0.8412\n",
      "Epoch 34/85\n",
      " - 10s - loss: 0.6890 - acc: 0.7527 - val_loss: 0.5787 - val_acc: 0.8465\n",
      "Epoch 35/85\n",
      " - 10s - loss: 0.6853 - acc: 0.7537 - val_loss: 0.5733 - val_acc: 0.8439\n",
      "Epoch 36/85\n",
      " - 10s - loss: 0.6886 - acc: 0.7522 - val_loss: 0.5517 - val_acc: 0.8423\n",
      "Epoch 37/85\n",
      " - 10s - loss: 0.6837 - acc: 0.7545 - val_loss: 0.6049 - val_acc: 0.8346\n",
      "Epoch 38/85\n",
      " - 10s - loss: 0.6843 - acc: 0.7532 - val_loss: 0.5831 - val_acc: 0.8373\n",
      "Epoch 39/85\n",
      " - 10s - loss: 0.6799 - acc: 0.7550 - val_loss: 0.6225 - val_acc: 0.8167\n",
      "Epoch 40/85\n",
      " - 10s - loss: 0.6786 - acc: 0.7562 - val_loss: 0.7398 - val_acc: 0.7572\n",
      "Epoch 41/85\n",
      " - 10s - loss: 0.6741 - acc: 0.7584 - val_loss: 0.6054 - val_acc: 0.8276\n",
      "Epoch 42/85\n",
      " - 10s - loss: 0.6709 - acc: 0.7589 - val_loss: 0.5630 - val_acc: 0.8461\n",
      "Epoch 43/85\n",
      " - 10s - loss: 0.6727 - acc: 0.7574 - val_loss: 0.5802 - val_acc: 0.8397\n",
      "Epoch 44/85\n",
      " - 10s - loss: 0.6700 - acc: 0.7582 - val_loss: 0.5786 - val_acc: 0.8408\n",
      "Epoch 45/85\n",
      " - 10s - loss: 0.6656 - acc: 0.7601 - val_loss: 0.5457 - val_acc: 0.8461\n",
      "Epoch 46/85\n",
      " - 10s - loss: 0.6681 - acc: 0.7575 - val_loss: 0.5511 - val_acc: 0.8467\n",
      "Epoch 47/85\n",
      " - 10s - loss: 0.6651 - acc: 0.7596 - val_loss: 0.5495 - val_acc: 0.8445\n",
      "Epoch 48/85\n",
      " - 10s - loss: 0.6609 - acc: 0.7624 - val_loss: 0.5568 - val_acc: 0.8445\n",
      "Epoch 49/85\n",
      " - 10s - loss: 0.6621 - acc: 0.7630 - val_loss: 0.6147 - val_acc: 0.8300\n",
      "Epoch 50/85\n",
      " - 10s - loss: 0.6602 - acc: 0.7619 - val_loss: 0.5451 - val_acc: 0.8499\n",
      "Epoch 51/85\n",
      " - 10s - loss: 0.6612 - acc: 0.7619 - val_loss: 0.6833 - val_acc: 0.7816\n",
      "Epoch 52/85\n",
      " - 10s - loss: 0.6617 - acc: 0.7620 - val_loss: 0.7402 - val_acc: 0.7503\n",
      "Epoch 53/85\n",
      " - 10s - loss: 0.6592 - acc: 0.7614 - val_loss: 0.6077 - val_acc: 0.8358\n",
      "Epoch 54/85\n",
      " - 10s - loss: 0.6551 - acc: 0.7619 - val_loss: 0.5615 - val_acc: 0.8417\n",
      "Epoch 55/85\n",
      " - 10s - loss: 0.6553 - acc: 0.7629 - val_loss: 0.5587 - val_acc: 0.8466\n",
      "Epoch 56/85\n",
      " - 10s - loss: 0.6547 - acc: 0.7618 - val_loss: 0.5486 - val_acc: 0.8494\n",
      "Epoch 57/85\n",
      " - 10s - loss: 0.6549 - acc: 0.7632 - val_loss: 0.5507 - val_acc: 0.8476\n",
      "Epoch 58/85\n",
      " - 10s - loss: 0.6577 - acc: 0.7632 - val_loss: 0.8155 - val_acc: 0.6813\n",
      "Epoch 59/85\n",
      " - 10s - loss: 0.6515 - acc: 0.7644 - val_loss: 0.5446 - val_acc: 0.8498\n",
      "Epoch 60/85\n",
      " - 10s - loss: 0.6505 - acc: 0.7644 - val_loss: 0.5498 - val_acc: 0.8464\n",
      "Epoch 61/85\n",
      " - 10s - loss: 0.6499 - acc: 0.7656 - val_loss: 0.9993 - val_acc: 0.6263\n",
      "Epoch 62/85\n",
      " - 10s - loss: 0.6517 - acc: 0.7617 - val_loss: 0.5428 - val_acc: 0.8491\n",
      "Epoch 63/85\n",
      " - 10s - loss: 0.6465 - acc: 0.7654 - val_loss: 0.5717 - val_acc: 0.8473\n",
      "Epoch 64/85\n",
      " - 10s - loss: 0.6532 - acc: 0.7634 - val_loss: 0.5509 - val_acc: 0.8459\n",
      "Epoch 65/85\n",
      " - 10s - loss: 0.6486 - acc: 0.7640 - val_loss: 0.6031 - val_acc: 0.8211\n",
      "Epoch 66/85\n",
      " - 10s - loss: 0.6497 - acc: 0.7625 - val_loss: 0.5572 - val_acc: 0.8486\n",
      "Epoch 67/85\n",
      " - 10s - loss: 0.6441 - acc: 0.7661 - val_loss: 0.5663 - val_acc: 0.8464\n",
      "Epoch 68/85\n",
      " - 10s - loss: 0.6460 - acc: 0.7655 - val_loss: 0.7894 - val_acc: 0.7096\n",
      "Epoch 69/85\n",
      " - 10s - loss: 0.6448 - acc: 0.7669 - val_loss: 0.5763 - val_acc: 0.8378\n",
      "Epoch 70/85\n",
      " - 10s - loss: 0.6477 - acc: 0.7629 - val_loss: 0.6156 - val_acc: 0.8108\n",
      "Epoch 71/85\n",
      " - 10s - loss: 0.6427 - acc: 0.7662 - val_loss: 0.5570 - val_acc: 0.8429\n",
      "Epoch 72/85\n",
      " - 10s - loss: 0.6443 - acc: 0.7663 - val_loss: 0.5603 - val_acc: 0.8427\n",
      "Epoch 73/85\n",
      " - 10s - loss: 0.6439 - acc: 0.7668 - val_loss: 0.5634 - val_acc: 0.8453\n",
      "Epoch 74/85\n",
      " - 10s - loss: 0.6420 - acc: 0.7673 - val_loss: 0.5625 - val_acc: 0.8425\n",
      "Epoch 75/85\n",
      " - 10s - loss: 0.6450 - acc: 0.7647 - val_loss: 0.5707 - val_acc: 0.8380\n",
      "Epoch 76/85\n",
      " - 10s - loss: 0.6497 - acc: 0.7643 - val_loss: 0.5426 - val_acc: 0.8494\n",
      "Epoch 77/85\n",
      " - 10s - loss: 0.6425 - acc: 0.7663 - val_loss: 0.5605 - val_acc: 0.8398\n",
      "Epoch 78/85\n",
      " - 10s - loss: 0.6424 - acc: 0.7668 - val_loss: 0.5461 - val_acc: 0.8469\n",
      "Epoch 79/85\n",
      " - 10s - loss: 0.6395 - acc: 0.7693 - val_loss: 0.5719 - val_acc: 0.8349\n",
      "Epoch 80/85\n",
      " - 10s - loss: 0.6413 - acc: 0.7663 - val_loss: 0.5476 - val_acc: 0.8496\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 81/85\n",
      " - 10s - loss: 0.6431 - acc: 0.7651 - val_loss: 0.5383 - val_acc: 0.8518\n",
      "Epoch 82/85\n",
      " - 10s - loss: 0.6424 - acc: 0.7650 - val_loss: 0.5443 - val_acc: 0.8494\n",
      "Epoch 83/85\n",
      " - 10s - loss: 0.6420 - acc: 0.7666 - val_loss: 0.5442 - val_acc: 0.8485\n",
      "Epoch 84/85\n",
      " - 10s - loss: 0.6427 - acc: 0.7641 - val_loss: 0.5479 - val_acc: 0.8425\n",
      "Epoch 85/85\n",
      " - 10s - loss: 0.6490 - acc: 0.7589 - val_loss: 0.5504 - val_acc: 0.8424\n",
      "49958/49958 [==============================] - 6s 112us/step\n",
      " Dropout : 0.3 accuracy is : 0.788442291525\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_25 (LSTM)               (None, 1, 600)            4996800   \n",
      "_________________________________________________________________\n",
      "dropout_25 (Dropout)         (None, 1, 600)            0         \n",
      "_________________________________________________________________\n",
      "lstm_26 (LSTM)               (None, 400)               1601600   \n",
      "_________________________________________________________________\n",
      "dropout_26 (Dropout)         (None, 400)               0         \n",
      "_________________________________________________________________\n",
      "dense_25 (Dense)             (None, 150)               60150     \n",
      "_________________________________________________________________\n",
      "dense_26 (Dense)             (None, 3)                 453       \n",
      "=================================================================\n",
      "Total params: 6,659,003\n",
      "Trainable params: 6,659,003\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      " Model build done!\n",
      "Train on 159866 samples, validate on 39967 samples\n",
      "Epoch 1/85\n",
      " - 12s - loss: 1.0454 - acc: 0.5380 - val_loss: 1.1088 - val_acc: 0.5333\n",
      "Epoch 2/85\n",
      " - 10s - loss: 1.0367 - acc: 0.5330 - val_loss: 1.1999 - val_acc: 0.5333\n",
      "Epoch 3/85\n",
      " - 10s - loss: 0.9916 - acc: 0.5585 - val_loss: 0.9051 - val_acc: 0.6456\n",
      "Epoch 4/85\n",
      " - 10s - loss: 0.9591 - acc: 0.5666 - val_loss: 0.8509 - val_acc: 0.6609\n",
      "Epoch 5/85\n",
      " - 10s - loss: 0.9284 - acc: 0.5866 - val_loss: 1.1064 - val_acc: 0.5642\n",
      "Epoch 6/85\n",
      " - 10s - loss: 0.9030 - acc: 0.6054 - val_loss: 0.8908 - val_acc: 0.6145\n",
      "Epoch 7/85\n",
      " - 10s - loss: 0.8768 - acc: 0.6228 - val_loss: 0.7693 - val_acc: 0.7699\n",
      "Epoch 8/85\n",
      " - 10s - loss: 0.8430 - acc: 0.6469 - val_loss: 0.6965 - val_acc: 0.8067\n",
      "Epoch 9/85\n",
      " - 10s - loss: 0.8323 - acc: 0.6597 - val_loss: 0.8638 - val_acc: 0.6304\n",
      "Epoch 10/85\n",
      " - 10s - loss: 0.8154 - acc: 0.6745 - val_loss: 0.6355 - val_acc: 0.8264\n",
      "Epoch 11/85\n",
      " - 10s - loss: 0.7922 - acc: 0.6901 - val_loss: 0.6587 - val_acc: 0.8072\n",
      "Epoch 12/85\n",
      " - 10s - loss: 0.7824 - acc: 0.6969 - val_loss: 0.5965 - val_acc: 0.8358\n",
      "Epoch 13/85\n",
      " - 10s - loss: 0.7707 - acc: 0.7051 - val_loss: 0.6201 - val_acc: 0.8357\n",
      "Epoch 14/85\n",
      " - 10s - loss: 0.7583 - acc: 0.7152 - val_loss: 0.5913 - val_acc: 0.8377\n",
      "Epoch 15/85\n",
      " - 10s - loss: 0.7522 - acc: 0.7177 - val_loss: 0.5914 - val_acc: 0.8363\n",
      "Epoch 16/85\n",
      " - 10s - loss: 0.7482 - acc: 0.7245 - val_loss: 0.7340 - val_acc: 0.7669\n",
      "Epoch 17/85\n",
      " - 10s - loss: 0.7481 - acc: 0.7199 - val_loss: 0.6022 - val_acc: 0.8346\n",
      "Epoch 18/85\n",
      " - 10s - loss: 0.7422 - acc: 0.7266 - val_loss: 0.5782 - val_acc: 0.8403\n",
      "Epoch 19/85\n",
      " - 10s - loss: 0.7318 - acc: 0.7334 - val_loss: 1.1200 - val_acc: 0.4539\n",
      "Epoch 20/85\n",
      " - 10s - loss: 0.7247 - acc: 0.7350 - val_loss: 0.5962 - val_acc: 0.8427\n",
      "Epoch 21/85\n",
      " - 10s - loss: 0.7215 - acc: 0.7379 - val_loss: 0.5747 - val_acc: 0.8384\n",
      "Epoch 22/85\n",
      " - 10s - loss: 0.7141 - acc: 0.7396 - val_loss: 0.6038 - val_acc: 0.8288\n",
      "Epoch 23/85\n",
      " - 10s - loss: 0.7175 - acc: 0.7400 - val_loss: 0.6198 - val_acc: 0.8182\n",
      "Epoch 24/85\n",
      " - 10s - loss: 0.7138 - acc: 0.7400 - val_loss: 0.6557 - val_acc: 0.7948\n",
      "Epoch 25/85\n",
      " - 10s - loss: 0.7088 - acc: 0.7412 - val_loss: 0.5985 - val_acc: 0.8368\n",
      "Epoch 26/85\n",
      " - 10s - loss: 0.7044 - acc: 0.7441 - val_loss: 0.6038 - val_acc: 0.8424\n",
      "Epoch 27/85\n",
      " - 10s - loss: 0.7014 - acc: 0.7471 - val_loss: 0.7139 - val_acc: 0.7686\n",
      "Epoch 28/85\n",
      " - 10s - loss: 0.6959 - acc: 0.7494 - val_loss: 0.6866 - val_acc: 0.7996\n",
      "Epoch 29/85\n",
      " - 10s - loss: 0.6970 - acc: 0.7482 - val_loss: 0.6483 - val_acc: 0.8281\n",
      "Epoch 30/85\n",
      " - 10s - loss: 0.6913 - acc: 0.7495 - val_loss: 0.7628 - val_acc: 0.7125\n",
      "Epoch 31/85\n",
      " - 10s - loss: 0.6903 - acc: 0.7494 - val_loss: 0.6065 - val_acc: 0.8347\n",
      "Epoch 32/85\n",
      " - 10s - loss: 0.6891 - acc: 0.7500 - val_loss: 0.6679 - val_acc: 0.7752\n",
      "Epoch 33/85\n",
      " - 10s - loss: 0.6797 - acc: 0.7532 - val_loss: 0.5741 - val_acc: 0.8434\n",
      "Epoch 34/85\n",
      " - 10s - loss: 0.6808 - acc: 0.7521 - val_loss: 0.6075 - val_acc: 0.8494\n",
      "Epoch 35/85\n",
      " - 10s - loss: 0.6794 - acc: 0.7522 - val_loss: 0.5890 - val_acc: 0.8431\n",
      "Epoch 36/85\n",
      " - 10s - loss: 0.6790 - acc: 0.7533 - val_loss: 0.7782 - val_acc: 0.6915\n",
      "Epoch 37/85\n",
      " - 10s - loss: 0.6762 - acc: 0.7531 - val_loss: 0.6453 - val_acc: 0.8421\n",
      "Epoch 38/85\n",
      " - 10s - loss: 0.6741 - acc: 0.7550 - val_loss: 1.1177 - val_acc: 0.4479\n",
      "Epoch 39/85\n",
      " - 10s - loss: 0.6741 - acc: 0.7558 - val_loss: 0.8089 - val_acc: 0.6910\n",
      "Epoch 40/85\n",
      " - 10s - loss: 0.6673 - acc: 0.7560 - val_loss: 0.6116 - val_acc: 0.8446\n",
      "Epoch 41/85\n",
      " - 10s - loss: 0.6646 - acc: 0.7586 - val_loss: 0.6073 - val_acc: 0.8378\n",
      "Epoch 42/85\n",
      " - 10s - loss: 0.6677 - acc: 0.7575 - val_loss: 0.5872 - val_acc: 0.8478\n",
      "Epoch 43/85\n",
      " - 10s - loss: 0.6612 - acc: 0.7589 - val_loss: 0.5547 - val_acc: 0.8488\n",
      "Epoch 44/85\n",
      " - 10s - loss: 0.6591 - acc: 0.7618 - val_loss: 0.6120 - val_acc: 0.8416\n",
      "Epoch 45/85\n",
      " - 10s - loss: 0.6611 - acc: 0.7595 - val_loss: 0.6507 - val_acc: 0.8154\n",
      "Epoch 46/85\n",
      " - 10s - loss: 0.6576 - acc: 0.7607 - val_loss: 0.5571 - val_acc: 0.8481\n",
      "Epoch 47/85\n",
      " - 10s - loss: 0.6588 - acc: 0.7576 - val_loss: 0.5607 - val_acc: 0.8504\n",
      "Epoch 48/85\n",
      " - 10s - loss: 0.6511 - acc: 0.7627 - val_loss: 0.5496 - val_acc: 0.8503\n",
      "Epoch 49/85\n",
      " - 10s - loss: 0.6541 - acc: 0.7618 - val_loss: 0.5606 - val_acc: 0.8476\n",
      "Epoch 50/85\n",
      " - 10s - loss: 0.6520 - acc: 0.7621 - val_loss: 0.6552 - val_acc: 0.8009\n",
      "Epoch 51/85\n",
      " - 10s - loss: 0.6530 - acc: 0.7616 - val_loss: 0.5514 - val_acc: 0.8508\n",
      "Epoch 52/85\n",
      " - 10s - loss: 0.6489 - acc: 0.7635 - val_loss: 0.5566 - val_acc: 0.8472\n",
      "Epoch 53/85\n",
      " - 10s - loss: 0.6466 - acc: 0.7639 - val_loss: 0.5499 - val_acc: 0.8500\n",
      "Epoch 54/85\n",
      " - 10s - loss: 0.6457 - acc: 0.7652 - val_loss: 0.5916 - val_acc: 0.8185\n",
      "Epoch 55/85\n",
      " - 10s - loss: 0.6460 - acc: 0.7631 - val_loss: 0.6155 - val_acc: 0.8189\n",
      "Epoch 56/85\n",
      " - 10s - loss: 0.6437 - acc: 0.7648 - val_loss: 0.5533 - val_acc: 0.8525\n",
      "Epoch 57/85\n",
      " - 10s - loss: 0.6419 - acc: 0.7657 - val_loss: 0.5609 - val_acc: 0.8507\n",
      "Epoch 58/85\n",
      " - 10s - loss: 0.6433 - acc: 0.7653 - val_loss: 0.5780 - val_acc: 0.8527\n",
      "Epoch 59/85\n",
      " - 10s - loss: 0.6421 - acc: 0.7655 - val_loss: 0.5572 - val_acc: 0.8487\n",
      "Epoch 60/85\n",
      " - 10s - loss: 0.6418 - acc: 0.7663 - val_loss: 0.5426 - val_acc: 0.8532\n",
      "Epoch 61/85\n",
      " - 10s - loss: 0.6405 - acc: 0.7653 - val_loss: 0.5625 - val_acc: 0.8458\n",
      "Epoch 62/85\n",
      " - 10s - loss: 0.6405 - acc: 0.7657 - val_loss: 0.6233 - val_acc: 0.8060\n",
      "Epoch 63/85\n",
      " - 10s - loss: 0.6375 - acc: 0.7659 - val_loss: 0.5543 - val_acc: 0.8475\n",
      "Epoch 64/85\n",
      " - 10s - loss: 0.6387 - acc: 0.7670 - val_loss: 0.5748 - val_acc: 0.8468\n",
      "Epoch 65/85\n",
      " - 10s - loss: 0.6370 - acc: 0.7680 - val_loss: 0.5596 - val_acc: 0.8483\n",
      "Epoch 66/85\n",
      " - 10s - loss: 0.6413 - acc: 0.7659 - val_loss: 0.5628 - val_acc: 0.8445\n",
      "Epoch 67/85\n",
      " - 10s - loss: 0.6384 - acc: 0.7665 - val_loss: 0.5563 - val_acc: 0.8468\n",
      "Epoch 68/85\n",
      " - 10s - loss: 0.6360 - acc: 0.7682 - val_loss: 0.5564 - val_acc: 0.8505\n",
      "Epoch 69/85\n",
      " - 10s - loss: 0.6346 - acc: 0.7688 - val_loss: 0.5871 - val_acc: 0.8448\n",
      "Epoch 70/85\n",
      " - 10s - loss: 0.6358 - acc: 0.7654 - val_loss: 0.5439 - val_acc: 0.8495\n",
      "Epoch 71/85\n",
      " - 10s - loss: 0.6359 - acc: 0.7674 - val_loss: 0.5629 - val_acc: 0.8484\n",
      "Epoch 72/85\n",
      " - 10s - loss: 0.6340 - acc: 0.7672 - val_loss: 0.5452 - val_acc: 0.8492\n",
      "Epoch 73/85\n",
      " - 10s - loss: 0.6324 - acc: 0.7686 - val_loss: 0.5986 - val_acc: 0.8206\n",
      "Epoch 74/85\n",
      " - 10s - loss: 0.6301 - acc: 0.7679 - val_loss: 0.7652 - val_acc: 0.7186\n",
      "Epoch 75/85\n",
      " - 10s - loss: 0.6378 - acc: 0.7636 - val_loss: 0.5500 - val_acc: 0.8511\n",
      "Epoch 76/85\n",
      " - 10s - loss: 0.6322 - acc: 0.7688 - val_loss: 0.5484 - val_acc: 0.8523\n",
      "Epoch 77/85\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 10s - loss: 0.6337 - acc: 0.7680 - val_loss: 0.5600 - val_acc: 0.8498\n",
      "Epoch 78/85\n",
      " - 10s - loss: 0.6317 - acc: 0.7690 - val_loss: 0.5573 - val_acc: 0.8507\n",
      "Epoch 79/85\n",
      " - 10s - loss: 0.6295 - acc: 0.7695 - val_loss: 0.5523 - val_acc: 0.8514\n",
      "Epoch 80/85\n",
      " - 10s - loss: 0.6297 - acc: 0.7698 - val_loss: 0.5604 - val_acc: 0.8488\n",
      "Epoch 81/85\n",
      " - 10s - loss: 0.6278 - acc: 0.7701 - val_loss: 0.5610 - val_acc: 0.8479\n",
      "Epoch 82/85\n",
      " - 10s - loss: 0.6269 - acc: 0.7694 - val_loss: 0.5551 - val_acc: 0.8485\n",
      "Epoch 83/85\n",
      " - 10s - loss: 0.6269 - acc: 0.7705 - val_loss: 0.5684 - val_acc: 0.8493\n",
      "Epoch 84/85\n",
      " - 10s - loss: 0.6280 - acc: 0.7696 - val_loss: 0.5578 - val_acc: 0.8505\n",
      "Epoch 85/85\n",
      " - 10s - loss: 0.6241 - acc: 0.7716 - val_loss: 0.5741 - val_acc: 0.8471\n",
      "49958/49958 [==============================] - 6s 112us/step\n",
      " Dropout : 0.35 accuracy is : 0.804896112735\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_27 (LSTM)               (None, 1, 600)            4996800   \n",
      "_________________________________________________________________\n",
      "dropout_27 (Dropout)         (None, 1, 600)            0         \n",
      "_________________________________________________________________\n",
      "lstm_28 (LSTM)               (None, 400)               1601600   \n",
      "_________________________________________________________________\n",
      "dropout_28 (Dropout)         (None, 400)               0         \n",
      "_________________________________________________________________\n",
      "dense_27 (Dense)             (None, 150)               60150     \n",
      "_________________________________________________________________\n",
      "dense_28 (Dense)             (None, 3)                 453       \n",
      "=================================================================\n",
      "Total params: 6,659,003\n",
      "Trainable params: 6,659,003\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      " Model build done!\n",
      "Train on 159866 samples, validate on 39967 samples\n",
      "Epoch 1/85\n",
      " - 13s - loss: 1.0460 - acc: 0.5370 - val_loss: 1.1094 - val_acc: 0.5333\n",
      "Epoch 2/85\n",
      " - 10s - loss: 1.0446 - acc: 0.5321 - val_loss: 1.0766 - val_acc: 0.5333\n",
      "Epoch 3/85\n",
      " - 10s - loss: 1.0087 - acc: 0.5501 - val_loss: 0.8922 - val_acc: 0.6493\n",
      "Epoch 4/85\n",
      " - 10s - loss: 0.9818 - acc: 0.5616 - val_loss: 1.0185 - val_acc: 0.5817\n",
      "Epoch 5/85\n",
      " - 10s - loss: 0.9493 - acc: 0.5743 - val_loss: 0.9812 - val_acc: 0.6019\n",
      "Epoch 6/85\n",
      " - 10s - loss: 0.9241 - acc: 0.5892 - val_loss: 0.9695 - val_acc: 0.6614\n",
      "Epoch 7/85\n",
      " - 10s - loss: 0.9004 - acc: 0.6063 - val_loss: 0.9899 - val_acc: 0.5722\n",
      "Epoch 8/85\n",
      " - 10s - loss: 0.8772 - acc: 0.6268 - val_loss: 0.8461 - val_acc: 0.7390\n",
      "Epoch 9/85\n",
      " - 10s - loss: 0.8581 - acc: 0.6418 - val_loss: 0.9455 - val_acc: 0.6920\n",
      "Epoch 10/85\n",
      " - 10s - loss: 0.8309 - acc: 0.6609 - val_loss: 0.6442 - val_acc: 0.8280\n",
      "Epoch 11/85\n",
      " - 10s - loss: 0.8227 - acc: 0.6687 - val_loss: 0.6116 - val_acc: 0.8356\n",
      "Epoch 12/85\n",
      " - 10s - loss: 0.8137 - acc: 0.6778 - val_loss: 0.6730 - val_acc: 0.8068\n",
      "Epoch 13/85\n",
      " - 10s - loss: 0.7897 - acc: 0.6929 - val_loss: 0.5971 - val_acc: 0.8361\n",
      "Epoch 14/85\n",
      " - 10s - loss: 0.7859 - acc: 0.6997 - val_loss: 0.6044 - val_acc: 0.8339\n",
      "Epoch 15/85\n",
      " - 10s - loss: 0.7803 - acc: 0.7005 - val_loss: 0.5993 - val_acc: 0.8379\n",
      "Epoch 16/85\n",
      " - 10s - loss: 0.7692 - acc: 0.7115 - val_loss: 0.6849 - val_acc: 0.8007\n",
      "Epoch 17/85\n",
      " - 10s - loss: 0.7645 - acc: 0.7147 - val_loss: 0.6412 - val_acc: 0.8206\n",
      "Epoch 18/85\n",
      " - 10s - loss: 0.7576 - acc: 0.7174 - val_loss: 0.5929 - val_acc: 0.8371\n",
      "Epoch 19/85\n",
      " - 10s - loss: 0.7476 - acc: 0.7259 - val_loss: 0.6757 - val_acc: 0.8129\n",
      "Epoch 20/85\n",
      " - 10s - loss: 0.7473 - acc: 0.7263 - val_loss: 0.5855 - val_acc: 0.8382\n",
      "Epoch 21/85\n",
      " - 10s - loss: 0.7447 - acc: 0.7265 - val_loss: 1.0683 - val_acc: 0.5315\n",
      "Epoch 22/85\n",
      " - 10s - loss: 0.7429 - acc: 0.7267 - val_loss: 0.5873 - val_acc: 0.8389\n",
      "Epoch 23/85\n",
      " - 10s - loss: 0.7390 - acc: 0.7305 - val_loss: 0.5840 - val_acc: 0.8387\n",
      "Epoch 24/85\n",
      " - 10s - loss: 0.7280 - acc: 0.7378 - val_loss: 0.5928 - val_acc: 0.8399\n",
      "Epoch 25/85\n",
      " - 10s - loss: 0.7273 - acc: 0.7374 - val_loss: 0.5912 - val_acc: 0.8376\n",
      "Epoch 26/85\n",
      " - 10s - loss: 0.7258 - acc: 0.7383 - val_loss: 0.6884 - val_acc: 0.7878\n",
      "Epoch 27/85\n",
      " - 10s - loss: 0.7189 - acc: 0.7408 - val_loss: 0.5928 - val_acc: 0.8392\n",
      "Epoch 28/85\n",
      " - 10s - loss: 0.7181 - acc: 0.7432 - val_loss: 0.5907 - val_acc: 0.8396\n",
      "Epoch 29/85\n",
      " - 10s - loss: 0.7127 - acc: 0.7450 - val_loss: 0.5906 - val_acc: 0.8376\n",
      "Epoch 30/85\n",
      " - 10s - loss: 0.7151 - acc: 0.7425 - val_loss: 0.5843 - val_acc: 0.8401\n",
      "Epoch 31/85\n",
      " - 10s - loss: 0.7055 - acc: 0.7476 - val_loss: 0.6114 - val_acc: 0.8334\n",
      "Epoch 32/85\n",
      " - 10s - loss: 0.7082 - acc: 0.7457 - val_loss: 0.5805 - val_acc: 0.8390\n",
      "Epoch 33/85\n",
      " - 10s - loss: 0.7069 - acc: 0.7437 - val_loss: 0.5902 - val_acc: 0.8383\n",
      "Epoch 34/85\n",
      " - 10s - loss: 0.7026 - acc: 0.7480 - val_loss: 0.5711 - val_acc: 0.8385\n",
      "Epoch 35/85\n",
      " - 10s - loss: 0.7022 - acc: 0.7480 - val_loss: 0.5928 - val_acc: 0.8428\n",
      "Epoch 36/85\n",
      " - 10s - loss: 0.7016 - acc: 0.7473 - val_loss: 0.5766 - val_acc: 0.8414\n",
      "Epoch 37/85\n",
      " - 10s - loss: 0.6940 - acc: 0.7500 - val_loss: 0.5898 - val_acc: 0.8371\n",
      "Epoch 38/85\n",
      " - 10s - loss: 0.6908 - acc: 0.7519 - val_loss: 0.6135 - val_acc: 0.8454\n",
      "Epoch 39/85\n",
      " - 10s - loss: 0.6916 - acc: 0.7526 - val_loss: 0.5952 - val_acc: 0.8426\n",
      "Epoch 40/85\n",
      " - 10s - loss: 0.6894 - acc: 0.7541 - val_loss: 0.5784 - val_acc: 0.8409\n",
      "Epoch 41/85\n",
      " - 10s - loss: 0.6846 - acc: 0.7544 - val_loss: 0.5791 - val_acc: 0.8426\n",
      "Epoch 42/85\n",
      " - 10s - loss: 0.6877 - acc: 0.7524 - val_loss: 0.6230 - val_acc: 0.8446\n",
      "Epoch 43/85\n",
      " - 10s - loss: 0.6801 - acc: 0.7570 - val_loss: 0.5977 - val_acc: 0.8443\n",
      "Epoch 44/85\n",
      " - 10s - loss: 0.6788 - acc: 0.7570 - val_loss: 0.6321 - val_acc: 0.8298\n",
      "Epoch 45/85\n",
      " - 10s - loss: 0.6825 - acc: 0.7539 - val_loss: 0.5649 - val_acc: 0.8412\n",
      "Epoch 46/85\n",
      " - 10s - loss: 0.6776 - acc: 0.7546 - val_loss: 0.5705 - val_acc: 0.8400\n",
      "Epoch 47/85\n",
      " - 10s - loss: 0.6809 - acc: 0.7531 - val_loss: 0.5953 - val_acc: 0.8465\n",
      "Epoch 48/85\n",
      " - 10s - loss: 0.6757 - acc: 0.7558 - val_loss: 0.5516 - val_acc: 0.8435\n",
      "Epoch 49/85\n",
      " - 10s - loss: 0.6735 - acc: 0.7567 - val_loss: 0.6015 - val_acc: 0.8419\n",
      "Epoch 50/85\n",
      " - 10s - loss: 0.6708 - acc: 0.7594 - val_loss: 0.5797 - val_acc: 0.8456\n",
      "Epoch 51/85\n",
      " - 10s - loss: 0.6720 - acc: 0.7560 - val_loss: 0.6397 - val_acc: 0.8167\n",
      "Epoch 52/85\n",
      " - 10s - loss: 0.6730 - acc: 0.7564 - val_loss: 0.6056 - val_acc: 0.8335\n",
      "Epoch 53/85\n",
      " - 10s - loss: 0.6682 - acc: 0.7590 - val_loss: 0.5643 - val_acc: 0.8442\n",
      "Epoch 54/85\n",
      " - 10s - loss: 0.6685 - acc: 0.7577 - val_loss: 0.6022 - val_acc: 0.8329\n",
      "Epoch 55/85\n",
      " - 10s - loss: 0.6685 - acc: 0.7582 - val_loss: 0.5667 - val_acc: 0.8481\n",
      "Epoch 56/85\n",
      " - 10s - loss: 0.6676 - acc: 0.7583 - val_loss: 0.5524 - val_acc: 0.8497\n",
      "Epoch 57/85\n",
      " - 10s - loss: 0.6679 - acc: 0.7586 - val_loss: 0.5497 - val_acc: 0.8428\n",
      "Epoch 58/85\n",
      " - 10s - loss: 0.6639 - acc: 0.7600 - val_loss: 0.5477 - val_acc: 0.8454\n",
      "Epoch 59/85\n",
      " - 10s - loss: 0.6666 - acc: 0.7572 - val_loss: 0.5676 - val_acc: 0.8468\n",
      "Epoch 60/85\n",
      " - 10s - loss: 0.6616 - acc: 0.7606 - val_loss: 0.5516 - val_acc: 0.8465\n",
      "Epoch 61/85\n",
      " - 10s - loss: 0.6648 - acc: 0.7591 - val_loss: 0.5564 - val_acc: 0.8451\n",
      "Epoch 62/85\n",
      " - 10s - loss: 0.6632 - acc: 0.7588 - val_loss: 0.5669 - val_acc: 0.8494\n",
      "Epoch 63/85\n",
      " - 10s - loss: 0.6652 - acc: 0.7545 - val_loss: 0.5459 - val_acc: 0.8478\n",
      "Epoch 64/85\n",
      " - 10s - loss: 0.6595 - acc: 0.7604 - val_loss: 0.5489 - val_acc: 0.8461\n",
      "Epoch 65/85\n",
      " - 10s - loss: 0.6603 - acc: 0.7611 - val_loss: 0.5713 - val_acc: 0.8478\n",
      "Epoch 66/85\n",
      " - 10s - loss: 0.6565 - acc: 0.7624 - val_loss: 0.5917 - val_acc: 0.8303\n",
      "Epoch 67/85\n",
      " - 10s - loss: 0.6571 - acc: 0.7594 - val_loss: 0.5486 - val_acc: 0.8452\n",
      "Epoch 68/85\n",
      " - 10s - loss: 0.6598 - acc: 0.7585 - val_loss: 0.5449 - val_acc: 0.8482\n",
      "Epoch 69/85\n",
      " - 10s - loss: 0.6549 - acc: 0.7620 - val_loss: 0.5653 - val_acc: 0.8431\n",
      "Epoch 70/85\n",
      " - 10s - loss: 0.6578 - acc: 0.7615 - val_loss: 0.5482 - val_acc: 0.8473\n",
      "Epoch 71/85\n",
      " - 10s - loss: 0.6551 - acc: 0.7628 - val_loss: 0.5494 - val_acc: 0.8458\n",
      "Epoch 72/85\n",
      " - 10s - loss: 0.6557 - acc: 0.7617 - val_loss: 0.5475 - val_acc: 0.8477\n",
      "Epoch 73/85\n",
      " - 10s - loss: 0.6530 - acc: 0.7623 - val_loss: 0.5439 - val_acc: 0.8470\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 74/85\n",
      " - 10s - loss: 0.6542 - acc: 0.7598 - val_loss: 0.5539 - val_acc: 0.8469\n",
      "Epoch 75/85\n",
      " - 10s - loss: 0.6528 - acc: 0.7614 - val_loss: 0.6434 - val_acc: 0.7913\n",
      "Epoch 76/85\n",
      " - 10s - loss: 0.6548 - acc: 0.7621 - val_loss: 0.5564 - val_acc: 0.8430\n",
      "Epoch 77/85\n",
      " - 10s - loss: 0.6511 - acc: 0.7630 - val_loss: 0.5499 - val_acc: 0.8478\n",
      "Epoch 78/85\n",
      " - 10s - loss: 0.6515 - acc: 0.7631 - val_loss: 0.5617 - val_acc: 0.8419\n",
      "Epoch 79/85\n",
      " - 10s - loss: 0.6494 - acc: 0.7642 - val_loss: 0.5513 - val_acc: 0.8452\n",
      "Epoch 80/85\n",
      " - 10s - loss: 0.6486 - acc: 0.7639 - val_loss: 0.5518 - val_acc: 0.8472\n",
      "Epoch 81/85\n",
      " - 10s - loss: 0.6532 - acc: 0.7623 - val_loss: 0.5545 - val_acc: 0.8472\n",
      "Epoch 82/85\n",
      " - 10s - loss: 0.6508 - acc: 0.7633 - val_loss: 0.5500 - val_acc: 0.8476\n",
      "Epoch 83/85\n",
      " - 10s - loss: 0.6503 - acc: 0.7626 - val_loss: 0.5487 - val_acc: 0.8423\n",
      "Epoch 84/85\n",
      " - 10s - loss: 0.6494 - acc: 0.7612 - val_loss: 0.5477 - val_acc: 0.8486\n",
      "Epoch 85/85\n",
      " - 10s - loss: 0.6505 - acc: 0.7612 - val_loss: 0.5561 - val_acc: 0.8454\n",
      "49958/49958 [==============================] - 6s 113us/step\n",
      " Dropout : 0.4 accuracy is : 0.794947756115\n"
     ]
    }
   ],
   "source": [
    "ml = run_architecture_2_with_different_dropouts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i,model in enumerate(ml):\n",
    "    dropout_list = [0.1, 0.15, 0.2, 0.25, 0.3, 0.35, 0.4]\n",
    "    name = 'model_arch_2_dp_'+str(dropout_list[i])+'.h5'\n",
    "    model.save(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "p = load_model('model_arch_2_dp_4.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49958/49958 [==============================] - 7s 140us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.70416599588219686, 0.79494775611513668]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_architecture_3_with_different_dropouts():\n",
    "    seq_len = 1\n",
    "    shape = [feature_df.shape[1], seq_len] # feature, window\n",
    "    neurons = [1000, 800, 150, 3]\n",
    "    dropout_list = [0.1, 0.15, 0.2, 0.25, 0.3, 0.35, 0.4]\n",
    "    model_list = []\n",
    "    for dropout in dropout_list:\n",
    "        model2 = build_model2(shape, neurons, dropout)\n",
    "        model2.fit(\n",
    "        X_train,\n",
    "        y_train,\n",
    "        batch_size=512,\n",
    "        epochs=85,\n",
    "        validation_split=0.2,\n",
    "        shuffle = False,\n",
    "        class_weight = {0 : 1.2, 1 : 1, 2 : 1.1},\n",
    "        verbose=2)\n",
    "        curr_eval = model2.evaluate(X_test,y_test)\n",
    "        print(\" Dropout : \" + str(dropout) + \" accuracy is : \" + str(curr_eval[1]))\n",
    "        model_list.append(model2)\n",
    "    return model_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_29 (LSTM)               (None, 1, 1000)           9928000   \n",
      "_________________________________________________________________\n",
      "dropout_29 (Dropout)         (None, 1, 1000)           0         \n",
      "_________________________________________________________________\n",
      "lstm_30 (LSTM)               (None, 800)               5763200   \n",
      "_________________________________________________________________\n",
      "dropout_30 (Dropout)         (None, 800)               0         \n",
      "_________________________________________________________________\n",
      "dense_29 (Dense)             (None, 150)               120150    \n",
      "_________________________________________________________________\n",
      "dense_30 (Dense)             (None, 3)                 453       \n",
      "=================================================================\n",
      "Total params: 15,811,803\n",
      "Trainable params: 15,811,803\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      " Model build done!\n",
      "Train on 159866 samples, validate on 39967 samples\n",
      "Epoch 1/85\n",
      " - 21s - loss: 1.0457 - acc: 0.5360 - val_loss: 1.1081 - val_acc: 0.5333\n",
      "Epoch 2/85\n",
      " - 18s - loss: 1.0397 - acc: 0.5329 - val_loss: 1.0007 - val_acc: 0.6039\n",
      "Epoch 3/85\n",
      " - 18s - loss: 1.0000 - acc: 0.5518 - val_loss: 0.9129 - val_acc: 0.6307\n",
      "Epoch 4/85\n",
      " - 18s - loss: 0.9574 - acc: 0.5712 - val_loss: 1.0261 - val_acc: 0.5433\n",
      "Epoch 5/85\n",
      " - 18s - loss: 0.9224 - acc: 0.5903 - val_loss: 0.9258 - val_acc: 0.5985\n",
      "Epoch 6/85\n",
      " - 18s - loss: 0.8917 - acc: 0.6132 - val_loss: 0.9966 - val_acc: 0.5822\n",
      "Epoch 7/85\n",
      " - 18s - loss: 0.8643 - acc: 0.6356 - val_loss: 0.9207 - val_acc: 0.6170\n",
      "Epoch 8/85\n",
      " - 18s - loss: 0.8432 - acc: 0.6529 - val_loss: 0.7724 - val_acc: 0.7758\n",
      "Epoch 9/85\n",
      " - 18s - loss: 0.8108 - acc: 0.6760 - val_loss: 0.9646 - val_acc: 0.6283\n",
      "Epoch 10/85\n",
      " - 18s - loss: 0.8092 - acc: 0.6807 - val_loss: 0.6421 - val_acc: 0.8255\n",
      "Epoch 11/85\n",
      " - 18s - loss: 0.7810 - acc: 0.6961 - val_loss: 0.7155 - val_acc: 0.7948\n",
      "Epoch 12/85\n",
      " - 18s - loss: 0.7673 - acc: 0.7068 - val_loss: 0.7050 - val_acc: 0.7888\n",
      "Epoch 13/85\n",
      " - 18s - loss: 0.7694 - acc: 0.7096 - val_loss: 0.5881 - val_acc: 0.8386\n",
      "Epoch 14/85\n",
      " - 18s - loss: 0.7721 - acc: 0.7035 - val_loss: 0.5895 - val_acc: 0.8406\n",
      "Epoch 15/85\n",
      " - 18s - loss: 0.7484 - acc: 0.7215 - val_loss: 0.6425 - val_acc: 0.8209\n",
      "Epoch 16/85\n",
      " - 18s - loss: 0.7349 - acc: 0.7301 - val_loss: 0.6504 - val_acc: 0.8236\n",
      "Epoch 17/85\n",
      " - 18s - loss: 0.7355 - acc: 0.7320 - val_loss: 0.5830 - val_acc: 0.8376\n",
      "Epoch 18/85\n",
      " - 18s - loss: 0.7253 - acc: 0.7362 - val_loss: 1.0640 - val_acc: 0.4793\n",
      "Epoch 19/85\n",
      " - 18s - loss: 0.7169 - acc: 0.7373 - val_loss: 0.5903 - val_acc: 0.8454\n",
      "Epoch 20/85\n",
      " - 18s - loss: 0.7135 - acc: 0.7410 - val_loss: 0.6657 - val_acc: 0.8064\n",
      "Epoch 21/85\n",
      " - 18s - loss: 0.7114 - acc: 0.7399 - val_loss: 0.5806 - val_acc: 0.8395\n",
      "Epoch 22/85\n",
      " - 18s - loss: 0.7147 - acc: 0.7407 - val_loss: 0.5836 - val_acc: 0.8462\n",
      "Epoch 23/85\n",
      " - 18s - loss: 0.7121 - acc: 0.7392 - val_loss: 0.6434 - val_acc: 0.8201\n",
      "Epoch 24/85\n",
      " - 18s - loss: 0.6961 - acc: 0.7516 - val_loss: 0.6001 - val_acc: 0.8364\n",
      "Epoch 25/85\n",
      " - 18s - loss: 0.7025 - acc: 0.7466 - val_loss: 0.5912 - val_acc: 0.8394\n",
      "Epoch 26/85\n",
      " - 18s - loss: 0.6901 - acc: 0.7484 - val_loss: 0.6539 - val_acc: 0.8194\n",
      "Epoch 27/85\n",
      " - 18s - loss: 0.6984 - acc: 0.7450 - val_loss: 0.6908 - val_acc: 0.7522\n",
      "Epoch 28/85\n",
      " - 18s - loss: 0.6865 - acc: 0.7542 - val_loss: 0.5950 - val_acc: 0.8368\n",
      "Epoch 29/85\n",
      " - 18s - loss: 0.6847 - acc: 0.7522 - val_loss: 0.5572 - val_acc: 0.8436\n",
      "Epoch 30/85\n",
      " - 18s - loss: 0.6795 - acc: 0.7522 - val_loss: 0.5661 - val_acc: 0.8430\n",
      "Epoch 31/85\n",
      " - 18s - loss: 0.6764 - acc: 0.7538 - val_loss: 0.6304 - val_acc: 0.8367\n",
      "Epoch 32/85\n",
      " - 18s - loss: 0.6718 - acc: 0.7547 - val_loss: 0.5630 - val_acc: 0.8432\n",
      "Epoch 33/85\n",
      " - 18s - loss: 0.6709 - acc: 0.7552 - val_loss: 0.5681 - val_acc: 0.8432\n",
      "Epoch 34/85\n",
      " - 18s - loss: 0.6700 - acc: 0.7565 - val_loss: 0.5582 - val_acc: 0.8460\n",
      "Epoch 35/85\n",
      " - 18s - loss: 0.6643 - acc: 0.7585 - val_loss: 0.6252 - val_acc: 0.8054\n",
      "Epoch 36/85\n",
      " - 18s - loss: 0.6617 - acc: 0.7594 - val_loss: 0.5832 - val_acc: 0.8401\n",
      "Epoch 37/85\n",
      " - 18s - loss: 0.6598 - acc: 0.7573 - val_loss: 0.5593 - val_acc: 0.8452\n",
      "Epoch 38/85\n",
      " - 18s - loss: 0.6638 - acc: 0.7549 - val_loss: 0.9260 - val_acc: 0.5469\n",
      "Epoch 39/85\n",
      " - 18s - loss: 0.6625 - acc: 0.7576 - val_loss: 0.6413 - val_acc: 0.8020\n",
      "Epoch 40/85\n",
      " - 18s - loss: 0.6525 - acc: 0.7607 - val_loss: 0.5678 - val_acc: 0.8420\n",
      "Epoch 41/85\n",
      " - 18s - loss: 0.6527 - acc: 0.7642 - val_loss: 0.6889 - val_acc: 0.7771\n",
      "Epoch 42/85\n",
      " - 18s - loss: 0.6500 - acc: 0.7645 - val_loss: 0.5719 - val_acc: 0.8436\n",
      "Epoch 43/85\n",
      " - 18s - loss: 0.6510 - acc: 0.7612 - val_loss: 0.5615 - val_acc: 0.8449\n",
      "Epoch 44/85\n",
      " - 18s - loss: 0.6469 - acc: 0.7649 - val_loss: 0.5521 - val_acc: 0.8452\n",
      "Epoch 45/85\n",
      " - 18s - loss: 0.6442 - acc: 0.7669 - val_loss: 0.5635 - val_acc: 0.8458\n",
      "Epoch 46/85\n",
      " - 18s - loss: 0.6464 - acc: 0.7654 - val_loss: 0.5606 - val_acc: 0.8447\n",
      "Epoch 47/85\n",
      " - 18s - loss: 0.6507 - acc: 0.7636 - val_loss: 0.5497 - val_acc: 0.8469\n",
      "Epoch 48/85\n",
      " - 18s - loss: 0.6419 - acc: 0.7657 - val_loss: 0.5498 - val_acc: 0.8473\n",
      "Epoch 49/85\n",
      " - 18s - loss: 0.6390 - acc: 0.7676 - val_loss: 0.5433 - val_acc: 0.8491\n",
      "Epoch 50/85\n",
      " - 18s - loss: 0.6390 - acc: 0.7682 - val_loss: 0.5569 - val_acc: 0.8450\n",
      "Epoch 51/85\n",
      " - 18s - loss: 0.6381 - acc: 0.7664 - val_loss: 0.5831 - val_acc: 0.8397\n",
      "Epoch 52/85\n",
      " - 18s - loss: 0.6375 - acc: 0.7658 - val_loss: 0.6045 - val_acc: 0.8353\n",
      "Epoch 53/85\n",
      " - 18s - loss: 0.6322 - acc: 0.7677 - val_loss: 0.5851 - val_acc: 0.8491\n",
      "Epoch 54/85\n",
      " - 18s - loss: 0.6343 - acc: 0.7692 - val_loss: 0.5580 - val_acc: 0.8494\n",
      "Epoch 55/85\n",
      " - 18s - loss: 0.6329 - acc: 0.7702 - val_loss: 0.5507 - val_acc: 0.8493\n",
      "Epoch 56/85\n",
      " - 18s - loss: 0.6305 - acc: 0.7694 - val_loss: 0.5729 - val_acc: 0.8459\n",
      "Epoch 57/85\n",
      " - 18s - loss: 0.6303 - acc: 0.7687 - val_loss: 0.5977 - val_acc: 0.8392\n",
      "Epoch 58/85\n",
      " - 18s - loss: 0.6280 - acc: 0.7693 - val_loss: 0.5697 - val_acc: 0.8469\n",
      "Epoch 59/85\n",
      " - 18s - loss: 0.6319 - acc: 0.7697 - val_loss: 0.5660 - val_acc: 0.8446\n",
      "Epoch 60/85\n",
      " - 18s - loss: 0.6286 - acc: 0.7695 - val_loss: 0.5730 - val_acc: 0.8411\n",
      "Epoch 61/85\n",
      " - 18s - loss: 0.6244 - acc: 0.7718 - val_loss: 0.5531 - val_acc: 0.8501\n",
      "Epoch 62/85\n",
      " - 18s - loss: 0.6263 - acc: 0.7708 - val_loss: 0.5704 - val_acc: 0.8463\n",
      "Epoch 63/85\n",
      " - 18s - loss: 0.6259 - acc: 0.7715 - val_loss: 0.5707 - val_acc: 0.8461\n",
      "Epoch 64/85\n",
      " - 18s - loss: 0.6287 - acc: 0.7696 - val_loss: 0.5489 - val_acc: 0.8489\n",
      "Epoch 65/85\n",
      " - 18s - loss: 0.6230 - acc: 0.7715 - val_loss: 0.5535 - val_acc: 0.8477\n",
      "Epoch 66/85\n",
      " - 18s - loss: 0.6227 - acc: 0.7728 - val_loss: 0.5714 - val_acc: 0.8478\n",
      "Epoch 67/85\n",
      " - 18s - loss: 0.6230 - acc: 0.7721 - val_loss: 0.5755 - val_acc: 0.8458\n",
      "Epoch 68/85\n",
      " - 18s - loss: 0.6208 - acc: 0.7728 - val_loss: 0.5553 - val_acc: 0.8504\n",
      "Epoch 69/85\n",
      " - 18s - loss: 0.6190 - acc: 0.7738 - val_loss: 0.5632 - val_acc: 0.8503\n",
      "Epoch 70/85\n",
      " - 18s - loss: 0.6180 - acc: 0.7742 - val_loss: 0.5671 - val_acc: 0.8472\n",
      "Epoch 71/85\n",
      " - 18s - loss: 0.6190 - acc: 0.7732 - val_loss: 0.5493 - val_acc: 0.8498\n",
      "Epoch 72/85\n",
      " - 18s - loss: 0.6166 - acc: 0.7740 - val_loss: 0.5636 - val_acc: 0.8484\n",
      "Epoch 73/85\n",
      " - 18s - loss: 0.6152 - acc: 0.7751 - val_loss: 0.5827 - val_acc: 0.8476\n",
      "Epoch 74/85\n",
      " - 18s - loss: 0.6170 - acc: 0.7733 - val_loss: 0.5638 - val_acc: 0.8478\n",
      "Epoch 75/85\n",
      " - 18s - loss: 0.6166 - acc: 0.7752 - val_loss: 0.5568 - val_acc: 0.8491\n",
      "Epoch 76/85\n",
      " - 18s - loss: 0.6154 - acc: 0.7742 - val_loss: 0.5861 - val_acc: 0.8462\n",
      "Epoch 77/85\n",
      " - 18s - loss: 0.6153 - acc: 0.7738 - val_loss: 0.5667 - val_acc: 0.8460\n",
      "Epoch 78/85\n",
      " - 18s - loss: 0.6155 - acc: 0.7747 - val_loss: 0.5581 - val_acc: 0.8492\n",
      "Epoch 79/85\n",
      " - 18s - loss: 0.6131 - acc: 0.7754 - val_loss: 0.5615 - val_acc: 0.8477\n",
      "Epoch 80/85\n",
      " - 18s - loss: 0.6117 - acc: 0.7757 - val_loss: 0.5568 - val_acc: 0.8485\n",
      "Epoch 81/85\n",
      " - 18s - loss: 0.6136 - acc: 0.7762 - val_loss: 0.5709 - val_acc: 0.8470\n",
      "Epoch 82/85\n",
      " - 18s - loss: 0.6123 - acc: 0.7754 - val_loss: 0.5564 - val_acc: 0.8508\n",
      "Epoch 83/85\n",
      " - 18s - loss: 0.6108 - acc: 0.7772 - val_loss: 0.5637 - val_acc: 0.8494\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 84/85\n",
      " - 18s - loss: 0.6097 - acc: 0.7757 - val_loss: 0.5558 - val_acc: 0.8520\n",
      "Epoch 85/85\n",
      " - 18s - loss: 0.6113 - acc: 0.7753 - val_loss: 0.5938 - val_acc: 0.8281\n",
      "49958/49958 [==============================] - 8s 156us/step\n",
      " Dropout : 0.1 accuracy is : 0.777393010129\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_31 (LSTM)               (None, 1, 1000)           9928000   \n",
      "_________________________________________________________________\n",
      "dropout_31 (Dropout)         (None, 1, 1000)           0         \n",
      "_________________________________________________________________\n",
      "lstm_32 (LSTM)               (None, 800)               5763200   \n",
      "_________________________________________________________________\n",
      "dropout_32 (Dropout)         (None, 800)               0         \n",
      "_________________________________________________________________\n",
      "dense_31 (Dense)             (None, 150)               120150    \n",
      "_________________________________________________________________\n",
      "dense_32 (Dense)             (None, 3)                 453       \n",
      "=================================================================\n",
      "Total params: 15,811,803\n",
      "Trainable params: 15,811,803\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      " Model build done!\n",
      "Train on 159866 samples, validate on 39967 samples\n",
      "Epoch 1/85\n",
      " - 21s - loss: 1.0457 - acc: 0.5361 - val_loss: 1.1093 - val_acc: 0.5333\n",
      "Epoch 2/85\n",
      " - 18s - loss: 1.0385 - acc: 0.5352 - val_loss: 1.0400 - val_acc: 0.5333\n",
      "Epoch 3/85\n",
      " - 18s - loss: 0.9956 - acc: 0.5538 - val_loss: 1.0350 - val_acc: 0.5552\n",
      "Epoch 4/85\n",
      " - 18s - loss: 0.9585 - acc: 0.5675 - val_loss: 1.0253 - val_acc: 0.5652\n",
      "Epoch 5/85\n",
      " - 18s - loss: 0.9321 - acc: 0.5791 - val_loss: 0.9087 - val_acc: 0.6965\n",
      "Epoch 6/85\n",
      " - 18s - loss: 0.9016 - acc: 0.6050 - val_loss: 1.0190 - val_acc: 0.5636\n",
      "Epoch 7/85\n",
      " - 18s - loss: 0.8761 - acc: 0.6232 - val_loss: 0.8072 - val_acc: 0.7501\n",
      "Epoch 8/85\n",
      " - 18s - loss: 0.8487 - acc: 0.6456 - val_loss: 0.7332 - val_acc: 0.7461\n",
      "Epoch 9/85\n",
      " - 18s - loss: 0.8343 - acc: 0.6561 - val_loss: 0.6487 - val_acc: 0.8309\n",
      "Epoch 10/85\n",
      " - 18s - loss: 0.8004 - acc: 0.6798 - val_loss: 0.6838 - val_acc: 0.8076\n",
      "Epoch 11/85\n",
      " - 18s - loss: 0.7922 - acc: 0.6919 - val_loss: 0.6915 - val_acc: 0.8126\n",
      "Epoch 12/85\n",
      " - 18s - loss: 0.7683 - acc: 0.7054 - val_loss: 0.9079 - val_acc: 0.7229\n",
      "Epoch 13/85\n",
      " - 18s - loss: 0.7803 - acc: 0.7000 - val_loss: 0.5958 - val_acc: 0.8401\n",
      "Epoch 14/85\n",
      " - 18s - loss: 0.7509 - acc: 0.7207 - val_loss: 0.6760 - val_acc: 0.7982\n",
      "Epoch 15/85\n",
      " - 18s - loss: 0.7456 - acc: 0.7241 - val_loss: 0.6634 - val_acc: 0.7588\n",
      "Epoch 16/85\n",
      " - 18s - loss: 0.7423 - acc: 0.7244 - val_loss: 0.5905 - val_acc: 0.8395\n",
      "Epoch 17/85\n",
      " - 18s - loss: 0.7345 - acc: 0.7281 - val_loss: 0.5876 - val_acc: 0.8430\n",
      "Epoch 18/85\n",
      " - 18s - loss: 0.7297 - acc: 0.7322 - val_loss: 0.5768 - val_acc: 0.8435\n",
      "Epoch 19/85\n",
      " - 18s - loss: 0.7174 - acc: 0.7413 - val_loss: 0.6621 - val_acc: 0.8052\n",
      "Epoch 20/85\n",
      " - 18s - loss: 0.7242 - acc: 0.7324 - val_loss: 0.5868 - val_acc: 0.8443\n",
      "Epoch 21/85\n",
      " - 18s - loss: 0.7105 - acc: 0.7434 - val_loss: 0.5705 - val_acc: 0.8470\n",
      "Epoch 22/85\n",
      " - 18s - loss: 0.7090 - acc: 0.7435 - val_loss: 0.6157 - val_acc: 0.8341\n",
      "Epoch 23/85\n",
      " - 18s - loss: 0.7042 - acc: 0.7429 - val_loss: 0.7241 - val_acc: 0.7683\n",
      "Epoch 24/85\n",
      " - 18s - loss: 0.7094 - acc: 0.7397 - val_loss: 0.5700 - val_acc: 0.8432\n",
      "Epoch 25/85\n",
      " - 18s - loss: 0.7005 - acc: 0.7451 - val_loss: 0.6590 - val_acc: 0.8480\n",
      "Epoch 26/85\n",
      " - 18s - loss: 0.6870 - acc: 0.7515 - val_loss: 0.8718 - val_acc: 0.6668\n",
      "Epoch 27/85\n",
      " - 18s - loss: 0.6929 - acc: 0.7476 - val_loss: 0.7517 - val_acc: 0.7860\n",
      "Epoch 28/85\n",
      " - 18s - loss: 0.6842 - acc: 0.7501 - val_loss: 0.5867 - val_acc: 0.8429\n",
      "Epoch 29/85\n",
      " - 18s - loss: 0.6870 - acc: 0.7511 - val_loss: 0.7712 - val_acc: 0.7475\n",
      "Epoch 30/85\n",
      " - 18s - loss: 0.6864 - acc: 0.7489 - val_loss: 0.5792 - val_acc: 0.8413\n",
      "Epoch 31/85\n",
      " - 18s - loss: 0.6757 - acc: 0.7545 - val_loss: 0.5962 - val_acc: 0.8400\n",
      "Epoch 32/85\n",
      " - 18s - loss: 0.6715 - acc: 0.7547 - val_loss: 0.6032 - val_acc: 0.8306\n",
      "Epoch 33/85\n",
      " - 18s - loss: 0.6745 - acc: 0.7533 - val_loss: 0.6004 - val_acc: 0.8397\n",
      "Epoch 34/85\n",
      " - 18s - loss: 0.6700 - acc: 0.7545 - val_loss: 0.6792 - val_acc: 0.8413\n",
      "Epoch 35/85\n",
      " - 18s - loss: 0.6720 - acc: 0.7546 - val_loss: 0.7656 - val_acc: 0.7316\n",
      "Epoch 36/85\n",
      " - 18s - loss: 0.6821 - acc: 0.7484 - val_loss: 0.6678 - val_acc: 0.8382\n",
      "Epoch 37/85\n",
      " - 18s - loss: 0.6589 - acc: 0.7606 - val_loss: 0.5935 - val_acc: 0.8374\n",
      "Epoch 38/85\n",
      " - 18s - loss: 0.6700 - acc: 0.7523 - val_loss: 0.6047 - val_acc: 0.8338\n",
      "Epoch 39/85\n",
      " - 18s - loss: 0.6675 - acc: 0.7549 - val_loss: 0.6560 - val_acc: 0.8424\n",
      "Epoch 40/85\n",
      " - 18s - loss: 0.6567 - acc: 0.7610 - val_loss: 0.6005 - val_acc: 0.8464\n",
      "Epoch 41/85\n",
      " - 18s - loss: 0.6603 - acc: 0.7584 - val_loss: 0.6710 - val_acc: 0.8380\n",
      "Epoch 42/85\n",
      " - 18s - loss: 0.6527 - acc: 0.7616 - val_loss: 0.5700 - val_acc: 0.8484\n",
      "Epoch 43/85\n",
      " - 18s - loss: 0.6523 - acc: 0.7628 - val_loss: 0.5734 - val_acc: 0.8473\n",
      "Epoch 44/85\n",
      " - 18s - loss: 0.6501 - acc: 0.7612 - val_loss: 0.7009 - val_acc: 0.7991\n",
      "Epoch 45/85\n",
      " - 18s - loss: 0.6483 - acc: 0.7610 - val_loss: 0.6143 - val_acc: 0.8419\n",
      "Epoch 46/85\n",
      " - 18s - loss: 0.6458 - acc: 0.7644 - val_loss: 0.5717 - val_acc: 0.8334\n",
      "Epoch 47/85\n",
      " - 18s - loss: 0.6427 - acc: 0.7650 - val_loss: 0.6756 - val_acc: 0.7941\n",
      "Epoch 48/85\n",
      " - 18s - loss: 0.6452 - acc: 0.7653 - val_loss: 0.5659 - val_acc: 0.8478\n",
      "Epoch 49/85\n",
      " - 18s - loss: 0.6407 - acc: 0.7646 - val_loss: 0.5960 - val_acc: 0.8482\n",
      "Epoch 50/85\n",
      " - 18s - loss: 0.6389 - acc: 0.7656 - val_loss: 0.5770 - val_acc: 0.8428\n",
      "Epoch 51/85\n",
      " - 18s - loss: 0.6433 - acc: 0.7618 - val_loss: 0.6585 - val_acc: 0.8198\n",
      "Epoch 52/85\n",
      " - 18s - loss: 0.6364 - acc: 0.7678 - val_loss: 0.5722 - val_acc: 0.8496\n",
      "Epoch 53/85\n",
      " - 18s - loss: 0.6393 - acc: 0.7658 - val_loss: 0.5679 - val_acc: 0.8478\n",
      "Epoch 54/85\n",
      " - 18s - loss: 0.6321 - acc: 0.7686 - val_loss: 0.6337 - val_acc: 0.8206\n",
      "Epoch 55/85\n",
      " - 18s - loss: 0.6355 - acc: 0.7654 - val_loss: 0.6045 - val_acc: 0.8340\n",
      "Epoch 56/85\n",
      " - 18s - loss: 0.6326 - acc: 0.7679 - val_loss: 0.5627 - val_acc: 0.8487\n",
      "Epoch 57/85\n",
      " - 18s - loss: 0.6305 - acc: 0.7692 - val_loss: 0.6130 - val_acc: 0.8231\n",
      "Epoch 58/85\n",
      " - 18s - loss: 0.6283 - acc: 0.7695 - val_loss: 0.7207 - val_acc: 0.7461\n",
      "Epoch 59/85\n",
      " - 18s - loss: 0.6325 - acc: 0.7673 - val_loss: 0.6248 - val_acc: 0.8302\n",
      "Epoch 60/85\n",
      " - 18s - loss: 0.6271 - acc: 0.7718 - val_loss: 0.5638 - val_acc: 0.8443\n",
      "Epoch 61/85\n",
      " - 18s - loss: 0.6270 - acc: 0.7689 - val_loss: 0.5634 - val_acc: 0.8454\n",
      "Epoch 62/85\n",
      " - 18s - loss: 0.6272 - acc: 0.7695 - val_loss: 0.6006 - val_acc: 0.8379\n",
      "Epoch 63/85\n",
      " - 18s - loss: 0.6281 - acc: 0.7692 - val_loss: 0.5854 - val_acc: 0.8413\n",
      "Epoch 64/85\n",
      " - 18s - loss: 0.6241 - acc: 0.7722 - val_loss: 0.5930 - val_acc: 0.8423\n",
      "Epoch 65/85\n",
      " - 18s - loss: 0.6258 - acc: 0.7712 - val_loss: 0.7738 - val_acc: 0.7300\n",
      "Epoch 66/85\n",
      " - 18s - loss: 0.6267 - acc: 0.7701 - val_loss: 0.5559 - val_acc: 0.8466\n",
      "Epoch 67/85\n",
      " - 18s - loss: 0.6209 - acc: 0.7725 - val_loss: 0.5889 - val_acc: 0.8362\n",
      "Epoch 68/85\n",
      " - 18s - loss: 0.6214 - acc: 0.7718 - val_loss: 0.6773 - val_acc: 0.7812\n",
      "Epoch 69/85\n",
      " - 18s - loss: 0.6238 - acc: 0.7723 - val_loss: 0.7496 - val_acc: 0.7316\n",
      "Epoch 70/85\n",
      " - 18s - loss: 0.6188 - acc: 0.7738 - val_loss: 0.5765 - val_acc: 0.8468\n",
      "Epoch 71/85\n",
      " - 18s - loss: 0.6234 - acc: 0.7716 - val_loss: 0.8671 - val_acc: 0.6433\n",
      "Epoch 72/85\n",
      " - 18s - loss: 0.6177 - acc: 0.7733 - val_loss: 0.5630 - val_acc: 0.8477\n",
      "Epoch 73/85\n",
      " - 18s - loss: 0.6183 - acc: 0.7738 - val_loss: 0.5635 - val_acc: 0.8450\n",
      "Epoch 74/85\n",
      " - 18s - loss: 0.6212 - acc: 0.7713 - val_loss: 0.5593 - val_acc: 0.8503\n",
      "Epoch 75/85\n",
      " - 18s - loss: 0.6180 - acc: 0.7735 - val_loss: 0.8197 - val_acc: 0.6930\n",
      "Epoch 76/85\n",
      " - 18s - loss: 0.6221 - acc: 0.7723 - val_loss: 0.7278 - val_acc: 0.7480\n",
      "Epoch 77/85\n",
      " - 18s - loss: 0.6145 - acc: 0.7748 - val_loss: 0.5797 - val_acc: 0.8365\n",
      "Epoch 78/85\n",
      " - 18s - loss: 0.6200 - acc: 0.7723 - val_loss: 0.5736 - val_acc: 0.8402\n",
      "Epoch 79/85\n",
      " - 18s - loss: 0.6182 - acc: 0.7730 - val_loss: 0.5557 - val_acc: 0.8464\n",
      "Epoch 80/85\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 18s - loss: 0.6126 - acc: 0.7747 - val_loss: 0.5675 - val_acc: 0.8457\n",
      "Epoch 81/85\n",
      " - 18s - loss: 0.6160 - acc: 0.7746 - val_loss: 0.5506 - val_acc: 0.8483\n",
      "Epoch 82/85\n",
      " - 18s - loss: 0.6142 - acc: 0.7751 - val_loss: 0.8134 - val_acc: 0.7065\n",
      "Epoch 83/85\n",
      " - 18s - loss: 0.6151 - acc: 0.7744 - val_loss: 0.5674 - val_acc: 0.8446\n",
      "Epoch 84/85\n",
      " - 18s - loss: 0.6150 - acc: 0.7729 - val_loss: 0.6387 - val_acc: 0.8098\n",
      "Epoch 85/85\n",
      " - 18s - loss: 0.6110 - acc: 0.7745 - val_loss: 0.6430 - val_acc: 0.8032\n",
      "49958/49958 [==============================] - 8s 155us/step\n",
      " Dropout : 0.15 accuracy is : 0.761759878298\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_33 (LSTM)               (None, 1, 1000)           9928000   \n",
      "_________________________________________________________________\n",
      "dropout_33 (Dropout)         (None, 1, 1000)           0         \n",
      "_________________________________________________________________\n",
      "lstm_34 (LSTM)               (None, 800)               5763200   \n",
      "_________________________________________________________________\n",
      "dropout_34 (Dropout)         (None, 800)               0         \n",
      "_________________________________________________________________\n",
      "dense_33 (Dense)             (None, 150)               120150    \n",
      "_________________________________________________________________\n",
      "dense_34 (Dense)             (None, 3)                 453       \n",
      "=================================================================\n",
      "Total params: 15,811,803\n",
      "Trainable params: 15,811,803\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      " Model build done!\n",
      "Train on 159866 samples, validate on 39967 samples\n",
      "Epoch 1/85\n",
      " - 22s - loss: 1.0459 - acc: 0.5350 - val_loss: 1.1089 - val_acc: 0.5333\n",
      "Epoch 2/85\n",
      " - 18s - loss: 1.0427 - acc: 0.5338 - val_loss: 1.0694 - val_acc: 0.5333\n",
      "Epoch 3/85\n",
      " - 18s - loss: 1.0099 - acc: 0.5485 - val_loss: 0.9666 - val_acc: 0.5968\n",
      "Epoch 4/85\n",
      " - 18s - loss: 0.9680 - acc: 0.5642 - val_loss: 0.9064 - val_acc: 0.6336\n",
      "Epoch 5/85\n",
      " - 18s - loss: 0.9360 - acc: 0.5756 - val_loss: 0.8787 - val_acc: 0.6847\n",
      "Epoch 6/85\n",
      " - 18s - loss: 0.9098 - acc: 0.5985 - val_loss: 0.9390 - val_acc: 0.6285\n",
      "Epoch 7/85\n",
      " - 18s - loss: 0.8851 - acc: 0.6165 - val_loss: 0.8175 - val_acc: 0.7504\n",
      "Epoch 8/85\n",
      " - 18s - loss: 0.8580 - acc: 0.6366 - val_loss: 0.6696 - val_acc: 0.7867\n",
      "Epoch 9/85\n",
      " - 18s - loss: 0.8359 - acc: 0.6540 - val_loss: 1.0099 - val_acc: 0.6836\n",
      "Epoch 10/85\n",
      " - 18s - loss: 0.8201 - acc: 0.6658 - val_loss: 0.7286 - val_acc: 0.7921\n",
      "Epoch 11/85\n",
      " - 18s - loss: 0.7968 - acc: 0.6859 - val_loss: 0.8232 - val_acc: 0.7574\n",
      "Epoch 12/85\n",
      " - 18s - loss: 0.7842 - acc: 0.7009 - val_loss: 0.5980 - val_acc: 0.8383\n",
      "Epoch 13/85\n",
      " - 18s - loss: 0.8024 - acc: 0.6828 - val_loss: 0.7305 - val_acc: 0.8017\n",
      "Epoch 14/85\n",
      " - 18s - loss: 0.7692 - acc: 0.7054 - val_loss: 0.6026 - val_acc: 0.8364\n",
      "Epoch 15/85\n",
      " - 18s - loss: 0.7573 - acc: 0.7149 - val_loss: 0.6155 - val_acc: 0.8385\n",
      "Epoch 16/85\n",
      " - 18s - loss: 0.7496 - acc: 0.7205 - val_loss: 0.5862 - val_acc: 0.8409\n",
      "Epoch 17/85\n",
      " - 18s - loss: 0.7431 - acc: 0.7244 - val_loss: 0.5919 - val_acc: 0.8398\n",
      "Epoch 18/85\n",
      " - 18s - loss: 0.7332 - acc: 0.7294 - val_loss: 0.5882 - val_acc: 0.8391\n",
      "Epoch 19/85\n",
      " - 18s - loss: 0.7351 - acc: 0.7304 - val_loss: 0.5817 - val_acc: 0.8392\n",
      "Epoch 20/85\n",
      " - 18s - loss: 0.7271 - acc: 0.7375 - val_loss: 0.6483 - val_acc: 0.8312\n",
      "Epoch 21/85\n",
      " - 18s - loss: 0.7192 - acc: 0.7413 - val_loss: 0.5823 - val_acc: 0.8402\n",
      "Epoch 22/85\n",
      " - 18s - loss: 0.7172 - acc: 0.7415 - val_loss: 0.5797 - val_acc: 0.8376\n",
      "Epoch 23/85\n",
      " - 18s - loss: 0.7103 - acc: 0.7486 - val_loss: 0.6721 - val_acc: 0.8139\n",
      "Epoch 24/85\n",
      " - 18s - loss: 0.7149 - acc: 0.7388 - val_loss: 0.5868 - val_acc: 0.8423\n",
      "Epoch 25/85\n",
      " - 18s - loss: 0.7139 - acc: 0.7456 - val_loss: 0.7104 - val_acc: 0.8283\n",
      "Epoch 26/85\n",
      " - 18s - loss: 0.7020 - acc: 0.7480 - val_loss: 0.6600 - val_acc: 0.8212\n",
      "Epoch 27/85\n",
      " - 18s - loss: 0.7017 - acc: 0.7480 - val_loss: 0.8579 - val_acc: 0.6470\n",
      "Epoch 28/85\n",
      " - 18s - loss: 0.6964 - acc: 0.7495 - val_loss: 0.6256 - val_acc: 0.8472\n",
      "Epoch 29/85\n",
      " - 18s - loss: 0.6898 - acc: 0.7513 - val_loss: 0.5671 - val_acc: 0.8405\n",
      "Epoch 30/85\n",
      " - 18s - loss: 0.6912 - acc: 0.7497 - val_loss: 0.9302 - val_acc: 0.5806\n",
      "Epoch 31/85\n",
      " - 18s - loss: 0.6857 - acc: 0.7540 - val_loss: 0.5602 - val_acc: 0.8412\n",
      "Epoch 32/85\n",
      " - 18s - loss: 0.6847 - acc: 0.7508 - val_loss: 0.6821 - val_acc: 0.8411\n",
      "Epoch 33/85\n",
      " - 18s - loss: 0.6800 - acc: 0.7560 - val_loss: 0.5514 - val_acc: 0.8421\n",
      "Epoch 34/85\n",
      " - 18s - loss: 0.6803 - acc: 0.7543 - val_loss: 0.6125 - val_acc: 0.8414\n",
      "Epoch 35/85\n",
      " - 18s - loss: 0.6768 - acc: 0.7558 - val_loss: 1.0414 - val_acc: 0.4704\n",
      "Epoch 36/85\n",
      " - 18s - loss: 0.6771 - acc: 0.7545 - val_loss: 0.6482 - val_acc: 0.8000\n",
      "Epoch 37/85\n",
      " - 18s - loss: 0.6729 - acc: 0.7562 - val_loss: 0.7162 - val_acc: 0.7787\n",
      "Epoch 38/85\n",
      " - 18s - loss: 0.6706 - acc: 0.7562 - val_loss: 0.6743 - val_acc: 0.8292\n",
      "Epoch 39/85\n",
      " - 18s - loss: 0.6664 - acc: 0.7601 - val_loss: 1.0066 - val_acc: 0.4978\n",
      "Epoch 40/85\n",
      " - 18s - loss: 0.6612 - acc: 0.7609 - val_loss: 0.6478 - val_acc: 0.8298\n",
      "Epoch 41/85\n",
      " - 18s - loss: 0.6612 - acc: 0.7620 - val_loss: 1.2083 - val_acc: 0.4097\n",
      "Epoch 42/85\n",
      " - 18s - loss: 0.6588 - acc: 0.7610 - val_loss: 0.5494 - val_acc: 0.8456\n",
      "Epoch 43/85\n",
      " - 18s - loss: 0.6592 - acc: 0.7630 - val_loss: 0.5442 - val_acc: 0.8513\n",
      "Epoch 44/85\n",
      " - 18s - loss: 0.6616 - acc: 0.7617 - val_loss: 1.2326 - val_acc: 0.5038\n",
      "Epoch 45/85\n",
      " - 18s - loss: 0.6580 - acc: 0.7624 - val_loss: 0.6158 - val_acc: 0.8467\n",
      "Epoch 46/85\n",
      " - 18s - loss: 0.6523 - acc: 0.7650 - val_loss: 0.5986 - val_acc: 0.8201\n",
      "Epoch 47/85\n",
      " - 18s - loss: 0.6514 - acc: 0.7645 - val_loss: 0.7102 - val_acc: 0.7555\n",
      "Epoch 48/85\n",
      " - 18s - loss: 0.6481 - acc: 0.7658 - val_loss: 0.6101 - val_acc: 0.8324\n",
      "Epoch 49/85\n",
      " - 18s - loss: 0.6493 - acc: 0.7658 - val_loss: 0.5784 - val_acc: 0.8400\n",
      "Epoch 50/85\n",
      " - 18s - loss: 0.6491 - acc: 0.7653 - val_loss: 0.5574 - val_acc: 0.8519\n",
      "Epoch 51/85\n",
      " - 18s - loss: 0.6484 - acc: 0.7652 - val_loss: 0.8722 - val_acc: 0.6318\n",
      "Epoch 52/85\n",
      " - 18s - loss: 0.6459 - acc: 0.7657 - val_loss: 0.5609 - val_acc: 0.8522\n",
      "Epoch 53/85\n",
      " - 18s - loss: 0.6440 - acc: 0.7660 - val_loss: 0.5695 - val_acc: 0.8504\n",
      "Epoch 54/85\n",
      " - 18s - loss: 0.6414 - acc: 0.7671 - val_loss: 0.5661 - val_acc: 0.8454\n",
      "Epoch 55/85\n",
      " - 18s - loss: 0.6433 - acc: 0.7661 - val_loss: 0.5532 - val_acc: 0.8483\n",
      "Epoch 56/85\n",
      " - 18s - loss: 0.6414 - acc: 0.7670 - val_loss: 0.6858 - val_acc: 0.8004\n",
      "Epoch 57/85\n",
      " - 18s - loss: 0.6403 - acc: 0.7666 - val_loss: 0.5597 - val_acc: 0.8457\n",
      "Epoch 58/85\n",
      " - 18s - loss: 0.6353 - acc: 0.7696 - val_loss: 0.5570 - val_acc: 0.8495\n",
      "Epoch 59/85\n",
      " - 18s - loss: 0.6377 - acc: 0.7692 - val_loss: 0.5698 - val_acc: 0.8495\n",
      "Epoch 60/85\n",
      " - 18s - loss: 0.6367 - acc: 0.7684 - val_loss: 0.5838 - val_acc: 0.8414\n",
      "Epoch 61/85\n",
      " - 18s - loss: 0.6360 - acc: 0.7685 - val_loss: 0.6709 - val_acc: 0.7963\n",
      "Epoch 62/85\n",
      " - 18s - loss: 0.6345 - acc: 0.7693 - val_loss: 0.6904 - val_acc: 0.7895\n",
      "Epoch 63/85\n",
      " - 18s - loss: 0.6332 - acc: 0.7699 - val_loss: 0.7179 - val_acc: 0.7685\n",
      "Epoch 64/85\n",
      " - 18s - loss: 0.6312 - acc: 0.7695 - val_loss: 0.5925 - val_acc: 0.8377\n",
      "Epoch 65/85\n",
      " - 18s - loss: 0.6312 - acc: 0.7709 - val_loss: 0.5677 - val_acc: 0.8490\n",
      "Epoch 66/85\n",
      " - 18s - loss: 0.6299 - acc: 0.7708 - val_loss: 0.5591 - val_acc: 0.8435\n",
      "Epoch 67/85\n",
      " - 18s - loss: 0.6308 - acc: 0.7693 - val_loss: 0.6690 - val_acc: 0.8066\n",
      "Epoch 68/85\n",
      " - 18s - loss: 0.6308 - acc: 0.7713 - val_loss: 0.5737 - val_acc: 0.8487\n",
      "Epoch 69/85\n",
      " - 18s - loss: 0.6301 - acc: 0.7715 - val_loss: 0.5646 - val_acc: 0.8460\n",
      "Epoch 70/85\n",
      " - 18s - loss: 0.6353 - acc: 0.7691 - val_loss: 0.5585 - val_acc: 0.8468\n",
      "Epoch 71/85\n",
      " - 18s - loss: 0.6279 - acc: 0.7724 - val_loss: 0.5608 - val_acc: 0.8492\n",
      "Epoch 72/85\n",
      " - 18s - loss: 0.6289 - acc: 0.7698 - val_loss: 0.6178 - val_acc: 0.8221\n",
      "Epoch 73/85\n",
      " - 18s - loss: 0.6276 - acc: 0.7715 - val_loss: 0.5478 - val_acc: 0.8508\n",
      "Epoch 74/85\n",
      " - 18s - loss: 0.6261 - acc: 0.7720 - val_loss: 0.5868 - val_acc: 0.8432\n",
      "Epoch 75/85\n",
      " - 18s - loss: 0.6277 - acc: 0.7704 - val_loss: 0.5444 - val_acc: 0.8491\n",
      "Epoch 76/85\n",
      " - 18s - loss: 0.6234 - acc: 0.7735 - val_loss: 0.5541 - val_acc: 0.8485\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 77/85\n",
      " - 18s - loss: 0.6262 - acc: 0.7720 - val_loss: 0.5528 - val_acc: 0.8489\n",
      "Epoch 78/85\n",
      " - 18s - loss: 0.6231 - acc: 0.7734 - val_loss: 0.5597 - val_acc: 0.8471\n",
      "Epoch 79/85\n",
      " - 18s - loss: 0.6241 - acc: 0.7713 - val_loss: 0.5588 - val_acc: 0.8456\n",
      "Epoch 80/85\n",
      " - 18s - loss: 0.6223 - acc: 0.7736 - val_loss: 0.5673 - val_acc: 0.8448\n",
      "Epoch 81/85\n",
      " - 18s - loss: 0.6214 - acc: 0.7731 - val_loss: 0.5964 - val_acc: 0.8290\n",
      "Epoch 82/85\n",
      " - 18s - loss: 0.6242 - acc: 0.7717 - val_loss: 0.5970 - val_acc: 0.8394\n",
      "Epoch 83/85\n",
      " - 18s - loss: 0.6251 - acc: 0.7719 - val_loss: 0.6260 - val_acc: 0.8062\n",
      "Epoch 84/85\n",
      " - 18s - loss: 0.6204 - acc: 0.7739 - val_loss: 0.6151 - val_acc: 0.8465\n",
      "Epoch 85/85\n",
      " - 18s - loss: 0.6188 - acc: 0.7737 - val_loss: 0.5613 - val_acc: 0.8473\n",
      "49958/49958 [==============================] - 8s 156us/step\n",
      " Dropout : 0.2 accuracy is : 0.817406621562\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_35 (LSTM)               (None, 1, 1000)           9928000   \n",
      "_________________________________________________________________\n",
      "dropout_35 (Dropout)         (None, 1, 1000)           0         \n",
      "_________________________________________________________________\n",
      "lstm_36 (LSTM)               (None, 800)               5763200   \n",
      "_________________________________________________________________\n",
      "dropout_36 (Dropout)         (None, 800)               0         \n",
      "_________________________________________________________________\n",
      "dense_35 (Dense)             (None, 150)               120150    \n",
      "_________________________________________________________________\n",
      "dense_36 (Dense)             (None, 3)                 453       \n",
      "=================================================================\n",
      "Total params: 15,811,803\n",
      "Trainable params: 15,811,803\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      " Model build done!\n",
      "Train on 159866 samples, validate on 39967 samples\n",
      "Epoch 1/85\n",
      " - 22s - loss: 1.0459 - acc: 0.5354 - val_loss: 1.1089 - val_acc: 0.5333\n",
      "Epoch 2/85\n",
      " - 18s - loss: 1.0404 - acc: 0.5314 - val_loss: 1.0258 - val_acc: 0.5333\n",
      "Epoch 3/85\n",
      " - 18s - loss: 0.9983 - acc: 0.5525 - val_loss: 0.8675 - val_acc: 0.6523\n",
      "Epoch 4/85\n",
      " - 18s - loss: 0.9620 - acc: 0.5680 - val_loss: 1.0018 - val_acc: 0.5711\n",
      "Epoch 5/85\n",
      " - 18s - loss: 0.9356 - acc: 0.5797 - val_loss: 0.9683 - val_acc: 0.5951\n",
      "Epoch 6/85\n",
      " - 18s - loss: 0.9013 - acc: 0.6084 - val_loss: 0.9813 - val_acc: 0.5858\n",
      "Epoch 7/85\n",
      " - 18s - loss: 0.8730 - acc: 0.6307 - val_loss: 0.9924 - val_acc: 0.5759\n",
      "Epoch 8/85\n",
      " - 18s - loss: 0.8517 - acc: 0.6424 - val_loss: 0.7180 - val_acc: 0.8094\n",
      "Epoch 9/85\n",
      " - 18s - loss: 0.8265 - acc: 0.6582 - val_loss: 0.6498 - val_acc: 0.8319\n",
      "Epoch 10/85\n",
      " - 18s - loss: 0.8037 - acc: 0.6785 - val_loss: 1.0710 - val_acc: 0.6824\n",
      "Epoch 11/85\n",
      " - 18s - loss: 0.7928 - acc: 0.6888 - val_loss: 0.6229 - val_acc: 0.8324\n",
      "Epoch 12/85\n",
      " - 18s - loss: 0.7795 - acc: 0.6996 - val_loss: 0.6777 - val_acc: 0.8120\n",
      "Epoch 13/85\n",
      " - 18s - loss: 0.7668 - acc: 0.7089 - val_loss: 0.5945 - val_acc: 0.8354\n",
      "Epoch 14/85\n",
      " - 18s - loss: 0.7947 - acc: 0.6936 - val_loss: 0.6372 - val_acc: 0.8346\n",
      "Epoch 15/85\n",
      " - 18s - loss: 0.7480 - acc: 0.7213 - val_loss: 0.6296 - val_acc: 0.8239\n",
      "Epoch 16/85\n",
      " - 18s - loss: 0.7424 - acc: 0.7251 - val_loss: 0.7006 - val_acc: 0.7925\n",
      "Epoch 17/85\n",
      " - 18s - loss: 0.7506 - acc: 0.7218 - val_loss: 0.6360 - val_acc: 0.8106\n",
      "Epoch 18/85\n",
      " - 18s - loss: 0.7325 - acc: 0.7295 - val_loss: 0.6219 - val_acc: 0.8299\n",
      "Epoch 19/85\n",
      " - 18s - loss: 0.7272 - acc: 0.7327 - val_loss: 0.7021 - val_acc: 0.8032\n",
      "Epoch 20/85\n",
      " - 18s - loss: 0.7208 - acc: 0.7390 - val_loss: 0.8536 - val_acc: 0.6394\n",
      "Epoch 21/85\n",
      " - 18s - loss: 0.7291 - acc: 0.7295 - val_loss: 0.5786 - val_acc: 0.8414\n",
      "Epoch 22/85\n",
      " - 18s - loss: 0.7080 - acc: 0.7461 - val_loss: 0.6392 - val_acc: 0.8167\n",
      "Epoch 23/85\n",
      " - 18s - loss: 0.7156 - acc: 0.7408 - val_loss: 0.5795 - val_acc: 0.8351\n",
      "Epoch 24/85\n",
      " - 18s - loss: 0.7046 - acc: 0.7442 - val_loss: 0.5640 - val_acc: 0.8391\n",
      "Epoch 25/85\n",
      " - 18s - loss: 0.7013 - acc: 0.7457 - val_loss: 0.6429 - val_acc: 0.8100\n",
      "Epoch 26/85\n",
      " - 18s - loss: 0.6971 - acc: 0.7484 - val_loss: 0.6924 - val_acc: 0.8171\n",
      "Epoch 27/85\n",
      " - 18s - loss: 0.6956 - acc: 0.7465 - val_loss: 0.5794 - val_acc: 0.8401\n",
      "Epoch 28/85\n",
      " - 18s - loss: 0.6865 - acc: 0.7514 - val_loss: 0.7598 - val_acc: 0.6718\n",
      "Epoch 29/85\n",
      " - 18s - loss: 0.6882 - acc: 0.7534 - val_loss: 0.5737 - val_acc: 0.8413\n",
      "Epoch 30/85\n",
      " - 18s - loss: 0.6833 - acc: 0.7547 - val_loss: 0.5766 - val_acc: 0.8400\n",
      "Epoch 31/85\n",
      " - 18s - loss: 0.6805 - acc: 0.7535 - val_loss: 0.5645 - val_acc: 0.8436\n",
      "Epoch 32/85\n",
      " - 18s - loss: 0.6805 - acc: 0.7517 - val_loss: 0.5511 - val_acc: 0.8441\n",
      "Epoch 33/85\n",
      " - 18s - loss: 0.6765 - acc: 0.7529 - val_loss: 0.6025 - val_acc: 0.8296\n",
      "Epoch 34/85\n",
      " - 18s - loss: 0.6755 - acc: 0.7545 - val_loss: 0.6081 - val_acc: 0.8476\n",
      "Epoch 35/85\n",
      " - 18s - loss: 0.6677 - acc: 0.7584 - val_loss: 0.7128 - val_acc: 0.6692\n",
      "Epoch 36/85\n",
      " - 18s - loss: 0.6750 - acc: 0.7474 - val_loss: 0.5626 - val_acc: 0.8456\n",
      "Epoch 37/85\n",
      " - 18s - loss: 0.6672 - acc: 0.7594 - val_loss: 0.5572 - val_acc: 0.8448\n",
      "Epoch 38/85\n",
      " - 18s - loss: 0.6671 - acc: 0.7554 - val_loss: 0.6473 - val_acc: 0.7926\n",
      "Epoch 39/85\n",
      " - 18s - loss: 0.6652 - acc: 0.7560 - val_loss: 0.6334 - val_acc: 0.8335\n",
      "Epoch 40/85\n",
      " - 18s - loss: 0.6582 - acc: 0.7611 - val_loss: 0.5799 - val_acc: 0.8452\n",
      "Epoch 41/85\n",
      " - 18s - loss: 0.6571 - acc: 0.7630 - val_loss: 0.5517 - val_acc: 0.8514\n",
      "Epoch 42/85\n",
      " - 18s - loss: 0.6619 - acc: 0.7587 - val_loss: 0.5532 - val_acc: 0.8475\n",
      "Epoch 43/85\n",
      " - 18s - loss: 0.6539 - acc: 0.7638 - val_loss: 0.5785 - val_acc: 0.8428\n",
      "Epoch 44/85\n",
      " - 18s - loss: 0.6495 - acc: 0.7631 - val_loss: 0.5577 - val_acc: 0.8474\n",
      "Epoch 45/85\n",
      " - 18s - loss: 0.6531 - acc: 0.7616 - val_loss: 0.5734 - val_acc: 0.8474\n",
      "Epoch 46/85\n",
      " - 18s - loss: 0.6483 - acc: 0.7636 - val_loss: 0.5609 - val_acc: 0.8470\n",
      "Epoch 47/85\n",
      " - 18s - loss: 0.6449 - acc: 0.7662 - val_loss: 0.6671 - val_acc: 0.7865\n",
      "Epoch 48/85\n",
      " - 18s - loss: 0.6516 - acc: 0.7619 - val_loss: 0.5995 - val_acc: 0.8327\n",
      "Epoch 49/85\n",
      " - 18s - loss: 0.6447 - acc: 0.7633 - val_loss: 0.6597 - val_acc: 0.8242\n",
      "Epoch 50/85\n",
      " - 18s - loss: 0.6466 - acc: 0.7643 - val_loss: 0.5517 - val_acc: 0.8506\n",
      "Epoch 51/85\n",
      " - 18s - loss: 0.6409 - acc: 0.7676 - val_loss: 0.5613 - val_acc: 0.8476\n",
      "Epoch 52/85\n",
      " - 18s - loss: 0.6405 - acc: 0.7676 - val_loss: 0.5603 - val_acc: 0.8467\n",
      "Epoch 53/85\n",
      " - 18s - loss: 0.6385 - acc: 0.7668 - val_loss: 0.5640 - val_acc: 0.8469\n",
      "Epoch 54/85\n",
      " - 18s - loss: 0.6362 - acc: 0.7670 - val_loss: 0.6430 - val_acc: 0.8105\n",
      "Epoch 55/85\n",
      " - 18s - loss: 0.6375 - acc: 0.7681 - val_loss: 0.5507 - val_acc: 0.8471\n",
      "Epoch 56/85\n",
      " - 18s - loss: 0.6359 - acc: 0.7681 - val_loss: 0.5583 - val_acc: 0.8470\n",
      "Epoch 57/85\n",
      " - 18s - loss: 0.6332 - acc: 0.7689 - val_loss: 0.5681 - val_acc: 0.8508\n",
      "Epoch 58/85\n",
      " - 18s - loss: 0.6338 - acc: 0.7684 - val_loss: 0.5637 - val_acc: 0.8496\n",
      "Epoch 59/85\n",
      " - 18s - loss: 0.6363 - acc: 0.7692 - val_loss: 0.6053 - val_acc: 0.8428\n",
      "Epoch 60/85\n",
      " - 18s - loss: 0.6331 - acc: 0.7679 - val_loss: 0.5500 - val_acc: 0.8502\n",
      "Epoch 61/85\n",
      " - 18s - loss: 0.6312 - acc: 0.7704 - val_loss: 0.5506 - val_acc: 0.8463\n",
      "Epoch 62/85\n",
      " - 18s - loss: 0.6294 - acc: 0.7706 - val_loss: 0.5651 - val_acc: 0.8493\n",
      "Epoch 63/85\n",
      " - 18s - loss: 0.6315 - acc: 0.7692 - val_loss: 0.5512 - val_acc: 0.8478\n",
      "Epoch 64/85\n",
      " - 18s - loss: 0.6297 - acc: 0.7705 - val_loss: 0.5619 - val_acc: 0.8475\n",
      "Epoch 65/85\n",
      " - 18s - loss: 0.6259 - acc: 0.7720 - val_loss: 0.5650 - val_acc: 0.8501\n",
      "Epoch 66/85\n",
      " - 18s - loss: 0.6275 - acc: 0.7710 - val_loss: 0.5598 - val_acc: 0.8494\n",
      "Epoch 67/85\n",
      " - 18s - loss: 0.6263 - acc: 0.7722 - val_loss: 0.5692 - val_acc: 0.8448\n",
      "Epoch 68/85\n",
      " - 18s - loss: 0.6241 - acc: 0.7730 - val_loss: 0.5641 - val_acc: 0.8511\n",
      "Epoch 69/85\n",
      " - 18s - loss: 0.6253 - acc: 0.7722 - val_loss: 0.5519 - val_acc: 0.8481\n",
      "Epoch 70/85\n",
      " - 18s - loss: 0.6260 - acc: 0.7732 - val_loss: 0.5465 - val_acc: 0.8476\n",
      "Epoch 71/85\n",
      " - 18s - loss: 0.6257 - acc: 0.7706 - val_loss: 0.5543 - val_acc: 0.8516\n",
      "Epoch 72/85\n",
      " - 18s - loss: 0.6237 - acc: 0.7724 - val_loss: 0.5470 - val_acc: 0.8502\n",
      "Epoch 73/85\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 18s - loss: 0.6221 - acc: 0.7731 - val_loss: 0.5681 - val_acc: 0.8465\n",
      "Epoch 74/85\n",
      " - 18s - loss: 0.6243 - acc: 0.7726 - val_loss: 0.5623 - val_acc: 0.8466\n",
      "Epoch 75/85\n",
      " - 18s - loss: 0.6240 - acc: 0.7718 - val_loss: 0.5785 - val_acc: 0.8482\n",
      "Epoch 76/85\n",
      " - 18s - loss: 0.6228 - acc: 0.7736 - val_loss: 0.5473 - val_acc: 0.8502\n",
      "Epoch 77/85\n",
      " - 18s - loss: 0.6239 - acc: 0.7705 - val_loss: 0.5472 - val_acc: 0.8503\n",
      "Epoch 78/85\n",
      " - 18s - loss: 0.6210 - acc: 0.7705 - val_loss: 0.5374 - val_acc: 0.8523\n",
      "Epoch 79/85\n",
      " - 18s - loss: 0.6248 - acc: 0.7696 - val_loss: 0.5569 - val_acc: 0.8459\n",
      "Epoch 80/85\n",
      " - 18s - loss: 0.6184 - acc: 0.7727 - val_loss: 0.5419 - val_acc: 0.8524\n",
      "Epoch 81/85\n",
      " - 18s - loss: 0.6170 - acc: 0.7738 - val_loss: 0.5574 - val_acc: 0.8493\n",
      "Epoch 82/85\n",
      " - 18s - loss: 0.6184 - acc: 0.7729 - val_loss: 0.5574 - val_acc: 0.8478\n",
      "Epoch 83/85\n",
      " - 18s - loss: 0.6191 - acc: 0.7751 - val_loss: 0.5495 - val_acc: 0.8511\n",
      "Epoch 84/85\n",
      " - 18s - loss: 0.6182 - acc: 0.7731 - val_loss: 0.5534 - val_acc: 0.8491\n",
      "Epoch 85/85\n",
      " - 18s - loss: 0.6164 - acc: 0.7741 - val_loss: 0.5546 - val_acc: 0.8493\n",
      "49958/49958 [==============================] - 8s 157us/step\n",
      " Dropout : 0.25 accuracy is : 0.806517474679\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_37 (LSTM)               (None, 1, 1000)           9928000   \n",
      "_________________________________________________________________\n",
      "dropout_37 (Dropout)         (None, 1, 1000)           0         \n",
      "_________________________________________________________________\n",
      "lstm_38 (LSTM)               (None, 800)               5763200   \n",
      "_________________________________________________________________\n",
      "dropout_38 (Dropout)         (None, 800)               0         \n",
      "_________________________________________________________________\n",
      "dense_37 (Dense)             (None, 150)               120150    \n",
      "_________________________________________________________________\n",
      "dense_38 (Dense)             (None, 3)                 453       \n",
      "=================================================================\n",
      "Total params: 15,811,803\n",
      "Trainable params: 15,811,803\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      " Model build done!\n",
      "Train on 159866 samples, validate on 39967 samples\n",
      "Epoch 1/85\n",
      " - 22s - loss: 1.0461 - acc: 0.5358 - val_loss: 1.1064 - val_acc: 0.5333\n",
      "Epoch 2/85\n",
      " - 18s - loss: 1.0400 - acc: 0.5322 - val_loss: 1.0501 - val_acc: 0.5333\n",
      "Epoch 3/85\n",
      " - 18s - loss: 0.9945 - acc: 0.5549 - val_loss: 1.0720 - val_acc: 0.5826\n",
      "Epoch 4/85\n",
      " - 18s - loss: 0.9659 - acc: 0.5684 - val_loss: 0.8714 - val_acc: 0.6585\n",
      "Epoch 5/85\n",
      " - 18s - loss: 0.9303 - acc: 0.5826 - val_loss: 0.9539 - val_acc: 0.5789\n",
      "Epoch 6/85\n",
      " - 18s - loss: 0.9066 - acc: 0.5996 - val_loss: 0.9320 - val_acc: 0.6505\n",
      "Epoch 7/85\n",
      " - 18s - loss: 0.8786 - acc: 0.6236 - val_loss: 0.7025 - val_acc: 0.7883\n",
      "Epoch 8/85\n",
      " - 18s - loss: 0.8518 - acc: 0.6414 - val_loss: 0.8363 - val_acc: 0.7423\n",
      "Epoch 9/85\n",
      " - 18s - loss: 0.8353 - acc: 0.6538 - val_loss: 0.6648 - val_acc: 0.8154\n",
      "Epoch 10/85\n",
      " - 18s - loss: 0.8132 - acc: 0.6733 - val_loss: 0.6281 - val_acc: 0.8295\n",
      "Epoch 11/85\n",
      " - 18s - loss: 0.7960 - acc: 0.6878 - val_loss: 0.6094 - val_acc: 0.8352\n",
      "Epoch 12/85\n",
      " - 18s - loss: 0.7821 - acc: 0.6989 - val_loss: 0.6019 - val_acc: 0.8377\n",
      "Epoch 13/85\n",
      " - 18s - loss: 0.7762 - acc: 0.7052 - val_loss: 0.6282 - val_acc: 0.8384\n",
      "Epoch 14/85\n",
      " - 18s - loss: 0.7637 - acc: 0.7149 - val_loss: 0.5972 - val_acc: 0.8389\n",
      "Epoch 15/85\n",
      " - 18s - loss: 0.7537 - acc: 0.7196 - val_loss: 0.6136 - val_acc: 0.8396\n",
      "Epoch 16/85\n",
      " - 18s - loss: 0.7495 - acc: 0.7239 - val_loss: 0.6059 - val_acc: 0.8357\n",
      "Epoch 17/85\n",
      " - 18s - loss: 0.7391 - acc: 0.7335 - val_loss: 0.6062 - val_acc: 0.8386\n",
      "Epoch 18/85\n",
      " - 18s - loss: 0.7336 - acc: 0.7305 - val_loss: 0.6116 - val_acc: 0.8235\n",
      "Epoch 19/85\n",
      " - 18s - loss: 0.7358 - acc: 0.7284 - val_loss: 0.5936 - val_acc: 0.8385\n",
      "Epoch 20/85\n",
      " - 18s - loss: 0.7267 - acc: 0.7392 - val_loss: 0.6003 - val_acc: 0.8370\n",
      "Epoch 21/85\n",
      " - 18s - loss: 0.7233 - acc: 0.7344 - val_loss: 0.7550 - val_acc: 0.7666\n",
      "Epoch 22/85\n",
      " - 18s - loss: 0.7195 - acc: 0.7395 - val_loss: 0.6163 - val_acc: 0.8360\n",
      "Epoch 23/85\n",
      " - 18s - loss: 0.7214 - acc: 0.7384 - val_loss: 0.6154 - val_acc: 0.8334\n",
      "Epoch 24/85\n",
      " - 18s - loss: 0.7126 - acc: 0.7420 - val_loss: 0.6100 - val_acc: 0.8194\n",
      "Epoch 25/85\n",
      " - 18s - loss: 0.7099 - acc: 0.7442 - val_loss: 0.6680 - val_acc: 0.7945\n",
      "Epoch 26/85\n",
      " - 18s - loss: 0.7051 - acc: 0.7499 - val_loss: 0.7031 - val_acc: 0.7906\n",
      "Epoch 27/85\n",
      " - 18s - loss: 0.7031 - acc: 0.7476 - val_loss: 0.6501 - val_acc: 0.8178\n",
      "Epoch 28/85\n",
      " - 18s - loss: 0.7012 - acc: 0.7501 - val_loss: 0.5842 - val_acc: 0.8422\n",
      "Epoch 29/85\n",
      " - 18s - loss: 0.6977 - acc: 0.7509 - val_loss: 0.6105 - val_acc: 0.8386\n",
      "Epoch 30/85\n",
      " - 18s - loss: 0.6974 - acc: 0.7486 - val_loss: 0.5811 - val_acc: 0.8381\n",
      "Epoch 31/85\n",
      " - 18s - loss: 0.6929 - acc: 0.7512 - val_loss: 0.8550 - val_acc: 0.6613\n",
      "Epoch 32/85\n",
      " - 18s - loss: 0.6869 - acc: 0.7518 - val_loss: 0.8618 - val_acc: 0.6866\n",
      "Epoch 33/85\n",
      " - 18s - loss: 0.6816 - acc: 0.7550 - val_loss: 0.5542 - val_acc: 0.8422\n",
      "Epoch 34/85\n",
      " - 18s - loss: 0.6793 - acc: 0.7554 - val_loss: 0.5891 - val_acc: 0.8465\n",
      "Epoch 35/85\n",
      " - 18s - loss: 0.6784 - acc: 0.7562 - val_loss: 0.6103 - val_acc: 0.8194\n",
      "Epoch 36/85\n",
      " - 18s - loss: 0.6792 - acc: 0.7543 - val_loss: 0.5754 - val_acc: 0.8443\n",
      "Epoch 37/85\n",
      " - 18s - loss: 0.6787 - acc: 0.7536 - val_loss: 0.6886 - val_acc: 0.8054\n",
      "Epoch 38/85\n",
      " - 18s - loss: 0.6716 - acc: 0.7595 - val_loss: 0.5500 - val_acc: 0.8435\n",
      "Epoch 39/85\n",
      " - 18s - loss: 0.6714 - acc: 0.7598 - val_loss: 0.5668 - val_acc: 0.8368\n",
      "Epoch 40/85\n",
      " - 18s - loss: 0.6709 - acc: 0.7577 - val_loss: 0.5883 - val_acc: 0.8476\n",
      "Epoch 41/85\n",
      " - 18s - loss: 0.6686 - acc: 0.7589 - val_loss: 0.5605 - val_acc: 0.8458\n",
      "Epoch 42/85\n",
      " - 18s - loss: 0.6641 - acc: 0.7620 - val_loss: 0.6036 - val_acc: 0.8355\n",
      "Epoch 43/85\n",
      " - 18s - loss: 0.6643 - acc: 0.7593 - val_loss: 0.5912 - val_acc: 0.8228\n",
      "Epoch 44/85\n",
      " - 18s - loss: 0.6591 - acc: 0.7619 - val_loss: 0.6397 - val_acc: 0.8284\n",
      "Epoch 45/85\n",
      " - 18s - loss: 0.6607 - acc: 0.7639 - val_loss: 0.6232 - val_acc: 0.8479\n",
      "Epoch 46/85\n",
      " - 18s - loss: 0.6644 - acc: 0.7607 - val_loss: 0.5776 - val_acc: 0.8473\n",
      "Epoch 47/85\n",
      " - 18s - loss: 0.6587 - acc: 0.7620 - val_loss: 0.5945 - val_acc: 0.8448\n",
      "Epoch 48/85\n",
      " - 18s - loss: 0.6565 - acc: 0.7638 - val_loss: 0.5640 - val_acc: 0.8459\n",
      "Epoch 49/85\n",
      " - 18s - loss: 0.6569 - acc: 0.7616 - val_loss: 1.1945 - val_acc: 0.4796\n",
      "Epoch 50/85\n",
      " - 18s - loss: 0.6533 - acc: 0.7638 - val_loss: 0.5580 - val_acc: 0.8447\n",
      "Epoch 51/85\n",
      " - 18s - loss: 0.6518 - acc: 0.7647 - val_loss: 0.5398 - val_acc: 0.8463\n",
      "Epoch 52/85\n",
      " - 18s - loss: 0.6510 - acc: 0.7654 - val_loss: 0.5699 - val_acc: 0.8474\n",
      "Epoch 53/85\n",
      " - 18s - loss: 0.6502 - acc: 0.7656 - val_loss: 0.5464 - val_acc: 0.8453\n",
      "Epoch 54/85\n",
      " - 18s - loss: 0.6500 - acc: 0.7655 - val_loss: 0.5401 - val_acc: 0.8506\n",
      "Epoch 55/85\n",
      " - 18s - loss: 0.6480 - acc: 0.7663 - val_loss: 0.5456 - val_acc: 0.8477\n",
      "Epoch 56/85\n",
      " - 18s - loss: 0.6455 - acc: 0.7664 - val_loss: 0.5555 - val_acc: 0.8443\n",
      "Epoch 57/85\n",
      " - 18s - loss: 0.6468 - acc: 0.7662 - val_loss: 0.5407 - val_acc: 0.8506\n",
      "Epoch 58/85\n",
      " - 18s - loss: 0.6444 - acc: 0.7672 - val_loss: 0.5437 - val_acc: 0.8490\n",
      "Epoch 59/85\n",
      " - 18s - loss: 0.6435 - acc: 0.7678 - val_loss: 0.5507 - val_acc: 0.8494\n",
      "Epoch 60/85\n",
      " - 18s - loss: 0.6424 - acc: 0.7678 - val_loss: 0.5564 - val_acc: 0.8502\n",
      "Epoch 61/85\n",
      " - 18s - loss: 0.6432 - acc: 0.7668 - val_loss: 0.5396 - val_acc: 0.8516\n",
      "Epoch 62/85\n",
      " - 18s - loss: 0.6435 - acc: 0.7667 - val_loss: 0.6612 - val_acc: 0.7891\n",
      "Epoch 63/85\n",
      " - 18s - loss: 0.6386 - acc: 0.7686 - val_loss: 0.5511 - val_acc: 0.8486\n",
      "Epoch 64/85\n",
      " - 18s - loss: 0.6391 - acc: 0.7683 - val_loss: 0.5619 - val_acc: 0.8493\n",
      "Epoch 65/85\n",
      " - 18s - loss: 0.6428 - acc: 0.7681 - val_loss: 0.5504 - val_acc: 0.8500\n",
      "Epoch 66/85\n",
      " - 18s - loss: 0.6391 - acc: 0.7693 - val_loss: 0.5452 - val_acc: 0.8483\n",
      "Epoch 67/85\n",
      " - 18s - loss: 0.6386 - acc: 0.7694 - val_loss: 0.5476 - val_acc: 0.8497\n",
      "Epoch 68/85\n",
      " - 18s - loss: 0.6364 - acc: 0.7698 - val_loss: 0.5482 - val_acc: 0.8483\n",
      "Epoch 69/85\n",
      " - 18s - loss: 0.6351 - acc: 0.7704 - val_loss: 0.5543 - val_acc: 0.8478\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 70/85\n",
      " - 18s - loss: 0.6341 - acc: 0.7706 - val_loss: 0.5559 - val_acc: 0.8500\n",
      "Epoch 71/85\n",
      " - 18s - loss: 0.6349 - acc: 0.7692 - val_loss: 0.5507 - val_acc: 0.8488\n",
      "Epoch 72/85\n",
      " - 18s - loss: 0.6321 - acc: 0.7720 - val_loss: 0.5522 - val_acc: 0.8500\n",
      "Epoch 73/85\n",
      " - 18s - loss: 0.6335 - acc: 0.7700 - val_loss: 0.5534 - val_acc: 0.8481\n",
      "Epoch 74/85\n",
      " - 18s - loss: 0.6346 - acc: 0.7695 - val_loss: 0.5583 - val_acc: 0.8457\n",
      "Epoch 75/85\n",
      " - 18s - loss: 0.6333 - acc: 0.7710 - val_loss: 0.6144 - val_acc: 0.8328\n",
      "Epoch 76/85\n",
      " - 18s - loss: 0.6319 - acc: 0.7711 - val_loss: 0.5746 - val_acc: 0.8424\n",
      "Epoch 77/85\n",
      " - 18s - loss: 0.6318 - acc: 0.7712 - val_loss: 0.6584 - val_acc: 0.7844\n",
      "Epoch 78/85\n",
      " - 18s - loss: 0.6308 - acc: 0.7702 - val_loss: 0.5738 - val_acc: 0.8468\n",
      "Epoch 79/85\n",
      " - 18s - loss: 0.6325 - acc: 0.7713 - val_loss: 0.5657 - val_acc: 0.8384\n",
      "Epoch 80/85\n",
      " - 18s - loss: 0.6303 - acc: 0.7713 - val_loss: 0.5557 - val_acc: 0.8467\n",
      "Epoch 81/85\n",
      " - 18s - loss: 0.6326 - acc: 0.7706 - val_loss: 0.5589 - val_acc: 0.8498\n",
      "Epoch 82/85\n",
      " - 18s - loss: 0.6321 - acc: 0.7705 - val_loss: 0.5415 - val_acc: 0.8499\n",
      "Epoch 83/85\n",
      " - 18s - loss: 0.6283 - acc: 0.7730 - val_loss: 0.5653 - val_acc: 0.8465\n",
      "Epoch 84/85\n",
      " - 18s - loss: 0.6317 - acc: 0.7712 - val_loss: 0.5481 - val_acc: 0.8518\n",
      "Epoch 85/85\n",
      " - 18s - loss: 0.6304 - acc: 0.7720 - val_loss: 0.5548 - val_acc: 0.8498\n",
      "49958/49958 [==============================] - 8s 157us/step\n",
      " Dropout : 0.3 accuracy is : 0.813823611834\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_39 (LSTM)               (None, 1, 1000)           9928000   \n",
      "_________________________________________________________________\n",
      "dropout_39 (Dropout)         (None, 1, 1000)           0         \n",
      "_________________________________________________________________\n",
      "lstm_40 (LSTM)               (None, 800)               5763200   \n",
      "_________________________________________________________________\n",
      "dropout_40 (Dropout)         (None, 800)               0         \n",
      "_________________________________________________________________\n",
      "dense_39 (Dense)             (None, 150)               120150    \n",
      "_________________________________________________________________\n",
      "dense_40 (Dense)             (None, 3)                 453       \n",
      "=================================================================\n",
      "Total params: 15,811,803\n",
      "Trainable params: 15,811,803\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      " Model build done!\n",
      "Train on 159866 samples, validate on 39967 samples\n",
      "Epoch 1/85\n",
      " - 22s - loss: 1.0469 - acc: 0.5369 - val_loss: 1.1058 - val_acc: 0.5333\n",
      "Epoch 2/85\n",
      " - 18s - loss: 1.0382 - acc: 0.5343 - val_loss: 1.0099 - val_acc: 0.6500\n",
      "Epoch 3/85\n",
      " - 18s - loss: 0.9943 - acc: 0.5561 - val_loss: 1.0202 - val_acc: 0.5998\n",
      "Epoch 4/85\n",
      " - 18s - loss: 0.9604 - acc: 0.5685 - val_loss: 0.9807 - val_acc: 0.6306\n",
      "Epoch 5/85\n",
      " - 18s - loss: 0.9277 - acc: 0.5850 - val_loss: 1.0126 - val_acc: 0.6438\n",
      "Epoch 6/85\n",
      " - 18s - loss: 0.8979 - acc: 0.6086 - val_loss: 0.8941 - val_acc: 0.7245\n",
      "Epoch 7/85\n",
      " - 18s - loss: 0.8697 - acc: 0.6304 - val_loss: 0.8106 - val_acc: 0.7592\n",
      "Epoch 8/85\n",
      " - 18s - loss: 0.8476 - acc: 0.6503 - val_loss: 1.1518 - val_acc: 0.5500\n",
      "Epoch 9/85\n",
      " - 18s - loss: 0.8357 - acc: 0.6606 - val_loss: 0.6437 - val_acc: 0.8270\n",
      "Epoch 10/85\n",
      " - 18s - loss: 0.8065 - acc: 0.6788 - val_loss: 0.9260 - val_acc: 0.7171\n",
      "Epoch 11/85\n",
      " - 18s - loss: 0.7896 - acc: 0.6935 - val_loss: 0.6112 - val_acc: 0.8320\n",
      "Epoch 12/85\n",
      " - 18s - loss: 0.7808 - acc: 0.6994 - val_loss: 0.6231 - val_acc: 0.8294\n",
      "Epoch 13/85\n",
      " - 18s - loss: 0.7702 - acc: 0.7081 - val_loss: 0.6216 - val_acc: 0.8290\n",
      "Epoch 14/85\n",
      " - 18s - loss: 0.7570 - acc: 0.7178 - val_loss: 0.6365 - val_acc: 0.8347\n",
      "Epoch 15/85\n",
      " - 18s - loss: 0.7476 - acc: 0.7234 - val_loss: 0.6077 - val_acc: 0.8355\n",
      "Epoch 16/85\n",
      " - 18s - loss: 0.7452 - acc: 0.7270 - val_loss: 0.6273 - val_acc: 0.8243\n",
      "Epoch 17/85\n",
      " - 18s - loss: 0.7363 - acc: 0.7301 - val_loss: 0.6103 - val_acc: 0.8355\n",
      "Epoch 18/85\n",
      " - 18s - loss: 0.7331 - acc: 0.7332 - val_loss: 0.6637 - val_acc: 0.7937\n",
      "Epoch 19/85\n",
      " - 18s - loss: 0.7329 - acc: 0.7306 - val_loss: 0.5805 - val_acc: 0.8385\n",
      "Epoch 20/85\n",
      " - 18s - loss: 0.7232 - acc: 0.7387 - val_loss: 0.5791 - val_acc: 0.8410\n",
      "Epoch 21/85\n",
      " - 18s - loss: 0.7234 - acc: 0.7351 - val_loss: 0.5748 - val_acc: 0.8417\n",
      "Epoch 22/85\n",
      " - 18s - loss: 0.7148 - acc: 0.7439 - val_loss: 0.5738 - val_acc: 0.8423\n",
      "Epoch 23/85\n",
      " - 18s - loss: 0.7076 - acc: 0.7478 - val_loss: 0.7222 - val_acc: 0.7630\n",
      "Epoch 24/85\n",
      " - 18s - loss: 0.7077 - acc: 0.7459 - val_loss: 0.5818 - val_acc: 0.8473\n",
      "Epoch 25/85\n",
      " - 18s - loss: 0.7106 - acc: 0.7425 - val_loss: 0.6554 - val_acc: 0.8334\n",
      "Epoch 26/85\n",
      " - 18s - loss: 0.7052 - acc: 0.7475 - val_loss: 0.5994 - val_acc: 0.8357\n",
      "Epoch 27/85\n",
      " - 18s - loss: 0.7010 - acc: 0.7472 - val_loss: 0.5996 - val_acc: 0.8384\n",
      "Epoch 28/85\n",
      " - 18s - loss: 0.6952 - acc: 0.7515 - val_loss: 0.6348 - val_acc: 0.8091\n",
      "Epoch 29/85\n",
      " - 18s - loss: 0.6941 - acc: 0.7532 - val_loss: 0.6654 - val_acc: 0.8035\n",
      "Epoch 30/85\n",
      " - 18s - loss: 0.6907 - acc: 0.7513 - val_loss: 0.5550 - val_acc: 0.8433\n",
      "Epoch 31/85\n",
      " - 18s - loss: 0.6899 - acc: 0.7529 - val_loss: 0.5862 - val_acc: 0.8316\n",
      "Epoch 32/85\n",
      " - 18s - loss: 0.6874 - acc: 0.7498 - val_loss: 0.5628 - val_acc: 0.8446\n",
      "Epoch 33/85\n",
      " - 18s - loss: 0.6811 - acc: 0.7580 - val_loss: 0.5510 - val_acc: 0.8428\n",
      "Epoch 34/85\n",
      " - 18s - loss: 0.6822 - acc: 0.7556 - val_loss: 0.5600 - val_acc: 0.8456\n",
      "Epoch 35/85\n",
      " - 18s - loss: 0.6782 - acc: 0.7554 - val_loss: 0.5985 - val_acc: 0.8425\n",
      "Epoch 36/85\n",
      " - 18s - loss: 0.6743 - acc: 0.7565 - val_loss: 0.6154 - val_acc: 0.8416\n",
      "Epoch 37/85\n",
      " - 18s - loss: 0.6736 - acc: 0.7581 - val_loss: 0.5651 - val_acc: 0.8448\n",
      "Epoch 38/85\n",
      " - 18s - loss: 0.6732 - acc: 0.7591 - val_loss: 0.5570 - val_acc: 0.8464\n",
      "Epoch 39/85\n",
      " - 18s - loss: 0.6688 - acc: 0.7596 - val_loss: 0.5504 - val_acc: 0.8491\n",
      "Epoch 40/85\n",
      " - 18s - loss: 0.6735 - acc: 0.7563 - val_loss: 0.5599 - val_acc: 0.8483\n",
      "Epoch 41/85\n",
      " - 18s - loss: 0.6664 - acc: 0.7607 - val_loss: 0.6669 - val_acc: 0.8127\n",
      "Epoch 42/85\n",
      " - 18s - loss: 0.6608 - acc: 0.7625 - val_loss: 0.5784 - val_acc: 0.8483\n",
      "Epoch 43/85\n",
      " - 18s - loss: 0.6659 - acc: 0.7591 - val_loss: 0.5954 - val_acc: 0.8351\n",
      "Epoch 44/85\n",
      " - 18s - loss: 0.6570 - acc: 0.7645 - val_loss: 0.5946 - val_acc: 0.8315\n",
      "Epoch 45/85\n",
      " - 18s - loss: 0.6566 - acc: 0.7624 - val_loss: 0.5787 - val_acc: 0.8476\n",
      "Epoch 46/85\n",
      " - 18s - loss: 0.6599 - acc: 0.7634 - val_loss: 0.6108 - val_acc: 0.8387\n",
      "Epoch 47/85\n",
      " - 18s - loss: 0.6563 - acc: 0.7637 - val_loss: 0.7162 - val_acc: 0.7645\n",
      "Epoch 48/85\n",
      " - 18s - loss: 0.6598 - acc: 0.7615 - val_loss: 0.5521 - val_acc: 0.8502\n",
      "Epoch 49/85\n",
      " - 18s - loss: 0.6538 - acc: 0.7648 - val_loss: 0.5822 - val_acc: 0.8450\n",
      "Epoch 50/85\n",
      " - 18s - loss: 0.6494 - acc: 0.7676 - val_loss: 0.5590 - val_acc: 0.8497\n",
      "Epoch 51/85\n",
      " - 18s - loss: 0.6525 - acc: 0.7650 - val_loss: 0.5599 - val_acc: 0.8452\n",
      "Epoch 52/85\n",
      " - 18s - loss: 0.6501 - acc: 0.7656 - val_loss: 0.5654 - val_acc: 0.8378\n",
      "Epoch 53/85\n",
      " - 18s - loss: 0.6491 - acc: 0.7653 - val_loss: 0.5808 - val_acc: 0.8343\n",
      "Epoch 54/85\n",
      " - 18s - loss: 0.6491 - acc: 0.7662 - val_loss: 0.5513 - val_acc: 0.8470\n",
      "Epoch 55/85\n",
      " - 18s - loss: 0.6494 - acc: 0.7657 - val_loss: 0.6834 - val_acc: 0.7857\n",
      "Epoch 56/85\n",
      " - 18s - loss: 0.6464 - acc: 0.7673 - val_loss: 0.5419 - val_acc: 0.8517\n",
      "Epoch 57/85\n",
      " - 18s - loss: 0.6457 - acc: 0.7684 - val_loss: 0.5482 - val_acc: 0.8508\n",
      "Epoch 58/85\n",
      " - 18s - loss: 0.6453 - acc: 0.7677 - val_loss: 0.5885 - val_acc: 0.8449\n",
      "Epoch 59/85\n",
      " - 18s - loss: 0.6446 - acc: 0.7680 - val_loss: 0.5513 - val_acc: 0.8470\n",
      "Epoch 60/85\n",
      " - 18s - loss: 0.6463 - acc: 0.7658 - val_loss: 0.5498 - val_acc: 0.8492\n",
      "Epoch 61/85\n",
      " - 18s - loss: 0.6423 - acc: 0.7683 - val_loss: 0.7486 - val_acc: 0.7200\n",
      "Epoch 62/85\n",
      " - 18s - loss: 0.6466 - acc: 0.7651 - val_loss: 0.5462 - val_acc: 0.8516\n",
      "Epoch 63/85\n",
      " - 18s - loss: 0.6404 - acc: 0.7691 - val_loss: 0.5541 - val_acc: 0.8504\n",
      "Epoch 64/85\n",
      " - 18s - loss: 0.6405 - acc: 0.7679 - val_loss: 0.5420 - val_acc: 0.8525\n",
      "Epoch 65/85\n",
      " - 18s - loss: 0.6421 - acc: 0.7669 - val_loss: 0.5903 - val_acc: 0.8418\n",
      "Epoch 66/85\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 18s - loss: 0.6396 - acc: 0.7690 - val_loss: 0.5475 - val_acc: 0.8478\n",
      "Epoch 67/85\n",
      " - 18s - loss: 0.6400 - acc: 0.7701 - val_loss: 0.5547 - val_acc: 0.8483\n",
      "Epoch 68/85\n",
      " - 18s - loss: 0.6375 - acc: 0.7677 - val_loss: 0.5419 - val_acc: 0.8503\n",
      "Epoch 69/85\n",
      " - 18s - loss: 0.6367 - acc: 0.7702 - val_loss: 0.5475 - val_acc: 0.8483\n",
      "Epoch 70/85\n",
      " - 18s - loss: 0.6347 - acc: 0.7704 - val_loss: 0.5750 - val_acc: 0.8322\n",
      "Epoch 71/85\n",
      " - 18s - loss: 0.6348 - acc: 0.7702 - val_loss: 0.5579 - val_acc: 0.8440\n",
      "Epoch 72/85\n",
      " - 18s - loss: 0.6363 - acc: 0.7697 - val_loss: 0.5450 - val_acc: 0.8500\n",
      "Epoch 73/85\n",
      " - 18s - loss: 0.6354 - acc: 0.7704 - val_loss: 0.5457 - val_acc: 0.8468\n",
      "Epoch 74/85\n",
      " - 18s - loss: 0.6377 - acc: 0.7709 - val_loss: 0.5461 - val_acc: 0.8495\n",
      "Epoch 75/85\n",
      " - 18s - loss: 0.6327 - acc: 0.7713 - val_loss: 0.5439 - val_acc: 0.8510\n",
      "Epoch 76/85\n",
      " - 18s - loss: 0.6294 - acc: 0.7717 - val_loss: 0.5545 - val_acc: 0.8457\n",
      "Epoch 77/85\n",
      " - 18s - loss: 0.6306 - acc: 0.7731 - val_loss: 0.5405 - val_acc: 0.8479\n",
      "Epoch 78/85\n",
      " - 18s - loss: 0.6315 - acc: 0.7712 - val_loss: 0.5507 - val_acc: 0.8501\n",
      "Epoch 79/85\n",
      " - 18s - loss: 0.6308 - acc: 0.7721 - val_loss: 0.5410 - val_acc: 0.8527\n",
      "Epoch 80/85\n",
      " - 18s - loss: 0.6286 - acc: 0.7721 - val_loss: 0.5468 - val_acc: 0.8473\n",
      "Epoch 81/85\n",
      " - 18s - loss: 0.6337 - acc: 0.7698 - val_loss: 0.5619 - val_acc: 0.8447\n",
      "Epoch 82/85\n",
      " - 18s - loss: 0.6300 - acc: 0.7722 - val_loss: 0.5404 - val_acc: 0.8523\n",
      "Epoch 83/85\n",
      " - 18s - loss: 0.6299 - acc: 0.7710 - val_loss: 0.5478 - val_acc: 0.8500\n",
      "Epoch 84/85\n",
      " - 18s - loss: 0.6313 - acc: 0.7725 - val_loss: 0.5631 - val_acc: 0.8473\n",
      "Epoch 85/85\n",
      " - 18s - loss: 0.6296 - acc: 0.7721 - val_loss: 0.5417 - val_acc: 0.8516\n",
      "49958/49958 [==============================] - 8s 157us/step\n",
      " Dropout : 0.35 accuracy is : 0.817646823332\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_41 (LSTM)               (None, 1, 1000)           9928000   \n",
      "_________________________________________________________________\n",
      "dropout_41 (Dropout)         (None, 1, 1000)           0         \n",
      "_________________________________________________________________\n",
      "lstm_42 (LSTM)               (None, 800)               5763200   \n",
      "_________________________________________________________________\n",
      "dropout_42 (Dropout)         (None, 800)               0         \n",
      "_________________________________________________________________\n",
      "dense_41 (Dense)             (None, 150)               120150    \n",
      "_________________________________________________________________\n",
      "dense_42 (Dense)             (None, 3)                 453       \n",
      "=================================================================\n",
      "Total params: 15,811,803\n",
      "Trainable params: 15,811,803\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      " Model build done!\n",
      "Train on 159866 samples, validate on 39967 samples\n",
      "Epoch 1/85\n",
      " - 22s - loss: 1.0461 - acc: 0.5361 - val_loss: 1.1087 - val_acc: 0.5333\n",
      "Epoch 2/85\n",
      " - 18s - loss: 1.0430 - acc: 0.5329 - val_loss: 1.0876 - val_acc: 0.5333\n",
      "Epoch 3/85\n",
      " - 18s - loss: 1.0065 - acc: 0.5510 - val_loss: 1.0727 - val_acc: 0.5493\n",
      "Epoch 4/85\n",
      " - 18s - loss: 0.9682 - acc: 0.5661 - val_loss: 0.9583 - val_acc: 0.6045\n",
      "Epoch 5/85\n",
      " - 18s - loss: 0.9421 - acc: 0.5772 - val_loss: 0.9598 - val_acc: 0.6005\n",
      "Epoch 6/85\n",
      " - 18s - loss: 0.9157 - acc: 0.5950 - val_loss: 1.1329 - val_acc: 0.5539\n",
      "Epoch 7/85\n",
      " - 18s - loss: 0.8904 - acc: 0.6163 - val_loss: 0.8907 - val_acc: 0.6131\n",
      "Epoch 8/85\n",
      " - 18s - loss: 0.8594 - acc: 0.6362 - val_loss: 0.7452 - val_acc: 0.7017\n",
      "Epoch 9/85\n",
      " - 18s - loss: 0.8473 - acc: 0.6513 - val_loss: 0.9767 - val_acc: 0.6652\n",
      "Epoch 10/85\n",
      " - 18s - loss: 0.8154 - acc: 0.6724 - val_loss: 0.7058 - val_acc: 0.7945\n",
      "Epoch 11/85\n",
      " - 18s - loss: 0.8055 - acc: 0.6842 - val_loss: 0.6592 - val_acc: 0.8148\n",
      "Epoch 12/85\n",
      " - 18s - loss: 0.7846 - acc: 0.7000 - val_loss: 0.7529 - val_acc: 0.7046\n",
      "Epoch 13/85\n",
      " - 18s - loss: 0.7757 - acc: 0.7052 - val_loss: 0.6273 - val_acc: 0.8339\n",
      "Epoch 14/85\n",
      " - 18s - loss: 0.7593 - acc: 0.7159 - val_loss: 0.6053 - val_acc: 0.8374\n",
      "Epoch 15/85\n",
      " - 18s - loss: 0.7575 - acc: 0.7175 - val_loss: 0.6302 - val_acc: 0.8291\n",
      "Epoch 16/85\n",
      " - 18s - loss: 0.7512 - acc: 0.7202 - val_loss: 0.6079 - val_acc: 0.8355\n",
      "Epoch 17/85\n",
      " - 18s - loss: 0.7463 - acc: 0.7234 - val_loss: 0.6004 - val_acc: 0.8330\n",
      "Epoch 18/85\n",
      " - 18s - loss: 0.7518 - acc: 0.7213 - val_loss: 0.5880 - val_acc: 0.8368\n",
      "Epoch 19/85\n",
      " - 18s - loss: 0.7317 - acc: 0.7369 - val_loss: 0.5810 - val_acc: 0.8386\n",
      "Epoch 20/85\n",
      " - 18s - loss: 0.7396 - acc: 0.7212 - val_loss: 0.7651 - val_acc: 0.6808\n",
      "Epoch 21/85\n",
      " - 18s - loss: 0.7316 - acc: 0.7337 - val_loss: 0.5815 - val_acc: 0.8403\n",
      "Epoch 22/85\n",
      " - 18s - loss: 0.7263 - acc: 0.7337 - val_loss: 0.5957 - val_acc: 0.8384\n",
      "Epoch 23/85\n",
      " - 18s - loss: 0.7180 - acc: 0.7391 - val_loss: 0.5702 - val_acc: 0.8404\n",
      "Epoch 24/85\n",
      " - 18s - loss: 0.7198 - acc: 0.7399 - val_loss: 0.5948 - val_acc: 0.8316\n",
      "Epoch 25/85\n",
      " - 18s - loss: 0.7129 - acc: 0.7447 - val_loss: 0.5643 - val_acc: 0.8469\n",
      "Epoch 26/85\n",
      " - 18s - loss: 0.7050 - acc: 0.7459 - val_loss: 0.7773 - val_acc: 0.7425\n",
      "Epoch 27/85\n",
      " - 18s - loss: 0.7070 - acc: 0.7461 - val_loss: 0.6237 - val_acc: 0.8354\n",
      "Epoch 28/85\n",
      " - 18s - loss: 0.7057 - acc: 0.7445 - val_loss: 0.5896 - val_acc: 0.8414\n",
      "Epoch 29/85\n",
      " - 18s - loss: 0.6963 - acc: 0.7508 - val_loss: 0.5729 - val_acc: 0.8482\n",
      "Epoch 30/85\n",
      " - 18s - loss: 0.6969 - acc: 0.7501 - val_loss: 0.6334 - val_acc: 0.8214\n",
      "Epoch 31/85\n",
      " - 18s - loss: 0.6926 - acc: 0.7531 - val_loss: 0.6656 - val_acc: 0.8046\n",
      "Epoch 32/85\n",
      " - 18s - loss: 0.6954 - acc: 0.7478 - val_loss: 0.6124 - val_acc: 0.8487\n",
      "Epoch 33/85\n",
      " - 18s - loss: 0.6879 - acc: 0.7523 - val_loss: 0.5611 - val_acc: 0.8462\n",
      "Epoch 34/85\n",
      " - 18s - loss: 0.6848 - acc: 0.7546 - val_loss: 0.6008 - val_acc: 0.8340\n",
      "Epoch 35/85\n",
      " - 18s - loss: 0.6861 - acc: 0.7572 - val_loss: 0.5949 - val_acc: 0.8450\n",
      "Epoch 36/85\n",
      " - 18s - loss: 0.6795 - acc: 0.7563 - val_loss: 0.5838 - val_acc: 0.8444\n",
      "Epoch 37/85\n",
      " - 18s - loss: 0.6841 - acc: 0.7558 - val_loss: 0.5840 - val_acc: 0.8461\n",
      "Epoch 38/85\n",
      " - 18s - loss: 0.6779 - acc: 0.7554 - val_loss: 0.5877 - val_acc: 0.8502\n",
      "Epoch 39/85\n",
      " - 18s - loss: 0.6830 - acc: 0.7531 - val_loss: 0.5741 - val_acc: 0.8482\n",
      "Epoch 40/85\n",
      " - 18s - loss: 0.6765 - acc: 0.7564 - val_loss: 0.5788 - val_acc: 0.8442\n",
      "Epoch 41/85\n",
      " - 18s - loss: 0.6728 - acc: 0.7576 - val_loss: 0.6171 - val_acc: 0.8483\n",
      "Epoch 42/85\n",
      " - 18s - loss: 0.6706 - acc: 0.7593 - val_loss: 0.5606 - val_acc: 0.8513\n",
      "Epoch 43/85\n",
      " - 18s - loss: 0.6685 - acc: 0.7592 - val_loss: 0.5446 - val_acc: 0.8472\n",
      "Epoch 44/85\n",
      " - 18s - loss: 0.6709 - acc: 0.7594 - val_loss: 0.5975 - val_acc: 0.8252\n",
      "Epoch 45/85\n",
      " - 18s - loss: 0.6651 - acc: 0.7623 - val_loss: 0.5474 - val_acc: 0.8505\n",
      "Epoch 46/85\n",
      " - 18s - loss: 0.6652 - acc: 0.7605 - val_loss: 0.5919 - val_acc: 0.8472\n",
      "Epoch 47/85\n",
      " - 18s - loss: 0.6636 - acc: 0.7609 - val_loss: 0.5470 - val_acc: 0.8482\n",
      "Epoch 48/85\n",
      " - 18s - loss: 0.6646 - acc: 0.7597 - val_loss: 0.5609 - val_acc: 0.8465\n",
      "Epoch 49/85\n",
      " - 18s - loss: 0.6609 - acc: 0.7626 - val_loss: 0.5486 - val_acc: 0.8452\n",
      "Epoch 50/85\n",
      " - 18s - loss: 0.6587 - acc: 0.7629 - val_loss: 0.5743 - val_acc: 0.8512\n",
      "Epoch 51/85\n",
      " - 18s - loss: 0.6586 - acc: 0.7628 - val_loss: 0.5505 - val_acc: 0.8489\n",
      "Epoch 52/85\n",
      " - 18s - loss: 0.6598 - acc: 0.7618 - val_loss: 0.5512 - val_acc: 0.8454\n",
      "Epoch 53/85\n",
      " - 18s - loss: 0.6546 - acc: 0.7650 - val_loss: 0.5393 - val_acc: 0.8471\n",
      "Epoch 54/85\n",
      " - 18s - loss: 0.6579 - acc: 0.7626 - val_loss: 0.5701 - val_acc: 0.8481\n",
      "Epoch 55/85\n",
      " - 18s - loss: 0.6570 - acc: 0.7639 - val_loss: 0.5892 - val_acc: 0.8273\n",
      "Epoch 56/85\n",
      " - 18s - loss: 0.6503 - acc: 0.7664 - val_loss: 0.5417 - val_acc: 0.8480\n",
      "Epoch 57/85\n",
      " - 18s - loss: 0.6543 - acc: 0.7651 - val_loss: 0.5413 - val_acc: 0.8514\n",
      "Epoch 58/85\n",
      " - 18s - loss: 0.6497 - acc: 0.7657 - val_loss: 0.5489 - val_acc: 0.8472\n",
      "Epoch 59/85\n",
      " - 18s - loss: 0.6511 - acc: 0.7652 - val_loss: 0.5668 - val_acc: 0.8493\n",
      "Epoch 60/85\n",
      " - 18s - loss: 0.6518 - acc: 0.7654 - val_loss: 0.5510 - val_acc: 0.8481\n",
      "Epoch 61/85\n",
      " - 18s - loss: 0.6465 - acc: 0.7668 - val_loss: 0.5413 - val_acc: 0.8484\n",
      "Epoch 62/85\n",
      " - 18s - loss: 0.6488 - acc: 0.7668 - val_loss: 0.5398 - val_acc: 0.8490\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 63/85\n",
      " - 18s - loss: 0.6461 - acc: 0.7677 - val_loss: 0.5355 - val_acc: 0.8481\n",
      "Epoch 64/85\n",
      " - 18s - loss: 0.6484 - acc: 0.7654 - val_loss: 0.5306 - val_acc: 0.8504\n",
      "Epoch 65/85\n",
      " - 18s - loss: 0.6432 - acc: 0.7686 - val_loss: 0.5454 - val_acc: 0.8494\n",
      "Epoch 66/85\n",
      " - 18s - loss: 0.6438 - acc: 0.7685 - val_loss: 0.5438 - val_acc: 0.8522\n",
      "Epoch 67/85\n",
      " - 18s - loss: 0.6471 - acc: 0.7668 - val_loss: 0.5466 - val_acc: 0.8506\n",
      "Epoch 68/85\n",
      " - 18s - loss: 0.6430 - acc: 0.7689 - val_loss: 0.5370 - val_acc: 0.8497\n",
      "Epoch 69/85\n",
      " - 18s - loss: 0.6447 - acc: 0.7678 - val_loss: 0.5738 - val_acc: 0.8439\n",
      "Epoch 70/85\n",
      " - 18s - loss: 0.6434 - acc: 0.7683 - val_loss: 0.5389 - val_acc: 0.8492\n",
      "Epoch 71/85\n",
      " - 18s - loss: 0.6424 - acc: 0.7683 - val_loss: 0.5332 - val_acc: 0.8500\n",
      "Epoch 72/85\n",
      " - 18s - loss: 0.6396 - acc: 0.7694 - val_loss: 0.5368 - val_acc: 0.8519\n",
      "Epoch 73/85\n",
      " - 18s - loss: 0.6411 - acc: 0.7685 - val_loss: 0.5519 - val_acc: 0.8410\n",
      "Epoch 74/85\n",
      " - 18s - loss: 0.6364 - acc: 0.7699 - val_loss: 0.5700 - val_acc: 0.8360\n",
      "Epoch 75/85\n",
      " - 18s - loss: 0.6407 - acc: 0.7692 - val_loss: 0.5470 - val_acc: 0.8514\n",
      "Epoch 76/85\n",
      " - 18s - loss: 0.6432 - acc: 0.7675 - val_loss: 0.5515 - val_acc: 0.8466\n",
      "Epoch 77/85\n",
      " - 18s - loss: 0.6401 - acc: 0.7695 - val_loss: 0.5402 - val_acc: 0.8496\n",
      "Epoch 78/85\n",
      " - 18s - loss: 0.6381 - acc: 0.7698 - val_loss: 0.5391 - val_acc: 0.8508\n",
      "Epoch 79/85\n",
      " - 18s - loss: 0.6385 - acc: 0.7691 - val_loss: 0.5404 - val_acc: 0.8499\n",
      "Epoch 80/85\n",
      " - 18s - loss: 0.6409 - acc: 0.7687 - val_loss: 0.5416 - val_acc: 0.8508\n",
      "Epoch 81/85\n",
      " - 18s - loss: 0.6375 - acc: 0.7702 - val_loss: 0.5404 - val_acc: 0.8481\n",
      "Epoch 82/85\n",
      " - 18s - loss: 0.6400 - acc: 0.7699 - val_loss: 0.5348 - val_acc: 0.8513\n",
      "Epoch 83/85\n",
      " - 18s - loss: 0.6371 - acc: 0.7699 - val_loss: 0.5418 - val_acc: 0.8498\n",
      "Epoch 84/85\n",
      " - 18s - loss: 0.6353 - acc: 0.7709 - val_loss: 0.5477 - val_acc: 0.8497\n",
      "Epoch 85/85\n",
      " - 18s - loss: 0.6359 - acc: 0.7710 - val_loss: 0.5419 - val_acc: 0.8450\n",
      "49958/49958 [==============================] - 8s 157us/step\n",
      " Dropout : 0.4 accuracy is : 0.8038552384\n"
     ]
    }
   ],
   "source": [
    "ml3 = run_architecture_3_with_different_dropouts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i,model in enumerate(ml3):\n",
    "    dropout_list = [0.1, 0.15, 0.2, 0.25, 0.3, 0.35, 0.4]\n",
    "    name = 'model_arch_3_dp_'+str(dropout_list[i])+'.h5'\n",
    "    model.save(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49958/49958 [==============================] - 6s 118us/step\n",
      "49958/49958 [==============================] - 5s 105us/step\n",
      "49958/49958 [==============================] - 5s 105us/step\n",
      "49958/49958 [==============================] - 5s 102us/step\n",
      "49958/49958 [==============================] - 5s 103us/step\n",
      "49958/49958 [==============================] - 5s 103us/step\n",
      "49958/49958 [==============================] - 5s 103us/step\n"
     ]
    }
   ],
   "source": [
    "accuracies_arch_1 = []\n",
    "predictions_arch_1 = []\n",
    "for model in model_list:\n",
    "    accuracies_arch_1.append(model.evaluate(X_test,y_test)[1])\n",
    "    predictions_arch_1.append(model.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49958/49958 [==============================] - 6s 117us/step\n",
      "49958/49958 [==============================] - 6s 116us/step\n",
      "49958/49958 [==============================] - 6s 114us/step\n",
      "49958/49958 [==============================] - 6s 115us/step\n",
      "49958/49958 [==============================] - 6s 114us/step\n",
      "49958/49958 [==============================] - 6s 114us/step\n",
      "49958/49958 [==============================] - 6s 115us/step\n"
     ]
    }
   ],
   "source": [
    "accuracies_arch_2 = []\n",
    "predictions_arch_2 = []\n",
    "for model in ml:\n",
    "    accuracies_arch_2.append(model.evaluate(X_test,y_test)[1])\n",
    "    predictions_arch_2.append(model.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49958/49958 [==============================] - 8s 157us/step\n",
      "49958/49958 [==============================] - 8s 157us/step\n",
      "49958/49958 [==============================] - 8s 157us/step\n",
      "49958/49958 [==============================] - 8s 157us/step\n",
      "49958/49958 [==============================] - 8s 157us/step\n",
      "49958/49958 [==============================] - 8s 157us/step\n",
      "49958/49958 [==============================] - 8s 157us/step\n"
     ]
    }
   ],
   "source": [
    "accuracies_arch_3 = []\n",
    "predictions_arch_3 = []\n",
    "for model in ml3:\n",
    "    accuracies_arch_3.append(model.evaluate(X_test,y_test)[1])\n",
    "    predictions_arch_3.append(model.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.77222867208455104,\n",
       " 0.7921854357660435,\n",
       " 0.72837183233916492,\n",
       " 0.70429160494815646,\n",
       " 0.71027663237119176,\n",
       " 0.69320228992353572,\n",
       " 0.78854237559550022]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#best model for architecture 1 is 2nd one with d = 0.15\n",
    "accuracies_arch_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.80986028263741539,\n",
       " 0.79228551983666284,\n",
       " 0.82072941270667366,\n",
       " 0.76872572961287478,\n",
       " 0.7884422915254774,\n",
       " 0.80489611273469719,\n",
       " 0.79494775611513668]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#best model for architecture 2 is 3nd one with d = 0.2\n",
    "accuracies_arch_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.77739301012850792,\n",
       " 0.76175987829777014,\n",
       " 0.81740662156211219,\n",
       " 0.80651747467873014,\n",
       " 0.81382361183394047,\n",
       " 0.81764682333159855,\n",
       " 0.80385523840025619]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#best model for architecture 3 is 5th one with d = 0.3\n",
    "accuracies_arch_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7f7e48a7fb70>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAskAAAI/CAYAAACF/kTpAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f7e48ae69b0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "best_model_arch_1 = model_list[numpy.argmax(accuracies_arch_1)]\n",
    "best_model_arch_2 = ml[numpy.argmax(accuracies_arch_2)]\n",
    "best_model_arch_3 = ml3[numpy.argmax(accuracies_arch_3)]\n",
    "\n",
    "preds_model_1 = best_model_arch_1.predict(X_test)\n",
    "preds_model_1 = numpy.argmax(preds_model_1, axis = 1)\n",
    "\n",
    "preds_model_2 = best_model_arch_2.predict(X_test)\n",
    "preds_model_2 = numpy.argmax(preds_model_2, axis = 1)\n",
    "\n",
    "preds_model_3 = best_model_arch_3.predict(X_test)\n",
    "preds_model_3 = numpy.argmax(preds_model_3, axis = 1)\n",
    "\n",
    "preds_model_1_df = pandas.DataFrame(preds_model_1, columns=['model_1_predictions'])\n",
    "preds_model_2_df = pandas.DataFrame(preds_model_2, columns=['model_2_predictions'])\n",
    "preds_model_3_df = pandas.DataFrame(preds_model_3, columns=['model_3_predictions'])\n",
    "\n",
    "years.head(2)\n",
    "years_test = years[-len(y_test) :]\n",
    "years_test = years_test.reset_index()\n",
    "years_test.head(2)\n",
    "\n",
    "justice_test = raw_data[-len(y_test) :]\n",
    "justice_test = justice_test.reset_index()\n",
    "\n",
    "temp_df = pandas.concat([preds_model_1_df, preds_model_2_df], axis =1)\n",
    "temp_df = pandas.concat([temp_df, preds_model_3_df], axis = 1)\n",
    "temp_df = pandas.concat([temp_df, justice_test[['justice_outcome_disposition']]], axis = 1)\n",
    "temp_df = pandas.concat([temp_df, years_test[['term_raw']]], axis=1)\n",
    "\n",
    "per_year_avg_preds_model_1 = temp_df.groupby(\"term_raw\")[\"model_1_predictions\"].mean()\n",
    "per_year_avg_preds_model_2 = temp_df.groupby(\"term_raw\")[\"model_2_predictions\"].mean()\n",
    "per_year_avg_preds_model_3 = temp_df.groupby(\"term_raw\")[\"model_3_predictions\"].mean()\n",
    "per_year_avg_true_model = temp_df.groupby(\"term_raw\")[\"justice_outcome_disposition\"].mean()\n",
    "\n",
    "f = plt.figure(figsize=(12, 10))\n",
    "plt.plot(per_year_avg_preds_model_1, color = 'b')\n",
    "plt.plot(per_year_avg_preds_model_2, color = 'r')\n",
    "plt.plot(per_year_avg_preds_model_3, color = 'g')\n",
    "plt.plot(per_year_avg_true_model, color = 'y')\n",
    "plt.legend(('Model 1', 'Model 2', 'Model 3', 'True'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "best_model_arch_1 = model_list[numpy.argmax(accuracies_arch_1)]\n",
    "best_model_arch_2 = ml[numpy.argmax(accuracies_arch_2)]\n",
    "best_model_arch_3 = ml3[numpy.argmax(accuracies_arch_3)]\n",
    "\n",
    "preds_model_1 = best_model_arch_1.predict(X_test)\n",
    "preds_model_1 = numpy.argmax(preds_model_1, axis = 1)\n",
    "\n",
    "preds_model_2 = best_model_arch_2.predict(X_test)\n",
    "preds_model_2 = numpy.argmax(preds_model_2, axis = 1)\n",
    "\n",
    "preds_model_3 = best_model_arch_3.predict(X_test)\n",
    "preds_model_3 = numpy.argmax(preds_model_3, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.90      0.76      0.83     28651\n",
      "          1       0.81      0.81      0.81     13841\n",
      "          2       0.54      0.87      0.67      7466\n",
      "\n",
      "avg / total       0.82      0.79      0.80     49958\n",
      "\n",
      "[[21912  2293  4446]\n",
      " [ 1687 11168   986]\n",
      " [  623   347  6496]]\n",
      "0.792185435766\n"
     ]
    }
   ],
   "source": [
    "print(sklearn.metrics.classification_report(preds_model_1, numpy.argmax(y_test, axis =1 )))\n",
    "print(sklearn.metrics.confusion_matrix(preds_model_1, numpy.argmax(y_test, axis =1 )))\n",
    "print(sklearn.metrics.accuracy_score(preds_model_1, numpy.argmax(y_test, axis =1 )))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.89      0.83      0.86     26000\n",
      "          1       0.82      0.78      0.80     14428\n",
      "          2       0.69      0.86      0.76      9530\n",
      "\n",
      "avg / total       0.83      0.82      0.82     49958\n",
      "\n",
      "[[21518  1998  2484]\n",
      " [ 1860 11304  1264]\n",
      " [  844   506  8180]]\n",
      "0.820729412707\n"
     ]
    }
   ],
   "source": [
    "print(sklearn.metrics.classification_report(preds_model_2, numpy.argmax(y_test, axis =1 )))\n",
    "print(sklearn.metrics.confusion_matrix(preds_model_2, numpy.argmax(y_test, axis =1 )))\n",
    "print(sklearn.metrics.accuracy_score(preds_model_2, numpy.argmax(y_test, axis =1 )))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.89      0.82      0.85     26217\n",
      "          1       0.82      0.78      0.80     14438\n",
      "          2       0.68      0.87      0.76      9303\n",
      "\n",
      "avg / total       0.83      0.82      0.82     49958\n",
      "\n",
      "[[21496  2064  2657]\n",
      " [ 1939 11290  1209]\n",
      " [  787   454  8062]]\n",
      "0.817646823332\n"
     ]
    }
   ],
   "source": [
    "print(sklearn.metrics.classification_report(preds_model_3, numpy.argmax(y_test, axis =1 )))\n",
    "print(sklearn.metrics.confusion_matrix(preds_model_3, numpy.argmax(y_test, axis =1 )))\n",
    "print(sklearn.metrics.accuracy_score(preds_model_3, numpy.argmax(y_test, axis =1 )))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_the_dummy_model_accuracy_for_all_decisions():\n",
    "    from sklearn.dummy import DummyClassifier\n",
    "    clf = DummyClassifier(strategy='most_frequent',random_state=0)\n",
    "    temp = feature_df.copy()\n",
    "    temp.drop(['justice_outcome_disposition'], axis =1, inplace=True)\n",
    "    \n",
    "    rows = round(0.8 * len(temp))\n",
    "    target_all = raw_data.loc[:,['justice_outcome_disposition']]\n",
    "    \n",
    "    dummy_train = temp[0:len(y_test)]\n",
    "    dummy_train_target = target_all[0:len(y_test)]\n",
    "    dummy_test = temp[-len(y_test):]\n",
    "    dummy_test_target = numpy.argmax(y_test,axis=1)\n",
    "    \n",
    "    \n",
    "    clf.fit(dummy_train, to_categorical(dummy_train_target))\n",
    "    dummy_preds = clf.predict(dummy_test)\n",
    "    dummy_preds = numpy.argmax(dummy_preds, axis = 1)\n",
    "    print(sklearn.metrics.classification_report(dummy_preds, dummy_test_target))\n",
    "    print(sklearn.metrics.confusion_matrix(dummy_preds, dummy_test_target))\n",
    "    print(sklearn.metrics.accuracy_score(dummy_preds, dummy_test_target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.48      0.65     49958\n",
      "          1       0.00      0.00      0.00         0\n",
      "          2       0.00      0.00      0.00         0\n",
      "\n",
      "avg / total       1.00      0.48      0.65     49958\n",
      "\n",
      "[[24222 13808 11928]\n",
      " [    0     0     0]\n",
      " [    0     0     0]]\n",
      "0.484847271708\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kshitijg1992/miniconda3/envs/carnd-term1/lib/python3.5/site-packages/sklearn/metrics/classification.py:1137: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "get_the_dummy_model_accuracy_for_all_decisions()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
